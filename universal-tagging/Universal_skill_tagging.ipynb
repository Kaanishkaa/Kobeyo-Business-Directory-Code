{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1CNF6hbUcnl",
        "outputId": "4bf72665-8cc2-42d3-cfa3-8b9df94839f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ]
        }
      ],
      "source": [
        "# Install all necessary packages\n",
        "!pip install fuzzywuzzy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzxvEtnIUkEv",
        "outputId": "51f3e79f-1b82-4338-cfaf-5f0750292780"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "‚úÖ Successfully changed directory to: /content/drive/My Drive/AI_Business_Classifier\n",
            "\n",
            "Files in your project directory:\n",
            ".:\n",
            "Extracted_Business_Groups_and_Types.csv     Outputs\n",
            "Extracted_Business_Groups_and_Types.gsheet  Skill_Tag_Rulebooks\n",
            "\n",
            "./Outputs:\n",
            "final_classified_businesses.csv  final_classified_businesses.gsheet\n",
            "\n",
            "./Skill_Tag_Rulebooks:\n",
            "'Beauty, Massage & Spa.csv'  'Food & Beverage Establishments.csv'\n"
          ]
        }
      ],
      "source": [
        "# Import the drive module from Google Colab\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount your Google Drive.\n",
        "# This will prompt you for authentication the first time.\n",
        "print(\"üöÄ Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- IMPORTANT: Define the path to your main project folder on Google Drive ---\n",
        "# Make sure this path exactly matches the folder structure you created.\n",
        "PROJECT_PATH = \"/content/drive/My Drive/AI_Business_Classifier/\"\n",
        "os.chdir(PROJECT_PATH) # Change the current working directory to your project folder\n",
        "\n",
        "# Verify that the directory was changed and list files to confirm\n",
        "print(f\"\\n‚úÖ Successfully changed directory to: {os.getcwd()}\")\n",
        "print(\"\\nFiles in your project directory:\")\n",
        "!ls -R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7nCF4WZUu8n"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# Cell 2: The Full Pipeline - Setup, Classes, and Execution\n",
        "# ==============================================================================\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import threading\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Dict, Any\n",
        "\n",
        "# LangChain, OpenAI, and Scraping Tools\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from openai import RateLimitError\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium_stealth import stealth\n",
        "from thefuzz import fuzz\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# ============================\n",
        "# 1. CONFIGURATION\n",
        "# ============================\n",
        "\n",
        "# # IMPORTANT: Set your API keys here\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"REMOVED_SECRET_KEY\"\n",
        "# GOOGLE_API_KEY = \"AIzaSyBXbfRXA9eJxdL3DmLt3TuDbCtvzP6RWLA\"\n",
        "# Option 2: Set directly in code (less secure - remove from production)\n",
        "# Option 1: Set from environment variables (recommended)\n",
        "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
        "GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "if not OPENAI_API_KEY:\n",
        "    OPENAI_API_KEY = \"\"\n",
        "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "if not GOOGLE_API_KEY:\n",
        "    GOOGLE_API_KEY = \"\"\n",
        "\n",
        "# Define file and folder paths relative to the project directory\n",
        "UNIVERSAL_TAGS_FILE = \"Extracted_Business_Groups_and_Types.csv\"\n",
        "RULEBOOKS_FOLDER = \"Skill_Tag_Rulebooks/\"\n",
        "OUTPUT_FOLDER = \"Outputs/\"\n",
        "FINAL_OUTPUT_FILE = os.path.join(OUTPUT_FOLDER, \"final_classified_businesses.csv\")\n",
        "\n",
        "CAREER_KEYWORDS = [\"career\", \"jobs\", \"employment\", \"hiring\", \"work with us\", \"join us\", \"opportunities\"]\n",
        "THIRD_PARTY_JOB_SITES = [\"indeed.com\", \"linkedin.com/jobs\", \"glassdoor.com\", \"workday.com\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wm_NrdGTVRd8",
        "outputId": "5f3c68d6-c6f9-4771-b468-202ce311ccf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Loading and parsing all configuration files...\n",
            "  ‚úÖ Loaded 215 business type mappings\n",
            "  ‚úÖ Loaded 2 rulebooks\n",
            "‚úÖ All configurations loaded and parsed successfully\n"
          ]
        }
      ],
      "source": [
        "# Initialize OpenAI LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# Thread-local storage for Selenium driver\n",
        "thread_local_driver = threading.local()\n",
        "\n",
        "def get_driver():\n",
        "    \"\"\"Initialize and return a thread-local Selenium driver with stealth mode.\"\"\"\n",
        "    if not hasattr(thread_local_driver, 'driver'):\n",
        "        print(\"  -  Initializing new STEALTH Selenium driver...\")\n",
        "        try:\n",
        "            chrome_options = Options()\n",
        "            chrome_options.add_argument(\"--headless\")\n",
        "            chrome_options.add_argument(\"--no-sandbox\")\n",
        "            chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "            chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "            chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "            chrome_options.add_experimental_option('useAutomationExtension', False)\n",
        "\n",
        "            driver = webdriver.Chrome(options=chrome_options)\n",
        "            stealth(driver,\n",
        "                languages=[\"en-US\", \"en\"],\n",
        "                vendor=\"Google Inc.\",\n",
        "                platform=\"Win32\",\n",
        "                webgl_vendor=\"Intel Inc.\",\n",
        "                renderer=\"Intel Iris OpenGL Engine\",\n",
        "                fix_hairline=True\n",
        "            )\n",
        "            thread_local_driver.driver = driver\n",
        "            print(\"  ‚úÖ Selenium driver initialized successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Error initializing Selenium driver: {e}\")\n",
        "            return None\n",
        "    return thread_local_driver.driver\n",
        "\n",
        "def load_and_parse_configs():\n",
        "    \"\"\"Load and parse all configuration files (universal tags and rulebooks).\"\"\"\n",
        "    print(\"üöÄ Loading and parsing all configuration files...\")\n",
        "\n",
        "    # Load universal tags mapping\n",
        "    try:\n",
        "        df_universal = pd.read_csv(UNIVERSAL_TAGS_FILE)\n",
        "        group_keyword_map = {}\n",
        "\n",
        "        for _, row in df_universal.iterrows():\n",
        "            group = row['Business Group']\n",
        "            types_str = row['Business Type']\n",
        "\n",
        "            if pd.notna(types_str):\n",
        "                for biz_type in [t.strip().lower() for t in types_str.split(',')]:\n",
        "                    group_keyword_map[biz_type] = group\n",
        "\n",
        "        print(f\"  ‚úÖ Loaded {len(group_keyword_map)} business type mappings\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"  ‚ùå Error: {UNIVERSAL_TAGS_FILE} not found\")\n",
        "        return {}, {}\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error loading universal tags: {e}\")\n",
        "        return {}, {}\n",
        "\n",
        "    # Load rulebooks\n",
        "    rulebook_data = {}\n",
        "    try:\n",
        "        if not os.path.exists(RULEBOOKS_FOLDER):\n",
        "            print(f\"  ‚ùå Error: {RULEBOOKS_FOLDER} directory not found\")\n",
        "            return group_keyword_map, {}\n",
        "\n",
        "        for filename in os.listdir(RULEBOOKS_FOLDER):\n",
        "            if filename.endswith(\".csv\"):\n",
        "                business_group_name = filename.replace(\".csv\", \"\")\n",
        "                df_rulebook = pd.read_csv(os.path.join(RULEBOOKS_FOLDER, filename))\n",
        "                df_rulebook.columns = df_rulebook.columns.str.strip()\n",
        "\n",
        "                rules = {}\n",
        "                skill_id_map = {}\n",
        "                header_notes = {}\n",
        "\n",
        "                for _, row in df_rulebook.iterrows():\n",
        "                    tag = str(row.get('Skills Tags', '')).strip()\n",
        "\n",
        "                    if tag.upper() in ['BUSINESS TYPES', 'SPECIAL TAGS', 'IMPORTANT NOTES']:\n",
        "                        header_notes[tag.upper()] = str(row.get('Prompt Rule', ''))\n",
        "                    elif pd.notna(tag) and tag:\n",
        "                        rules[tag] = str(row.get('Prompt Rule', ''))\n",
        "                        skill_id_map[tag] = str(row.get('Skills IDs', ''))\n",
        "\n",
        "                rulebook_data[business_group_name] = {\n",
        "                    \"rules\": rules,\n",
        "                    \"skill_id_map\": skill_id_map,\n",
        "                    \"header_notes\": header_notes\n",
        "                }\n",
        "\n",
        "        print(f\"  ‚úÖ Loaded {len(rulebook_data)} rulebooks\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error loading rulebooks: {e}\")\n",
        "        return group_keyword_map, {}\n",
        "\n",
        "    print(\"‚úÖ All configurations loaded and parsed successfully\")\n",
        "    return group_keyword_map, rulebook_data\n",
        "\n",
        "# Load configurations\n",
        "group_keyword_map, rulebook_data = load_and_parse_configs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SJ1GI1AV1Pc",
        "outputId": "929b2c86-2908-424d-e5f2-a9eebdcbe319"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ BusinessFinder class defined\n"
          ]
        }
      ],
      "source": [
        "# ============================\n",
        "# CELL 4: BUSINESS FINDER CLASS\n",
        "# ============================\n",
        "\n",
        "class BusinessFinder:\n",
        "    \"\"\"Class to find businesses using Google Places API.\"\"\"\n",
        "\n",
        "    def __init__(self, google_api_key, universal_map):\n",
        "        self.google_api_key = google_api_key\n",
        "        self.universal_map = universal_map\n",
        "        self.places_base_url = \"https://maps.googleapis.com/maps/api/place\"\n",
        "\n",
        "    def determine_business_group(self, query):\n",
        "        \"\"\"Determine the business group based on query keywords.\"\"\"\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        # Check for exact matches first\n",
        "        for keyword, group in self.universal_map.items():\n",
        "            if keyword in query_lower:\n",
        "                return group\n",
        "\n",
        "        # If no exact match, try partial matches\n",
        "        for keyword, group in self.universal_map.items():\n",
        "            if any(word in query_lower for word in keyword.split()):\n",
        "                return group\n",
        "\n",
        "        return None\n",
        "\n",
        "    def find_businesses(self, query: str, max_results: int = 15):\n",
        "        \"\"\"Find businesses using Google Places API.\"\"\"\n",
        "        business_group = self.determine_business_group(query)\n",
        "        if not business_group:\n",
        "            print(f\"‚ö†Ô∏è Could not determine a business group for query: '{query}'\")\n",
        "            return [], None\n",
        "\n",
        "        print(f\"\\nüîé Identified Business Group: '{business_group}'\")\n",
        "        print(f\"üîç Searching Google Places for '{query}'...\")\n",
        "\n",
        "        try:\n",
        "            params = {\n",
        "                'query': query,\n",
        "                'key': self.google_api_key,\n",
        "                'type': 'establishment'\n",
        "            }\n",
        "\n",
        "            response = requests.get(f\"{self.places_base_url}/textsearch/json\", params=params)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            if data.get('status') != 'OK':\n",
        "                print(f\"‚ö†Ô∏è Google Places API returned status: {data.get('status')}\")\n",
        "                return [], business_group\n",
        "\n",
        "            businesses = []\n",
        "            place_ids = set()\n",
        "\n",
        "            # Get more initial results to filter for websites\n",
        "            for place in data.get(\"results\", [])[:max_results * 2]:\n",
        "                place_id = place.get(\"place_id\")\n",
        "                if not place_id or place_id in place_ids:\n",
        "                    continue\n",
        "\n",
        "                details = self._get_details(place_id)\n",
        "                if details.get(\"website\"):\n",
        "                    businesses.append(details)\n",
        "                    place_ids.add(place_id)\n",
        "\n",
        "                if len(businesses) >= max_results:\n",
        "                    break\n",
        "\n",
        "            print(f\"‚úÖ Found {len(businesses)} businesses with websites\")\n",
        "            return businesses, business_group\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"‚ùå Error with Google Places API: {e}\")\n",
        "            return [], business_group\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Unexpected error in find_businesses: {e}\")\n",
        "            return [], business_group\n",
        "\n",
        "    def _get_details(self, place_id):\n",
        "        \"\"\"Get detailed information for a specific place.\"\"\"\n",
        "        if not place_id:\n",
        "            return {}\n",
        "\n",
        "        try:\n",
        "            params = {\n",
        "                'place_id': place_id,\n",
        "                'fields': 'name,website,formatted_address,business_status',\n",
        "                'key': self.google_api_key\n",
        "            }\n",
        "\n",
        "            response = requests.get(f\"{self.places_base_url}/details/json\", params=params)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            if data.get('status') == 'OK':\n",
        "                return data.get('result', {})\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Details API returned status: {data.get('status')} for place_id: {place_id}\")\n",
        "                return {}\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"‚ùå Error getting place details: {e}\")\n",
        "            return {}\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Unexpected error in _get_details: {e}\")\n",
        "            return {}\n",
        "\n",
        "print(\"‚úÖ BusinessFinder class defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BL1bH8-IV4YW",
        "outputId": "11b135b9-5da3-4116-9f78-26bf192940b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ WebsiteAnalyzer class defined\n"
          ]
        }
      ],
      "source": [
        "# ============================\n",
        "# CELL 5: WEBSITE ANALYZER CLASS\n",
        "# ============================\n",
        "\n",
        "class WebsiteAnalyzer:\n",
        "    \"\"\"Class to analyze websites and classify business skills.\"\"\"\n",
        "\n",
        "    def __init__(self, llm_instance, rulebook_data):\n",
        "        self.llm_instance = llm_instance\n",
        "        self.rulebook_data = rulebook_data\n",
        "\n",
        "        # Create the prompt template\n",
        "        self.prompt_template = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are an expert business classification analyst. Analyze the website text based on the provided rules.\n",
        "\n",
        "**PRIORITY 1: GLOBAL SPECIAL RULES**\n",
        "{special_rules}\n",
        "\n",
        "**PRIORITY 2: INDUSTRY-SPECIFIC RULES**\n",
        "{dynamic_rules}\n",
        "\n",
        "**IMPORTANT INSTRUCTIONS:**\n",
        "- Read the website text carefully\n",
        "- Apply the rules in order of priority\n",
        "- Only return skills that have clear evidence in the text\n",
        "- Be conservative - if unsure, don't include the skill\n",
        "\n",
        "**OUTPUT FORMAT:**\n",
        "Return a single JSON object with one key: \"applied_skills\". The value must be a list of strings, with each string being an EXACT match from the rule names provided above. If no skills apply, return an empty list.\n",
        "\n",
        "Example:\n",
        "{{\"applied_skills\": [\"skill1\", \"skill2\"]}}\n",
        "\n",
        "**WEBSITE TEXT TO ANALYZE:**\n",
        "{context}\n",
        "\"\"\")\n",
        "\n",
        "        # Create the chain\n",
        "        self.chain = self.prompt_template | llm_instance | StrOutputParser()\n",
        "\n",
        "    def scrape_and_analyze(self, url: str, business_group: str):\n",
        "        \"\"\"Scrape website and analyze for business skills.\"\"\"\n",
        "        print(f\"  -> Scraping and analyzing: {url}\")\n",
        "\n",
        "        # Scrape the website\n",
        "        soup = self._get_soup_from_url(url)\n",
        "        if not soup:\n",
        "            return {\"careers_page\": \"\", \"skill_names\": []}\n",
        "\n",
        "        # Find careers page\n",
        "        careers_page = self._find_careers_page(soup, url)\n",
        "\n",
        "        # Get clean text for analysis\n",
        "        scraped_text = self._get_clean_text(soup)\n",
        "\n",
        "        # Classify with AI\n",
        "        skill_names = self._classify_with_ai(scraped_text, business_group)\n",
        "\n",
        "        return {\n",
        "            \"careers_page\": careers_page,\n",
        "            \"skill_names\": skill_names\n",
        "        }\n",
        "\n",
        "    def _get_soup_from_url(self, url):\n",
        "        \"\"\"Get BeautifulSoup object from URL, with fallback to Selenium.\"\"\"\n",
        "        # Try requests first (faster)\n",
        "        try:\n",
        "            headers = {\n",
        "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, timeout=10)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                return BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"  - Requests failed for {url}: {e}\")\n",
        "\n",
        "        # Fallback to Selenium\n",
        "        print(f\"  - Falling back to Selenium for: {url}\")\n",
        "        try:\n",
        "            driver = get_driver()\n",
        "            if not driver:\n",
        "                return None\n",
        "\n",
        "            driver.get(url)\n",
        "            time.sleep(3)  # Wait for page to load\n",
        "\n",
        "            return BeautifulSoup(driver.page_source, 'html.parser')\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  - Selenium failed for {url}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _get_clean_text(self, soup):\n",
        "        \"\"\"Extract and clean text from BeautifulSoup object.\"\"\"\n",
        "        # Remove unwanted elements\n",
        "        for tag in soup.select('nav, header, footer, script, style, noscript, iframe'):\n",
        "            tag.decompose()\n",
        "\n",
        "        # Get text and clean it\n",
        "        text = soup.get_text(\" \", strip=True)\n",
        "\n",
        "        # Remove excessive whitespace\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        # Limit text length to avoid token limits\n",
        "        if len(text) > 10000:\n",
        "            text = text[:10000] + \"...\"\n",
        "\n",
        "        return text\n",
        "\n",
        "    def _find_careers_page(self, soup, base_url):\n",
        "        \"\"\"Find the careers page URL from the website.\"\"\"\n",
        "        best_link = \"\"\n",
        "        highest_score = 0\n",
        "\n",
        "        for a in soup.find_all(\"a\", href=True):\n",
        "            if not a.get('href'):\n",
        "                continue\n",
        "\n",
        "            href = a.get('href')\n",
        "            text = a.get_text().lower().strip()\n",
        "\n",
        "            # Direct text match\n",
        "            score = 0\n",
        "            if any(keyword in text for keyword in CAREER_KEYWORDS):\n",
        "                score = 100\n",
        "\n",
        "            # URL path analysis\n",
        "            if not score:\n",
        "                try:\n",
        "                    parsed_url = urlparse(href)\n",
        "                    path_parts = parsed_url.path.lower().split('/')\n",
        "\n",
        "                    for part in path_parts:\n",
        "                        if not part:\n",
        "                            continue\n",
        "\n",
        "                        # Check for career-related terms in URL\n",
        "                        part_clean = part.replace('-', ' ').replace('_', ' ')\n",
        "                        for keyword in CAREER_KEYWORDS:\n",
        "                            similarity = fuzz.ratio(part_clean, keyword)\n",
        "                            if similarity > 80:\n",
        "                                score = max(score, similarity)\n",
        "\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "            # Avoid third-party job sites\n",
        "            if score > highest_score:\n",
        "                if not any(site in href.lower() for site in THIRD_PARTY_JOB_SITES):\n",
        "                    highest_score = score\n",
        "                    best_link = urljoin(base_url, href)\n",
        "\n",
        "        # Clean the URL (remove fragments and query params)\n",
        "        if best_link:\n",
        "            best_link = best_link.split('#')[0].split('?')[0]\n",
        "\n",
        "        return best_link\n",
        "\n",
        "    def _classify_with_ai(self, text_content, business_group):\n",
        "        \"\"\"Classify business skills using AI.\"\"\"\n",
        "        if not text_content.strip():\n",
        "            return []\n",
        "\n",
        "        if not business_group or business_group not in self.rulebook_data:\n",
        "            print(f\"  - No rulebook found for business group: {business_group}\")\n",
        "            return []\n",
        "\n",
        "        rules = self.rulebook_data[business_group]\n",
        "\n",
        "        # Format the rules for the prompt\n",
        "        prompt_rules = []\n",
        "        for tag, rule in rules['rules'].items():\n",
        "            prompt_rules.append(f\"- {tag}:\\n  {rule}\")\n",
        "\n",
        "        prompt_rules_str = \"\\n\".join(prompt_rules)\n",
        "\n",
        "        # Get special rules\n",
        "        special_notes = rules['header_notes'].get('SPECIAL TAGS', 'No special rules apply.')\n",
        "\n",
        "        # Try classification with retries\n",
        "        for attempt in range(3):\n",
        "            try:\n",
        "                response_str = self.chain.invoke({\n",
        "                    \"special_rules\": special_notes,\n",
        "                    \"dynamic_rules\": prompt_rules_str,\n",
        "                    \"context\": text_content\n",
        "                })\n",
        "\n",
        "                # Clean up response\n",
        "                if \"```json\" in response_str:\n",
        "                    response_str = response_str.split(\"```json\")[1].split(\"```\")[0].strip()\n",
        "                elif \"```\" in response_str:\n",
        "                    response_str = response_str.split(\"```\")[1].split(\"```\")[0].strip()\n",
        "\n",
        "                # Parse JSON\n",
        "                result = json.loads(response_str)\n",
        "                applied_skills = result.get(\"applied_skills\", [])\n",
        "\n",
        "                # Validate that returned skills exist in our rules\n",
        "                valid_skills = []\n",
        "                for skill in applied_skills:\n",
        "                    if skill in rules['rules']:\n",
        "                        valid_skills.append(skill)\n",
        "                    else:\n",
        "                        print(f\"  - Warning: AI returned invalid skill '{skill}'\")\n",
        "\n",
        "                return valid_skills\n",
        "\n",
        "            except RateLimitError:\n",
        "                wait_time = 2 ** attempt\n",
        "                print(f\"  - Rate limit hit, waiting {wait_time} seconds...\")\n",
        "                time.sleep(wait_time)\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"  - JSON decode error (attempt {attempt + 1}): {e}\")\n",
        "                if attempt == 2:  # Last attempt\n",
        "                    print(f\"  - Raw response: {response_str}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  - AI classification error (attempt {attempt + 1}): {e}\")\n",
        "\n",
        "        return []\n",
        "\n",
        "print(\"‚úÖ WebsiteAnalyzer class defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzHj1CnCV6qz",
        "outputId": "2e60b199-be6d-45a5-dd04-008c29681113"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Main execution function defined\n"
          ]
        }
      ],
      "source": [
        "# ============================\n",
        "# CELL 6: MAIN EXECUTION FUNCTION\n",
        "# ============================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution pipeline.\"\"\"\n",
        "    print(\"üöÄ Starting Business Analysis Pipeline\")\n",
        "\n",
        "    # Validate API keys\n",
        "    if not GOOGLE_API_KEY or GOOGLE_API_KEY == \"YOUR_GOOGLE_API_KEY_HERE\":\n",
        "        print(\"‚ùå CRITICAL ERROR: Google API Key is not set.\")\n",
        "        print(\"   Please set the GOOGLE_API_KEY environment variable or update the configuration.\")\n",
        "        return\n",
        "\n",
        "    if not OPENAI_API_KEY or OPENAI_API_KEY == \"YOUR_OPENAI_API_KEY_HERE\":\n",
        "        print(\"‚ùå CRITICAL ERROR: OpenAI API Key is not set.\")\n",
        "        print(\"   Please set the OPENAI_API_KEY environment variable or update the configuration.\")\n",
        "        return\n",
        "\n",
        "    # Initialize components\n",
        "    finder = BusinessFinder(GOOGLE_API_KEY, group_keyword_map)\n",
        "    analyzer = WebsiteAnalyzer(llm, rulebook_data)\n",
        "\n",
        "    # Get user input\n",
        "    user_query = input(\"\\nüîç Enter your business search query (e.g., 'bakeries in los angeles'): \").strip()\n",
        "    if not user_query:\n",
        "        print(\"‚ùå No query provided. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Find businesses\n",
        "    businesses_to_analyze, business_group = finder.find_businesses(user_query, max_results=15)\n",
        "\n",
        "    if not businesses_to_analyze:\n",
        "        print(\"‚ùå No businesses found or could not determine business group for this query.\")\n",
        "        return\n",
        "\n",
        "    if not business_group:\n",
        "        print(\"‚ùå Could not determine business group for this query.\")\n",
        "        return\n",
        "\n",
        "    # Load existing results to avoid duplicates\n",
        "    try:\n",
        "        df_processed = pd.read_csv(FINAL_OUTPUT_FILE)\n",
        "        processed_websites = set(df_processed['Website'].dropna())\n",
        "        print(f\"üìä Found {len(processed_websites)} previously processed websites\")\n",
        "    except FileNotFoundError:\n",
        "        df_processed = pd.DataFrame()\n",
        "        processed_websites = set()\n",
        "        print(\"üìä No previous results found, starting fresh\")\n",
        "\n",
        "    # Filter out already processed businesses\n",
        "    businesses_to_process = [\n",
        "        b for b in businesses_to_analyze\n",
        "        if b.get('website') and b.get('website') not in processed_websites\n",
        "    ]\n",
        "\n",
        "    if not businesses_to_process:\n",
        "        print(\"‚úÖ All businesses from this search have already been processed.\")\n",
        "        print(f\"üìä Check results in: {FINAL_OUTPUT_FILE}\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nüî¨ Starting analysis for {len(businesses_to_process)} new businesses...\")\n",
        "    print(f\"üìä Business Group: {business_group}\")\n",
        "\n",
        "    # Process each business\n",
        "    new_results = []\n",
        "    for i, business in enumerate(businesses_to_process, 1):\n",
        "        print(f\"\\n--- Processing {i}/{len(businesses_to_process)}: {business.get('name', 'Unknown')} ---\")\n",
        "\n",
        "        try:\n",
        "            # Analyze the website\n",
        "            analysis = analyzer.scrape_and_analyze(business['website'], business_group)\n",
        "\n",
        "            skill_names = analysis.get(\"skill_names\", [])\n",
        "            skill_id_map = rulebook_data[business_group]['skill_id_map']\n",
        "\n",
        "            # Map skill names to IDs\n",
        "            matched_ids = []\n",
        "            for skill in skill_names:\n",
        "                ids = skill_id_map.get(skill, \"\")\n",
        "                if ids:\n",
        "                    matched_ids.extend([s.strip() for s in str(ids).split(',') if s.strip()])\n",
        "\n",
        "            # Create result record\n",
        "            result = {\n",
        "                \"Business_Name\": business.get(\"name\", \"\"),\n",
        "                \"Website\": business.get(\"website\", \"\"),\n",
        "                \"Address\": business.get(\"formatted_address\", \"\"),\n",
        "                \"Business_Group\": business_group,\n",
        "                \"Careers_Page\": analysis.get(\"careers_page\", \"\"),\n",
        "                \"Skill_IDs\": \", \".join(sorted(list(set(matched_ids)))),\n",
        "                \"Skill_Names\": \", \".join(skill_names),\n",
        "                \"Processing_Date\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            }\n",
        "\n",
        "            new_results.append(result)\n",
        "\n",
        "            print(f\"  ‚úÖ Found {len(skill_names)} skills: {', '.join(skill_names)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Error processing {business.get('name', 'Unknown')}: {e}\")\n",
        "\n",
        "            # Add error record\n",
        "            error_result = {\n",
        "                \"Business_Name\": business.get(\"name\", \"\"),\n",
        "                \"Website\": business.get(\"website\", \"\"),\n",
        "                \"Address\": business.get(\"formatted_address\", \"\"),\n",
        "                \"Business_Group\": business_group,\n",
        "                \"Careers_Page\": \"\",\n",
        "                \"Skill_IDs\": \"\",\n",
        "                \"Skill_Names\": f\"ERROR: {str(e)}\",\n",
        "                \"Processing_Date\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            }\n",
        "            new_results.append(error_result)\n",
        "\n",
        "        # Small delay between requests\n",
        "        time.sleep(1)\n",
        "\n",
        "    # Save results\n",
        "    if new_results:\n",
        "        df_new = pd.DataFrame(new_results)\n",
        "\n",
        "        # Combine with existing results\n",
        "        if not df_processed.empty:\n",
        "            # Ensure column alignment\n",
        "            for col in df_new.columns:\n",
        "                if col not in df_processed.columns:\n",
        "                    df_processed[col] = \"\"\n",
        "            for col in df_processed.columns:\n",
        "                if col not in df_new.columns:\n",
        "                    df_new[col] = \"\"\n",
        "\n",
        "            df_final = pd.concat([df_processed, df_new], ignore_index=True)\n",
        "        else:\n",
        "            df_final = df_new\n",
        "\n",
        "        # Save to CSV\n",
        "        df_final.to_csv(FINAL_OUTPUT_FILE, index=False)\n",
        "\n",
        "        print(f\"\\nüéâ Pipeline complete!\")\n",
        "        print(f\"üìä {len(new_results)} new businesses processed\")\n",
        "        print(f\"üìÅ Results saved to: {FINAL_OUTPUT_FILE}\")\n",
        "\n",
        "        # Show summary\n",
        "        print(\"\\n--- Summary of New Results ---\")\n",
        "        successful_results = df_new[~df_new['Skill_Names'].str.contains('ERROR:', na=False)]\n",
        "        if not successful_results.empty:\n",
        "            print(f\"‚úÖ Successfully processed: {len(successful_results)} businesses\")\n",
        "            print(f\"üìä Skills found: {successful_results['Skill_Names'].str.split(', ').explode().nunique()} unique skills\")\n",
        "\n",
        "        error_results = df_new[df_new['Skill_Names'].str.contains('ERROR:', na=False)]\n",
        "        if not error_results.empty:\n",
        "            print(f\"‚ùå Errors encountered: {len(error_results)} businesses\")\n",
        "\n",
        "        print(f\"\\nüìã Sample results:\")\n",
        "        print(df_new[['Business_Name', 'Website', 'Skill_Names']].head().to_string(index=False))\n",
        "\n",
        "    else:\n",
        "        print(\"\\n‚ùå No new results to save.\")\n",
        "\n",
        "def cleanup_driver():\n",
        "    \"\"\"Clean up Selenium driver.\"\"\"\n",
        "    if hasattr(thread_local_driver, 'driver'):\n",
        "        try:\n",
        "            thread_local_driver.driver.quit()\n",
        "            print(\"‚úÖ Selenium driver closed successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error closing Selenium driver: {e}\")\n",
        "\n",
        "print(\"‚úÖ Main execution function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJWU9CBMX0jM",
        "outputId": "05ebcf21-ad4b-4dbb-aa55-c5d434f1d8c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting Business Analysis Pipeline\n",
            "\n",
            "üîç Enter your business search query (e.g., 'bakeries in los angeles'): salon in LA\n",
            "\n",
            "üîé Identified Business Group: 'Beauty, Massage & Spa'\n",
            "üîç Searching Google Places for 'salon in LA'...\n",
            "‚úÖ Found 15 businesses with websites\n",
            "üìä Found 21 previously processed websites\n",
            "\n",
            "üî¨ Starting analysis for 15 new businesses...\n",
            "üìä Business Group: Beauty, Massage & Spa\n",
            "\n",
            "--- Processing 1/15: Arianna Hair Boutique ---\n",
            "  -> Scraping and analyzing: https://arianna-hairboutique.com/\n",
            "  ‚úÖ Found 3 skills: Salon, Women Hair, Makeup\n",
            "\n",
            "--- Processing 2/15: Hairroin Salon ---\n",
            "  -> Scraping and analyzing: http://www.hairroinsalon.com/\n",
            "  ‚úÖ Found 2 skills: Salon, Women Hair\n",
            "\n",
            "--- Processing 3/15: Atelier by Tiffany ---\n",
            "  -> Scraping and analyzing: http://atelierbytiffany.com/\n",
            "  - Requests failed for http://atelierbytiffany.com/: 404 Client Error: Not Found for url: http://atelierbytiffany.com/\n",
            "  - Falling back to Selenium for: http://atelierbytiffany.com/\n",
            "  -  Initializing new STEALTH Selenium driver...\n",
            "  ‚úÖ Selenium driver initialized successfully\n",
            "  ‚úÖ Found 4 skills: Salon, Women Hair, Makeup, Tattoos\n",
            "\n",
            "--- Processing 4/15: ùñ≥ùóÅùñæ ùñßùñ†ùñ®ùñ± PARLOR on 8th ---\n",
            "  -> Scraping and analyzing: https://thehairparloron8th.com/\n",
            "  ‚úÖ Found 2 skills: Salon, Women Hair\n",
            "\n",
            "--- Processing 5/15: Bang Bang LA ---\n",
            "  -> Scraping and analyzing: http://bangbang.la/\n",
            "  ‚úÖ Found 2 skills: Salon, Women Hair\n",
            "\n",
            "--- Processing 6/15: KSY Kim Sun Young Hair & Beauty Salon LA ---\n",
            "  -> Scraping and analyzing: http://www.kimsunyoung.com/\n",
            "  ‚úÖ Found 2 skills: Salon, Women Hair\n",
            "\n",
            "--- Processing 7/15: HAIR Los Angeles ---\n",
            "  -> Scraping and analyzing: http://hairlosangeles.com/\n",
            "  ‚úÖ Found 2 skills: Salon, Women Hair\n",
            "\n",
            "--- Processing 8/15: HAIR BY MINA COLORIST TEAM ---\n",
            "  -> Scraping and analyzing: https://www.hairbymina.com/contact/\n",
            "  ‚úÖ Found 1 skills: Women Hair\n",
            "\n",
            "--- Processing 9/15: Planet Salon ---\n",
            "  -> Scraping and analyzing: https://www.planetsalon.com/\n",
            "  - Requests failed for https://www.planetsalon.com/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "  - Falling back to Selenium for: https://www.planetsalon.com/\n",
            "  ‚úÖ Found 1 skills: Salon\n",
            "\n",
            "--- Processing 10/15: Spoke & Weal ---\n",
            "  -> Scraping and analyzing: https://spokeandweal.com/los-angeles/\n",
            "  ‚úÖ Found 2 skills: Salon, Women Hair\n",
            "\n",
            "--- Processing 11/15: GLOSS & GLAM Hair Salon & Head Spa ---\n",
            "  -> Scraping and analyzing: https://www.glossglamhairspa.com/\n",
            "  ‚úÖ Found 5 skills: Salon, Spa, Women Hair, Makeup, Aestheticians\n",
            "\n",
            "--- Processing 12/15: Hills Beauty Club ---\n",
            "  -> Scraping and analyzing: http://hillsbeauty.com/\n",
            "  ‚úÖ Found 3 skills: Women Hair, Makeup, Aestheticians\n",
            "\n",
            "--- Processing 13/15: Logunova Beauty Salon ---\n",
            "  -> Scraping and analyzing: https://www.logunova.com/?utm_source=gmb&utm_medium=click&utm_campaign=gmb_listing\n",
            "  ‚úÖ Found 7 skills: Salon, Men Hair, Women Hair, Nails, Makeup, Aestheticians, Massage\n",
            "\n",
            "--- Processing 14/15: Hair Arca ---\n",
            "  -> Scraping and analyzing: http://www.arcagenolus.com/\n",
            "  ‚úÖ Found 2 skills: Salon, Women Hair\n",
            "\n",
            "--- Processing 15/15: Sono Felice by Christine ---\n",
            "  -> Scraping and analyzing: https://sonofelicebychristine.com/\n",
            "  - Requests failed for https://sonofelicebychristine.com/: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "  - Falling back to Selenium for: https://sonofelicebychristine.com/\n",
            "  ‚úÖ Found 3 skills: Salon, Men Hair, Women Hair\n",
            "\n",
            "üéâ Pipeline complete!\n",
            "üìä 15 new businesses processed\n",
            "üìÅ Results saved to: Outputs/final_classified_businesses.csv\n",
            "\n",
            "--- Summary of New Results ---\n",
            "‚úÖ Successfully processed: 15 businesses\n",
            "üìä Skills found: 9 unique skills\n",
            "\n",
            "üìã Sample results:\n",
            "         Business_Name                           Website                        Skill_Names\n",
            " Arianna Hair Boutique https://arianna-hairboutique.com/          Salon, Women Hair, Makeup\n",
            "        Hairroin Salon     http://www.hairroinsalon.com/                  Salon, Women Hair\n",
            "    Atelier by Tiffany      http://atelierbytiffany.com/ Salon, Women Hair, Makeup, Tattoos\n",
            "ùñ≥ùóÅùñæ ùñßùñ†ùñ®ùñ± PARLOR on 8th   https://thehairparloron8th.com/                  Salon, Women Hair\n",
            "          Bang Bang LA               http://bangbang.la/                  Salon, Women Hair\n",
            "‚úÖ Selenium driver closed successfully\n",
            "üèÅ Pipeline execution completed\n",
            "‚úÖ Complete pipeline ready for execution\n"
          ]
        }
      ],
      "source": [
        "# ============================\n",
        "# CELL 7: EXECUTION BLOCK\n",
        "# ============================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        main()\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nüõë Process interrupted by user\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Unexpected error: {e}\")\n",
        "    finally:\n",
        "        cleanup_driver()\n",
        "        print(\"üèÅ Pipeline execution completed\")\n",
        "\n",
        "print(\"‚úÖ Complete pipeline ready for execution\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "IACV_RqiX2n9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4no52b_ah5I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojFr4UvCBPiI"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Agki5P8lah2Q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ujfm0thXahzo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zR-LEzY1BLr6"
      },
      "source": [
        "# Ignore below code\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sxHnVVYahxQ",
        "outputId": "3760c31e-7ad5-4516-d146-d302b53f9d99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Installing necessary packages...\n",
            "‚úÖ Packages installed.\n",
            "\n",
            "üöÄ Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "‚úÖ Successfully changed directory to: /content/drive/My Drive/AI_Business_Classifier\n",
            "\n",
            "Files in your project directory:\n",
            ".:\n",
            "Extracted_Business_Groups_and_Types.csv  Outputs  Skill_Tag_Rulebooks\n",
            "\n",
            "./Outputs:\n",
            "final_classified_businesses.csv\n",
            "\n",
            "./Skill_Tag_Rulebooks:\n",
            "'Food & Beverage Establishments.csv'\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# Cell 1: Installations and Google Drive Mount\n",
        "# ==============================================================================\n",
        "print(\"üöÄ Installing necessary packages...\")\n",
        "# Install all necessary packages in quiet mode to keep the output clean\n",
        "!pip install pandas openpyxl requests beautifulsoup4 selenium thefuzz[speedup] openai langchain langchain_openai selenium-stealth -q\n",
        "print(\"‚úÖ Packages installed.\")\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount your Google Drive. This will prompt you for authentication.\n",
        "print(\"\\nüöÄ Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- IMPORTANT: Define the path to your main project folder on Google Drive ---\n",
        "# Make sure this path exactly matches your folder structure.\n",
        "PROJECT_PATH = \"/content/drive/My Drive/AI_Business_Classifier/\"\n",
        "\n",
        "# Check if the path exists\n",
        "if os.path.exists(PROJECT_PATH):\n",
        "    os.chdir(PROJECT_PATH) # Change the current working directory\n",
        "    print(f\"\\n‚úÖ Successfully changed directory to: {os.getcwd()}\")\n",
        "    print(\"\\nFiles in your project directory:\")\n",
        "    # Use !ls -R to recursively list contents, confirming your setup\n",
        "    !ls -R\n",
        "else:\n",
        "    print(f\"‚ùå ERROR: The specified project path does not exist in your Google Drive.\")\n",
        "    print(f\"Please create the folder structure or correct the path: '{PROJECT_PATH}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enK1vJTBas6Z",
        "outputId": "78b884eb-49ab-4927-fccc-b71ccb37fd4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Loading and parsing all configuration files...\n",
            "‚úÖ All configurations loaded and parsed.\n",
            "üîç Enter your business search query (e.g., 'bakeries in los angeles'): bakeries in LA\n",
            "üîé Identified Business Group: 'Food & Beverage Establishments'. Finding businesses...\n",
            "üî¨ Starting analysis for 1 new businesses...\n",
            "\n",
            "-> Analyzing: Black Forest Bakery (https://www.blackforestbakery.com/)\n",
            "  - AI Analysis ERROR: Expecting value: line 1 column 1 (char 0)\n",
            "‚úîÔ∏è Finished & Saved: Black Forest Bakery\n",
            "\n",
            "üéâ Pipeline complete! All results are saved in 'Outputs/final_classified_businesses.csv'\n",
            "\n",
            "‚úÖ Script finished.\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# Cell 2: The Full Pipeline - From Scratch, Simplified and Corrected\n",
        "# ==============================================================================\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import threading\n",
        "from typing import List, Optional, Dict\n",
        "\n",
        "# LangChain, OpenAI, and Scraping Tools\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from openai import RateLimitError\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium_stealth import stealth\n",
        "from thefuzz import fuzz\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# ============================\n",
        "# 1. CONFIGURATION\n",
        "# ============================\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"REMOVED_SECRET_KEY\"\n",
        "GOOGLE_API_KEY = \"AIzaSyBXbfRXA9eJxdL3DmLt3TuDbCtvzP6RWLA\"\n",
        "\n",
        "# --- Using paths relative to the project folder set in Cell 1 ---\n",
        "UNIVERSAL_TAGS_FILE = \"Extracted_Business_Groups_and_Types.csv\"\n",
        "RULEBOOKS_FOLDER = \"Skill_Tag_Rulebooks/\"\n",
        "OUTPUT_FOLDER = \"Outputs/\"\n",
        "FINAL_OUTPUT_FILE = os.path.join(OUTPUT_FOLDER, \"final_classified_businesses.csv\")\n",
        "\n",
        "CAREER_KEYWORDS = [\"career\", \"jobs\", \"employment\", \"hiring\", \"work with us\", \"join us\", \"opportunities\"]\n",
        "THIRD_PARTY_JOB_SITES = [\"indeed.com\", \"linkedin.com/jobs\", \"glassdoor.com\", \"workday.com\"]\n",
        "\n",
        "# ============================\n",
        "# 2. SETUP (LLM and Selenium)\n",
        "# ============================\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "thread_local_driver = threading.local()\n",
        "\n",
        "def get_driver():\n",
        "    if not hasattr(thread_local_driver, 'driver'):\n",
        "        print(\"  -  Initializing new STEALTH Selenium driver...\")\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument(\"--headless\")\n",
        "        chrome_options.add_argument(\"--no-sandbox\")\n",
        "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "        chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "        driver = webdriver.Chrome(options=chrome_options)\n",
        "        stealth(driver, languages=[\"en-US\", \"en\"], vendor=\"Google Inc.\", platform=\"Win32\")\n",
        "        thread_local_driver.driver = driver\n",
        "    return thread_local_driver.driver\n",
        "\n",
        "def close_thread_drivers():\n",
        "    # This function will be called at the end to clean up\n",
        "    if hasattr(thread_local_driver, 'driver'):\n",
        "        thread_local_driver.driver.quit()\n",
        "        del thread_local_driver.driver\n",
        "\n",
        "# ============================\n",
        "# 3. DATA LOADING & PARSING\n",
        "# ============================\n",
        "def load_and_parse_configs():\n",
        "    print(\"üöÄ Loading and parsing all configuration files...\")\n",
        "    df_universal = pd.read_csv(UNIVERSAL_TAGS_FILE)\n",
        "    group_keyword_map = {}\n",
        "    for _, row in df_universal.iterrows():\n",
        "        group, types_str = row['Business Group'], row['Business Type']\n",
        "        if pd.notna(types_str):\n",
        "            for biz_type in [t.strip().lower() for t in types_str.split(',')]:\n",
        "                group_keyword_map[biz_type] = group\n",
        "\n",
        "    rulebook_data = {}\n",
        "    for filename in os.listdir(RULEBOOKS_FOLDER):\n",
        "        if filename.endswith(\".csv\"):\n",
        "            business_group_name = filename.replace(\".csv\", \"\")\n",
        "            df_rulebook = pd.read_csv(os.path.join(RULEBOOKS_FOLDER, filename))\n",
        "            df_rulebook.columns = df_rulebook.columns.str.strip()\n",
        "            rules, skill_id_map = {}, {}\n",
        "            for _, row in df_rulebook.iterrows():\n",
        "                tag, skill_ids = str(row.get('Skills Tags', '')).strip(), str(row.get('Skills IDs', ''))\n",
        "                if pd.notna(tag) and tag and tag.upper() not in ['BUSINESS TYPES', 'SPECIAL TAGS', 'IMPORTANT NOTES']:\n",
        "                    rules[tag] = str(row.get('Prompt Rule', ''))\n",
        "                    if pd.notna(skill_ids): skill_id_map[tag] = skill_ids\n",
        "            rulebook_data[business_group_name] = {\"rules\": rules, \"skill_id_map\": skill_id_map}\n",
        "\n",
        "    print(\"‚úÖ All configurations loaded and parsed.\")\n",
        "    return group_keyword_map, rulebook_data\n",
        "\n",
        "group_keyword_map, rulebook_data = load_and_parse_configs()\n",
        "\n",
        "# ============================\n",
        "# 4. CORE WORKER FUNCTION\n",
        "# ============================\n",
        "\n",
        "def find_and_classify_business(business_info: Dict, business_group: str):\n",
        "    name, website, address = business_info['name'], business_info['website'], business_info['formatted_address']\n",
        "    print(f\"\\n-> Analyzing: {name} ({website})\")\n",
        "\n",
        "    # --- Scrape Website ---\n",
        "    soup = None\n",
        "    try:\n",
        "        response = requests.get(website, headers={\"User-Agent\": \"Mozilla/5.0...\"}, timeout=10)\n",
        "        if response.status_code == 200: soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    except requests.RequestException: pass\n",
        "\n",
        "    if not soup:\n",
        "        print(f\"  - Falling back to Selenium for: {website}\")\n",
        "        try:\n",
        "            driver = get_driver()\n",
        "            driver.get(website)\n",
        "            time.sleep(3)\n",
        "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "        except Exception as e: print(f\"  - Selenium ERROR: {e}\"); return {\"error\": f\"Failed to scrape: {e}\"}\n",
        "\n",
        "    if not soup: return {\"error\": \"Scraping failed completely.\"}\n",
        "\n",
        "    # --- Extract Careers Page ---\n",
        "    best_link, highest_score = \"\", 0\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        score, text_lower = 0, a.get_text().lower()\n",
        "        if any(kw in text_lower for kw in CAREER_KEYWORDS): score = 100\n",
        "        if not score:\n",
        "            try:\n",
        "                path = urlparse(a['href']).path.lower()\n",
        "                if any(fuzz.ratio(path, kw) > 90 for kw in CAREER_KEYWORDS): score = 90\n",
        "            except: pass\n",
        "        if score > highest_score and not any(site in a['href'].lower() for site in THIRD_PARTY_JOB_SITES):\n",
        "            highest_score, best_link = score, urljoin(website, a['href'])\n",
        "    careers_page = best_link.split('#')[0].split('?')[0]\n",
        "\n",
        "    # --- Get Clean Text for AI ---\n",
        "    for tag in soup.select('nav, header, footer, script, style'): tag.decompose()\n",
        "    scraped_text = \" \".join(soup.get_text(strip=True).split())[:12000]\n",
        "\n",
        "    # --- Classify with AI ---\n",
        "    skill_names = []\n",
        "    if scraped_text:\n",
        "        prompt_rules = \"\\n\".join([f\"- {tag}:\\n  {rule}\" for tag, rule in rulebook_data[business_group]['rules'].items()])\n",
        "        prompt = ChatPromptTemplate.from_template(\"You are an expert... {rules} ... {context}\") # Your full prompt here\n",
        "        chain = prompt | llm | StrOutputParser()\n",
        "        try:\n",
        "            response_str = chain.invoke({\"rules\": prompt_rules, \"context\": scraped_text})\n",
        "            if \"```json\" in response_str: response_str = response_str.split(\"```json\")[1].split(\"```\")[0].strip()\n",
        "            skill_names = json.loads(response_str).get(\"applied_skills\", [])\n",
        "            print(f\"  - AI Found Skills: {skill_names}\")\n",
        "        except Exception as e: print(f\"  - AI Analysis ERROR: {e}\")\n",
        "\n",
        "    # --- Map Skill Names to IDs ---\n",
        "    skill_id_map = rulebook_data[business_group]['skill_id_map']\n",
        "    matched_ids = []\n",
        "    for skill in skill_names:\n",
        "        ids = skill_id_map.get(skill)\n",
        "        if ids: matched_ids.extend([s.strip() for s in str(ids).split(',') if s.strip()])\n",
        "\n",
        "    return {\n",
        "        \"Business_Name\": name, \"Website\": website, \"Address\": address,\n",
        "        \"Careers_Page\": careers_page, \"Skill_IDs\": \", \".join(sorted(list(set(matched_ids)))),\n",
        "        \"Skill_Names\": \", \".join(skill_names)\n",
        "    }\n",
        "\n",
        "# ============================\n",
        "# 5. MAIN EXECUTION PIPELINE\n",
        "# ============================\n",
        "def main():\n",
        "    if GOOGLE_API_KEY == \"YOUR_GOOGLE_API_KEY_HERE\" or os.environ.get(\"OPENAI_API_KEY\") == \"YOUR_OPENAI_API_KEY_HERE\":\n",
        "        print(\"‚ùå CRITICAL ERROR: Please set your API Keys in the configuration section.\")\n",
        "        return\n",
        "\n",
        "    user_query = input(\"üîç Enter your business search query (e.g., 'bakeries in los angeles'): \").strip()\n",
        "    if not user_query: return\n",
        "\n",
        "    # Determine Business Group\n",
        "    business_group = None\n",
        "    for keyword, group in group_keyword_map.items():\n",
        "        if keyword in user_query.lower():\n",
        "            business_group = group\n",
        "            break\n",
        "    if not business_group or business_group not in rulebook_data:\n",
        "        print(f\"‚ö†Ô∏è Could not map query to a known business group. Please be more specific.\"); return\n",
        "\n",
        "    # Find Businesses\n",
        "    print(f\"üîé Identified Business Group: '{business_group}'. Finding businesses...\")\n",
        "    params = {'query': user_query, 'key': GOOGLE_API_KEY}\n",
        "    response = requests.get(\"https://maps.googleapis.com/maps/api/place/textsearch/json\", params=params)\n",
        "    places = response.json().get(\"results\", [])\n",
        "\n",
        "    businesses_to_analyze = []\n",
        "    for place in places[:20]:\n",
        "        details_params = {'place_id': place.get('place_id'), 'fields': 'name,website,formatted_address', 'key': GOOGLE_API_KEY}\n",
        "        details_response = requests.get(\"https://maps.googleapis.com/maps/api/place/details/json\", params=details_params)\n",
        "        details = details_response.json().get('result', {})\n",
        "        if details.get(\"website\"): businesses_to_analyze.append(details)\n",
        "\n",
        "    if not businesses_to_analyze: print(\"No businesses with websites found.\"); return\n",
        "\n",
        "    # Load existing results to prevent duplicates\n",
        "    try:\n",
        "        df_processed = pd.read_csv(FINAL_OUTPUT_FILE)\n",
        "        processed_websites = set(df_processed['Website'])\n",
        "    except FileNotFoundError:\n",
        "        df_processed = pd.DataFrame()\n",
        "        processed_websites = set()\n",
        "\n",
        "    businesses_to_process = [b for b in businesses_to_analyze if b.get('website') not in processed_websites]\n",
        "    if not businesses_to_process: print(\"‚úÖ All businesses from this search have already been processed.\"); return\n",
        "    print(f\"üî¨ Starting analysis for {len(businesses_to_process)} new businesses...\")\n",
        "\n",
        "    # Process in Parallel\n",
        "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
        "        future_to_business = {executor.submit(find_and_classify_business, b, business_group): b.get('name') for b in businesses_to_process}\n",
        "\n",
        "        for future in as_completed(future_to_business):\n",
        "            try:\n",
        "                result = future.result()\n",
        "                if \"error\" not in result:\n",
        "                    # Append result and save progress immediately\n",
        "                    temp_df = pd.DataFrame([result])\n",
        "                    if not os.path.exists(FINAL_OUTPUT_FILE):\n",
        "                        temp_df.to_csv(FINAL_OUTPUT_FILE, index=False)\n",
        "                    else:\n",
        "                        temp_df.to_csv(FINAL_OUTPUT_FILE, mode='a', header=False, index=False)\n",
        "                    print(f\"‚úîÔ∏è Finished & Saved: {result['Business_Name']}\")\n",
        "            except Exception as e: print(f\"‚ùå Worker thread error for {future_to_business[future]}: {e}\")\n",
        "\n",
        "    print(f\"\\nüéâ Pipeline complete! All results are saved in '{FINAL_OUTPUT_FILE}'\")\n",
        "\n",
        "# --- Run Pipeline ---\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        main()\n",
        "    finally:\n",
        "        # Final cleanup\n",
        "        close_thread_drivers()\n",
        "        print(\"\\n‚úÖ Script finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQ7DFPAZcbMu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
