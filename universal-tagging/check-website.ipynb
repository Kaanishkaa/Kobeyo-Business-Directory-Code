{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec9d6d0e",
   "metadata": {},
   "source": [
    "# Check if website is being scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dff6868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Attempt 1: Fast Scrape on https://hansolo.com/ ---\n",
      "  - Fast Scrape got 1643 characters.\n",
      "âœ… Fast Scrape was successful. Using this result.\n",
      "\n",
      "==================== SCRAPE RESULT ====================\n",
      "âœ… SUCCESS! Successfully scraped 1643 characters.\n",
      "\n",
      "--- Full scraped text: ---\n",
      "HANSOLO Building Servicesâ„¢ â€“ Tenant Improvements and Commercial Property Maintenance Services FAQs Portfolio Projects Resources Workorder Contact Home HANSOLO Building Servicesâ„¢ â€“ Los Angeles, California Tenant Improvements, Structural Repairs, Concrete Flatwork, Parking Lot Repair and Commercial Property Maintenance services in the Greater Los Angeles California area and surrounding communities. 24-Hour Emergency Response! 2-hour emergency response within Los Angeles County for established clients. We do not provide residential services. Commercial Building Services and Maintenance in Los Angeles Office Remodeling and Tenant Improvements Demolition Services Framing, Drywall and Plaster Work ADA Restrooms Asphalt Repair and Replacement Asphalt Slurry Seal Coating Interior Painting, Exterior and Stucco Concrete and Masonry Services Roofing Repairs and Maintenance Approaches and Driveways Parking Lot Striping and Painting Steel Construction Emergency Water Extraction Waterproofing, Leak Repairs, Testing Pressure Washing Bird Abatement and Maintenance Acoustic Spray and Removal Building Services Commercial Building Services Tenant Improvements Structural Repairs Concrete Flatwork Parking Lot Maintenance HANSOLO Building Services A Full Service Company License #633318 HANSOLO 411 HANSOLO FAQs HANSOLO Client Portfolio HANSOLO Projects Resources of Interest Privacy Policy Workorder Request Contact HANSOLO HANSOLO Home Page 323-277-3070 24/7/365 Emergency Response Â©âˆ HANSOLO Building Servicesâ„¢, Inc. â€¢ 4580 East 49th Street â€¢ Vernon , California 90058 USA â€¢ Tel: 323-277-3070 â€¢ \n",
      "            Fax: 323-277-3074 Ã¢Â–Â² Back to Top\n",
      "=======================================================\n",
      "\n",
      "\n",
      "ğŸ‘‹ Exiting test script.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. SETUP THE SELENIUM DRIVER\n",
    "# ==============================================================================\n",
    "# We only need one driver for this test script.\n",
    "driver = None\n",
    "\n",
    "def initialize_driver():\n",
    "    \"\"\"Initializes a single Selenium driver instance.\"\"\"\n",
    "    global driver\n",
    "    if driver is None:\n",
    "        print(\"ğŸš€ Initializing Selenium driver...\")\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        # Add headers to appear more like a real user\n",
    "        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        print(\"âœ… Driver is ready.\")\n",
    "\n",
    "def close_driver():\n",
    "    \"\"\"Closes the driver if it was initialized.\"\"\"\n",
    "    global driver\n",
    "    if driver:\n",
    "        driver.quit()\n",
    "        driver = None\n",
    "        print(\"âœ… Driver has been closed.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. THE HYBRID SCRAPING FUNCTION\n",
    "# ==============================================================================\n",
    "# This is the exact same function from our main pipeline, for an accurate test.\n",
    "\n",
    "def scrape_website_with_hybrid_approach(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Implements the hybrid scraping strategy. Tries a fast 'requests' scrape first.\n",
    "    If it fails or returns minimal content, it falls back to Selenium.\n",
    "    \"\"\"\n",
    "    page_text = \"\"\n",
    "    MIN_TEXT_LENGTH = 300  # Threshold to decide if the fast scrape was successful\n",
    "\n",
    "    print(f\"\\n--- Attempt 1: Fast Scrape on {url} ---\")\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            page_text = soup.get_text(\" \", strip=True)\n",
    "            print(f\"  - Fast Scrape got {len(page_text)} characters.\")\n",
    "        else:\n",
    "            print(f\"  - Fast Scrape failed with status code: {response.status_code}\")\n",
    "            \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"  - Fast Scrape failed with exception: {e}\")\n",
    "        page_text = \"\"\n",
    "\n",
    "    if len(page_text) > MIN_TEXT_LENGTH:\n",
    "        print(\"âœ… Fast Scrape was successful. Using this result.\")\n",
    "        return page_text\n",
    "\n",
    "    print(f\"\\n--- Attempt 2: Selenium Fallback on {url} ---\")\n",
    "    try:\n",
    "        # Ensure the driver is running before we use it\n",
    "        initialize_driver()\n",
    "        \n",
    "        driver.get(url)\n",
    "        print(\"  - Page requested with Selenium.\")\n",
    "        time.sleep(3) # Wait for JavaScript to render\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        page_text = soup.get_text(\" \", strip=True)\n",
    "        print(f\"  - Selenium Scrape got {len(page_text)} characters.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Selenium Scrape failed with exception: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "    return page_text\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. INTERACTIVE TEST LOOP\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        while True:\n",
    "            # Get URL from user\n",
    "            test_url = input(\"\\nEnter the full website URL to test (or type 'quit' to exit): \").strip()\n",
    "\n",
    "            if test_url.lower() == 'quit':\n",
    "                break\n",
    "            \n",
    "            if not test_url.startswith('http'):\n",
    "                print(\"âš ï¸ Please enter a full URL, including 'http://' or 'https://'.\")\n",
    "                continue\n",
    "\n",
    "            # Run the scraper\n",
    "            scraped_content = scrape_website_with_hybrid_approach(test_url)\n",
    "\n",
    "            # --- Report Results ---\n",
    "            print(\"\\n\" + \"=\"*20 + \" SCRAPE RESULT \" + \"=\"*20)\n",
    "            if scraped_content and len(scraped_content) > 10:\n",
    "                print(f\"âœ… SUCCESS! Successfully scraped {len(scraped_content)} characters.\")\n",
    "                print(\"\\n--- Full scraped text: ---\")\n",
    "                print(scraped_content)\n",
    "            else:\n",
    "                print(\"âŒ FAILED. The scraper could not retrieve meaningful content from this URL.\")\n",
    "            print(\"=\"*55 + \"\\n\")\n",
    "\n",
    "    finally:\n",
    "        # Make sure the driver is closed when the loop ends\n",
    "        close_driver()\n",
    "        print(\"\\nğŸ‘‹ Exiting test script.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cbec31",
   "metadata": {},
   "source": [
    "# Web crawling with email finding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35405aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Crawling starting from https://hansolo.com/\n",
      "Will only crawl pages on domain: hansolo.com\n",
      "\n",
      "ğŸ” Scraping: https://hansolo.com/\n",
      "ğŸ” Scraping: https://hansolo.com/services\n",
      "ğŸ” Scraping: https://hansolo.com/faqs\n",
      "ğŸ” Scraping: https://hansolo.com/portfolio\n",
      "ğŸ” Scraping: https://hansolo.com/projects/\n",
      "ğŸ” Scraping: https://hansolo.com/resources\n",
      "ğŸ” Scraping: https://hansolo.com/workorder\n",
      "ğŸ” Scraping: https://hansolo.com/contact\n",
      "ğŸ” Scraping: https://hansolo.com/tenant-improvements\n",
      "ğŸ” Scraping: https://hansolo.com/structural-repairs\n",
      "ğŸ” Scraping: https://hansolo.com/concrete-flatwork\n",
      "ğŸ” Scraping: https://hansolo.com/parking-lot-maintenance\n",
      "ğŸ” Scraping: https://hansolo.com/privacy\n",
      "ğŸ” Scraping: https://hansolo.com/projects/beach-and-lincoln\n",
      "ğŸ” Scraping: https://hansolo.com/projects/hq\n",
      "ğŸ” Scraping: https://hansolo.com/projects/kling\n",
      "ğŸ” Scraping: https://hansolo.com/projects/safety-net\n",
      "ğŸ” Scraping: https://hansolo.com/projects/silo\n",
      "ğŸ” Scraping: https://hansolo.com/yard\n",
      "ğŸ” Scraping: https://hansolo.com/images/bl/beach-and-lincoln-8020001-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/bl/beach-and-lincoln-8210002-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/bl/beach-and-lincoln-2770-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/bl/beach-and-lincoln-2791-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/bl/beach-and-lincoln-2796-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/bl/beach-and-lincoln-2797-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/bl/beach-and-lincoln-2798-600.jpg\n",
      "  âœ… Found 1 new email(s): 1@9.UC\n",
      "ğŸ” Scraping: https://hansolo.com/images/bl/beach-and-lincoln-2799-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/bl/beach-and-lincoln-2800-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/bl/beach-and-lincoln-2806-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/bl/beach-and-lincoln-2818-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/bl/beach-and-lincoln-2822-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/bl/beach-and-lincoln-2897-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/bl/beach-and-lincoln-2902-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/bl/beach-and-lincoln-2940-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/bl/beach-and-lincoln-2957-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/bl/beach-and-lincoln-2958-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/bl/beach-and-lincoln-3058-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/hq/hq-001-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/hq/hq-002-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/hq/hq-004-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/hq/hq-007-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/hq/hq-008-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/hq/hq-009-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/hq/hq-010-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/hq/hq-011-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/hq/hq-012-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/hq/hq-013-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/hq/hq-014-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/hq/hq-016-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/hq/hq-017-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/hq/hq-018-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/hq/hq-019-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/hq/hq-020-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/hq/hq-022-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/hq/hq-024-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/hq/hq-025-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/hq/hq-027-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/hq/hq-029-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/hq/hq-030-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/hq/hq-032-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/hq/hq-033-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/hq/hq-034-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/hq/hq-035-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/kling/kling-20050401-033-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/kling/kling-20050401-036-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/kling/kling-20050401-040-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/kling/kling-20050401-041-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/kling/kling-20050401-052-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/kling/kling-20050401-070-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/kling/kling-20050401-078-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/safety-net/safety-net-installed-mta-1-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/safety-net/safety-net-installed-mta-2-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/safety-net/safety-net-installed-mta-3-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/safety-net/safety-net-installed-mta-demo-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/safety-net/safety-net-installed-mta-closed-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/silo/silo-foundation-2006-003-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/silo/silo-foundation-2006-004-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/silo/silo-foundation-2006-005-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/silo/silo-foundation-2006-006-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/silo/silo-foundation-2006-007-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/silo/silo-foundation-2006-008-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/silo/silo-foundation-2006-001-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/silo/silo-foundation-2006-002-600.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080421-yard-project-028-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080421-yard-project-023-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080421-yard-project-004-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080414-yard-project-029-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080414-yard-project-017-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080404-yard-project-041-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080404-yard-project-036-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080401-yard-project-043-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080401-yard-project-027-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080401-yard-project-024-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080401-yard-project-013-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080329-yard-project-015-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080327-yard-project-005-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080321-yard-project-025-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080321-yard-project-016-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080321-yard-project-011-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080321-yard-project-020-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080319-yard-project-004-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080310-yard-project-009-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080310-yard-project-008-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080310-yard-project-005-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080310-yard-project-003-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080310-yard-project-001-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080305-yard-project-039-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080222-yard-project-001-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080220-yard-project-020-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080214-yard-project-015-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080208-yard-project-017-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080206-yard-project-005-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080205-yard-project-013-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080201-yard-project-016-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080201-yard-project-001-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080122-yard-project-016-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080122-yard-project-011-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080117-yard-project-014-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080115-yard-project-001-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080110-yard-project-004-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080108-yard-project-021-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20080108-yard-project-003-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20071227-yard-project-057-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20071227-yard-project-017-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20071216-yard-project-044-3x.jpg\n",
      "ğŸ” Scraping: https://hansolo.com/images/yard-project/20071114-yard-project-023-2x.jpg\n",
      "\n",
      "==================== CRAWL COMPLETE ====================\n",
      "Crawl finished in 19.81 seconds.\n",
      "Crawled a total of 126 pages.\n",
      "\n",
      "âœ… SUCCESS! Found 1 unique email address(es):\n",
      "  1. 1@9.UC\n",
      "==========================================================\n",
      "\n",
      "âš ï¸ Please enter a full URL, including 'http://' or 'https://'.\n",
      "\n",
      "ğŸ‘‹ Exiting test script.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import threading\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. SETUP THE SELENIUM DRIVER (Unchanged)\n",
    "# ==============================================================================\n",
    "driver = None\n",
    "\n",
    "def initialize_driver():\n",
    "    \"\"\"Initializes a single Selenium driver instance if not already running.\"\"\"\n",
    "    global driver\n",
    "    if driver is None:\n",
    "        print(\"ğŸš€ Initializing Selenium driver...\")\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        print(\"âœ… Driver is ready.\")\n",
    "\n",
    "def close_driver():\n",
    "    \"\"\"Closes the driver if it was initialized.\"\"\"\n",
    "    global driver\n",
    "    if driver:\n",
    "        driver.quit()\n",
    "        driver = None\n",
    "        print(\"âœ… Driver has been closed.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. MODIFIED HYBRID SCRAPING FUNCTION (Returns full HTML source)\n",
    "# ==============================================================================\n",
    "# This is modified to return the full page source, which we need for\n",
    "# finding links and emails, not just the visible text.\n",
    "\n",
    "def get_page_source_hybrid(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Implements the hybrid scraping strategy. Returns the full page source HTML.\n",
    "    Tries 'requests' first, falls back to Selenium if content is minimal.\n",
    "    \"\"\"\n",
    "    html_source = \"\"\n",
    "    MIN_HTML_LENGTH = 500  # Threshold for raw HTML to decide if scrape was successful\n",
    "\n",
    "    # --- Attempt 1: Fast Scrape (requests) ---\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            html_source = response.text\n",
    "        else:\n",
    "            print(f\"  - Fast Scrape on {url} failed with status code: {response.status_code}\")\n",
    "            \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"  - Fast Scrape on {url} failed with exception: {e}\")\n",
    "        html_source = \"\"\n",
    "\n",
    "    if len(html_source) > MIN_HTML_LENGTH:\n",
    "        return html_source\n",
    "\n",
    "    # --- Attempt 2: Selenium Fallback ---\n",
    "    try:\n",
    "        initialize_driver()\n",
    "        driver.get(url)\n",
    "        time.sleep(3) # Wait for JavaScript to render\n",
    "        html_source = driver.page_source\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Selenium Scrape on {url} failed with exception: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "    return html_source\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. THE CRAWLER & EXTRACTOR LOGIC\n",
    "# ==============================================================================\n",
    "\n",
    "def crawl_and_extract(start_url: str):\n",
    "    \"\"\"\n",
    "    Crawls a website starting from a given URL, scraping every internal page\n",
    "    to find emails and links.\n",
    "    \"\"\"\n",
    "    # Use urlparse to get the domain name, which we'll use to stay on the site\n",
    "    base_netloc = urlparse(start_url).netloc\n",
    "    \n",
    "    # --- Data structures to manage the crawl ---\n",
    "    urls_to_visit = [start_url]  # A queue of pages to crawl\n",
    "    visited_urls = set()         # A set of pages we've already crawled to avoid loops\n",
    "    found_emails = set()         # A set to store unique emails found\n",
    "\n",
    "    print(f\"\\nCrawling starting from {start_url}\")\n",
    "    print(f\"Will only crawl pages on domain: {base_netloc}\\n\")\n",
    "    \n",
    "    while urls_to_visit:\n",
    "        current_url = urls_to_visit.pop(0) # Get the next URL to visit\n",
    "\n",
    "        if current_url in visited_urls:\n",
    "            continue\n",
    "            \n",
    "        print(f\"ğŸ” Scraping: {current_url}\")\n",
    "        visited_urls.add(current_url)\n",
    "\n",
    "        # Get the full HTML of the page using our hybrid function\n",
    "        html_content = get_page_source_hybrid(current_url)\n",
    "\n",
    "        if not html_content:\n",
    "            print(f\"  - Skipping page, no content retrieved.\")\n",
    "            continue\n",
    "        \n",
    "        # --- 1. Extract Emails from the current page ---\n",
    "        # A simple but effective regex for finding emails\n",
    "        email_regex = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "        emails_on_page = re.findall(email_regex, html_content)\n",
    "        \n",
    "        if emails_on_page:\n",
    "            new_emails = set(emails_on_page) - found_emails\n",
    "            if new_emails:\n",
    "                print(f\"  âœ… Found {len(new_emails)} new email(s): {', '.join(new_emails)}\")\n",
    "                found_emails.update(new_emails)\n",
    "\n",
    "        # --- 2. Find all links and add internal ones to the queue ---\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        links_on_page = soup.find_all('a', href=True)\n",
    "\n",
    "        for link_tag in links_on_page:\n",
    "            href = link_tag['href']\n",
    "            # Use urljoin to handle relative links (e.g., '/about-us')\n",
    "            absolute_link = urljoin(current_url, href)\n",
    "            \n",
    "            # Parse the link to check its components\n",
    "            parsed_link = urlparse(absolute_link)\n",
    "\n",
    "            # Check if the link is on the same domain and is an http/https link\n",
    "            if parsed_link.netloc == base_netloc and parsed_link.scheme in ['http', 'https']:\n",
    "                # Clean up fragments (#) and query params (?) for cleaner crawling\n",
    "                clean_link = parsed_link._replace(query=\"\", fragment=\"\").geturl()\n",
    "                \n",
    "                if clean_link not in visited_urls and clean_link not in urls_to_visit:\n",
    "                    urls_to_visit.append(clean_link)\n",
    "    \n",
    "    return found_emails, visited_urls\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. INTERACTIVE TEST LOOP\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        while True:\n",
    "            start_url = input(\"\\nEnter the full website URL to crawl (or 'quit' to exit): \").strip()\n",
    "\n",
    "            if start_url.lower() == 'quit':\n",
    "                break\n",
    "            \n",
    "            if not start_url.startswith('http'):\n",
    "                print(\"âš ï¸ Please enter a full URL, including 'http://' or 'https://'.\")\n",
    "                continue\n",
    "\n",
    "            # Run the crawler\n",
    "            start_time = time.time()\n",
    "            all_emails, all_crawled_urls = crawl_and_extract(start_url)\n",
    "            end_time = time.time()\n",
    "\n",
    "            # --- Report Results ---\n",
    "            print(\"\\n\" + \"=\"*20 + \" CRAWL COMPLETE \" + \"=\"*20)\n",
    "            print(f\"Crawl finished in {end_time - start_time:.2f} seconds.\")\n",
    "            print(f\"Crawled a total of {len(all_crawled_urls)} pages.\")\n",
    "            \n",
    "            if all_emails:\n",
    "                print(f\"\\nâœ… SUCCESS! Found {len(all_emails)} unique email address(es):\")\n",
    "                for i, email in enumerate(sorted(list(all_emails)), 1):\n",
    "                    print(f\"  {i}. {email}\")\n",
    "            else:\n",
    "                print(\"\\nâŒ No email addresses were found on this website.\")\n",
    "            print(\"=\"*58 + \"\\n\")\n",
    "\n",
    "    finally:\n",
    "        # Make sure the driver is closed when the loop ends\n",
    "        close_driver()\n",
    "        print(\"\\nğŸ‘‹ Exiting test script.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82f46ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ•·ï¸  Starting crawl of 'www.marshalls.com'...\n",
      "  - Scraping: https://www.marshalls.com/\n",
      "ğŸš€ Initializing Selenium driver...\n",
      "âœ… Driver is ready.\n",
      "\n",
      "==================== CRAWL RESULT ====================\n",
      "Crawl finished in 16.52 seconds.\n",
      "âŒ SCRAPE FAILED: Little to no text content was found. The site may be blocking scrapers or requires complex interaction.\n",
      "\n",
      "- No email addresses were found during the crawl.\n",
      "========================================================\n",
      "\n",
      "âœ… Driver has been closed.\n",
      "\n",
      "ğŸ‘‹ Exiting test script.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. SETUP THE SELENIUM DRIVER (Unchanged)\n",
    "# ==============================================================================\n",
    "# We only need one driver for the entire crawl session.\n",
    "driver = None\n",
    "\n",
    "def initialize_driver():\n",
    "    \"\"\"Initializes a single Selenium driver instance if not already running.\"\"\"\n",
    "    global driver\n",
    "    if driver is None:\n",
    "        print(\"ğŸš€ Initializing Selenium driver...\")\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        print(\"âœ… Driver is ready.\")\n",
    "\n",
    "def close_driver():\n",
    "    \"\"\"Closes the driver if it was initialized.\"\"\"\n",
    "    global driver\n",
    "    if driver:\n",
    "        driver.quit()\n",
    "        driver = None\n",
    "        print(\"âœ… Driver has been closed.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. MODIFIED HYBRID SCRAPING FUNCTION (Returns full HTML source)\n",
    "# ==============================================================================\n",
    "# We now return the full HTML source to find links and emails, not just text.\n",
    "\n",
    "def get_page_source_hybrid(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Implements the hybrid scraping strategy. Returns the full page source HTML.\n",
    "    Tries 'requests' first, falls back to Selenium if content is minimal.\n",
    "    \"\"\"\n",
    "    html_source = \"\"\n",
    "    MIN_HTML_LENGTH = 500  # Threshold for raw HTML to decide if scrape was successful\n",
    "\n",
    "    # --- Attempt 1: Fast Scrape (requests) ---\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            html_source = response.text\n",
    "        else:\n",
    "            # Don't print for every failed page, it clutters the output.\n",
    "            # print(f\"  - Fast Scrape on {url} failed with status code: {response.status_code}\")\n",
    "            pass\n",
    "            \n",
    "    except requests.RequestException:\n",
    "        html_source = \"\"\n",
    "\n",
    "    if len(html_source) > MIN_HTML_LENGTH:\n",
    "        return html_source\n",
    "\n",
    "    # --- Attempt 2: Selenium Fallback ---\n",
    "    try:\n",
    "        initialize_driver()\n",
    "        driver.get(url)\n",
    "        # A small dynamic wait can be better than a fixed sleep\n",
    "        time.sleep(2) \n",
    "        html_source = driver.page_source\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  - âŒ Selenium Scrape on {url} failed: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "    return html_source\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. NEW: THE CRAWLER & EXTRACTOR LOGIC\n",
    "# ==============================================================================\n",
    "\n",
    "def crawl_and_extract(start_url: str):\n",
    "    \"\"\"\n",
    "    Crawls a website starting from a given URL, scraping every internal page\n",
    "    to find emails and aggregate all text content.\n",
    "    \"\"\"\n",
    "    base_netloc = urlparse(start_url).netloc\n",
    "    \n",
    "    urls_to_visit = [start_url]\n",
    "    visited_urls = set()\n",
    "    found_emails = set()\n",
    "    total_text_scraped = 0\n",
    "\n",
    "    print(f\"\\nğŸ•·ï¸  Starting crawl of '{base_netloc}'...\")\n",
    "    \n",
    "    while urls_to_visit:\n",
    "        current_url = urls_to_visit.pop(0)\n",
    "\n",
    "        if current_url in visited_urls:\n",
    "            continue\n",
    "            \n",
    "        print(f\"  - Scraping: {current_url}\")\n",
    "        visited_urls.add(current_url)\n",
    "\n",
    "        # Get the full HTML of the page using our hybrid function\n",
    "        html_content = get_page_source_hybrid(current_url)\n",
    "\n",
    "        if not html_content:\n",
    "            continue\n",
    "        \n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # --- 1. Aggregate total text content ---\n",
    "        page_text = soup.get_text()\n",
    "        total_text_scraped += len(page_text)\n",
    "\n",
    "        # --- 2. Extract Emails from the current page's HTML ---\n",
    "        email_regex = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "        emails_on_page = re.findall(email_regex, html_content)\n",
    "        if emails_on_page:\n",
    "            found_emails.update(emails_on_page)\n",
    "\n",
    "        # --- 3. Find all links and add internal ones to the queue ---\n",
    "        for link_tag in soup.find_all('a', href=True):\n",
    "            absolute_link = urljoin(current_url, link_tag['href'])\n",
    "            parsed_link = urlparse(absolute_link)\n",
    "\n",
    "            # Check if the link is on the same domain and is http/https\n",
    "            if parsed_link.netloc == base_netloc and parsed_link.scheme in ['http', 'https']:\n",
    "                clean_link = parsed_link._replace(query=\"\", fragment=\"\").geturl()\n",
    "                \n",
    "                if clean_link not in visited_urls and clean_link not in urls_to_visit:\n",
    "                    urls_to_visit.append(clean_link)\n",
    "    \n",
    "    return found_emails, visited_urls, total_text_scraped\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. UPDATED INTERACTIVE TEST LOOP\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        while True:\n",
    "            start_url = input(\"\\nEnter the full website URL to crawl (or 'quit' to exit): \").strip()\n",
    "\n",
    "            if start_url.lower() == 'quit':\n",
    "                break\n",
    "            \n",
    "            if not start_url.startswith('http'):\n",
    "                print(\"âš ï¸ Please enter a full URL, including 'http://' or 'https://'.\")\n",
    "                continue\n",
    "\n",
    "            # Run the new crawler function\n",
    "            start_time = time.time()\n",
    "            all_emails, all_crawled_urls, total_chars = crawl_and_extract(start_url)\n",
    "            end_time = time.time()\n",
    "\n",
    "            # --- Report Comprehensive Results ---\n",
    "            print(\"\\n\" + \"=\"*20 + \" CRAWL RESULT \" + \"=\"*20)\n",
    "            print(f\"Crawl finished in {end_time - start_time:.2f} seconds.\")\n",
    "            \n",
    "            # Report on scrapability (total content found)\n",
    "            if total_chars > 100:\n",
    "                 print(f\"âœ… SCRAPE SUCCESS: Found {total_chars:,} characters of text across {len(all_crawled_urls)} pages.\")\n",
    "            else:\n",
    "                 print(f\"âŒ SCRAPE FAILED: Little to no text content was found. The site may be blocking scrapers or requires complex interaction.\")\n",
    "            \n",
    "            # Report on emails found\n",
    "            if all_emails:\n",
    "                print(f\"\\nğŸ“§ Found {len(all_emails)} unique email address(es):\")\n",
    "                for i, email in enumerate(sorted(list(all_emails)), 1):\n",
    "                    print(f\"  {i}. {email}\")\n",
    "            else:\n",
    "                print(\"\\n- No email addresses were found during the crawl.\")\n",
    "            print(\"=\"*56 + \"\\n\")\n",
    "\n",
    "    finally:\n",
    "        # Make sure the driver is closed when the loop ends\n",
    "        close_driver()\n",
    "        print(\"\\nğŸ‘‹ Exiting test script.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfe3f0c",
   "metadata": {},
   "source": [
    "# Trying with Playwright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3e75c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: playwright in /usr/local/python/3.12.1/lib/python3.12/site-packages (1.53.0)\n",
      "Requirement already satisfied: pyee<14,>=13 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from playwright) (13.0.0)\n",
      "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from playwright) (3.2.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pyee<14,>=13->playwright) (4.13.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "You are using a frozen webkit browser which does not receive updates anymore on ubuntu20.04-x64. Please update to the latest version of your operating system to test up-to-date browsers.\n",
      "Playwright Host validation warning: \n",
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘ Host system is missing dependencies to run browsers. â•‘\n",
      "â•‘ Missing libraries:                                   â•‘\n",
      "â•‘     libwoff2dec.so.1.0.2                             â•‘\n",
      "â•‘     libopus.so.0                                     â•‘\n",
      "â•‘     libwebpdemux.so.2                                â•‘\n",
      "â•‘     libharfbuzz-icu.so.0                             â•‘\n",
      "â•‘     libwebpmux.so.3                                  â•‘\n",
      "â•‘     libenchant-2.so.2                                â•‘\n",
      "â•‘     libhyphen.so.0                                   â•‘\n",
      "â•‘     libEGL.so.1                                      â•‘\n",
      "â•‘     libGLX.so.0                                      â•‘\n",
      "â•‘     libgudev-1.0.so.0                                â•‘\n",
      "â•‘     libevdev.so.2                                    â•‘\n",
      "â•‘     libGLESv2.so.2                                   â•‘\n",
      "â•‘     libx264.so                                       â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "    at validateDependenciesLinux (/usr/local/python/3.12.1/lib/python3.12/site-packages/playwright/driver/package/lib/server/registry/dependencies.js:269:9)\n",
      "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
      "    at async Registry._validateHostRequirements (/usr/local/python/3.12.1/lib/python3.12/site-packages/playwright/driver/package/lib/server/registry/index.js:927:14)\n",
      "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/python/3.12.1/lib/python3.12/site-packages/playwright/driver/package/lib/server/registry/index.js:1049:7)\n",
      "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/python/3.12.1/lib/python3.12/site-packages/playwright/driver/package/lib/server/registry/index.js:1038:7)\n",
      "    at async t.<anonymous> (/usr/local/python/3.12.1/lib/python3.12/site-packages/playwright/driver/package/lib/cli/program.js:217:7)\n"
     ]
    }
   ],
   "source": [
    "! pip install playwright\n",
    "! playwright install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87842ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest_asyncio in /home/codespace/.local/lib/python3.12/site-packages (1.6.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5930964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Initializing Playwright browser...\n",
      "âœ… Browser is ready.\n",
      "\n",
      "ğŸ•·ï¸ Starting crawl of 'hansolo.com'...\n",
      "ğŸ” Scraping: https://hansolo.com/\n",
      "ğŸ” Scraping: https://hansolo.com/projects/\n",
      "ğŸ” Scraping: https://hansolo.com/contact\n",
      "ğŸ” Scraping: https://hansolo.com/projects/beach-and-lincoln\n",
      "ğŸ” Scraping: https://hansolo.com/resources\n",
      "ğŸ” Scraping: https://hansolo.com/tenant-improvements\n",
      "ğŸ” Scraping: https://hansolo.com/faqs\n",
      "ğŸ” Scraping: https://hansolo.com/projects/safety-net\n",
      "ğŸ” Scraping: https://hansolo.com/projects/silo\n",
      "ğŸ” Scraping: https://hansolo.com/parking-lot-maintenance\n",
      "ğŸ” Scraping: https://hansolo.com/projects/hq\n",
      "ğŸ” Scraping: https://hansolo.com/portfolio\n",
      "ğŸ” Scraping: https://hansolo.com/workorder\n",
      "ğŸ” Scraping: https://hansolo.com/services\n",
      "ğŸ” Scraping: https://hansolo.com/privacy\n",
      "ğŸ” Scraping: https://hansolo.com/projects/kling\n",
      "ğŸ” Scraping: https://hansolo.com/structural-repairs\n",
      "ğŸ” Scraping: https://hansolo.com/concrete-flatwork\n",
      "ğŸ” Scraping: https://hansolo.com/yard\n",
      "\n",
      "==================== CRAWL COMPLETE ====================\n",
      "Crawl finished in 1.98 seconds.\n",
      "Crawled a total of 19 pages.\n",
      "\n",
      "âŒ No email addresses were found on this website.\n",
      "==========================================================\n",
      "\n",
      "\n",
      "ğŸ‘‹ Closing browser...\n",
      "âœ… Browser has been closed.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "import requests\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright, Page, Error \n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# --- Constants are unchanged ---\n",
    "IGNORED_EXTENSIONS = {'.pdf', '.jpg', '.jpeg', '.png', '.gif', '.zip', '.rar', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx', '.mp3', '.mp4', '.avi', '.mov'}\n",
    "IMAGE_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.gif', '.svg', '.webp', '.bmp'}\n",
    "\n",
    "# ... (The 'get_page_source_hybrid' and 'crawl_and_extract' functions are perfect, no changes needed) ...\n",
    "\n",
    "async def get_page_source_hybrid(url: str, page: Page) -> str:\n",
    "    html_source = \"\"\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"}\n",
    "        response = requests.get(url, headers=headers, timeout=10, allow_redirects=True)\n",
    "        response.raise_for_status()\n",
    "        content_type = response.headers.get('content-type', '').lower()\n",
    "        if 'text/html' in content_type:\n",
    "            html_source = response.text\n",
    "        else:\n",
    "            # print(f\"  - Skipping non-HTML content at {url}\")\n",
    "            return \"\"\n",
    "    except requests.RequestException as e:\n",
    "        # print(f\"  - Fast scrape on {url} failed: {e}. Falling back to browser.\")\n",
    "        html_source = \"\"\n",
    "    if len(html_source) > 1000:\n",
    "        return html_source\n",
    "    # print(f\"  - Using browser fallback for {url}\")\n",
    "    try:\n",
    "        await page.goto(url, wait_until='networkidle', timeout=20000)\n",
    "        html_source = await page.content()\n",
    "    except Error as e:\n",
    "        print(f\"âŒ Playwright failed on {url}: {e}\")\n",
    "        return \"\"\n",
    "    return html_source\n",
    "\n",
    "async def crawl_and_extract(start_url: str, page: Page):\n",
    "    base_netloc = urlparse(start_url).netloc\n",
    "    urls_to_visit = {start_url}\n",
    "    visited_urls = set()\n",
    "    found_emails = set()\n",
    "    print(f\"\\nğŸ•·ï¸ Starting crawl of '{base_netloc}'...\")\n",
    "    while urls_to_visit:\n",
    "        current_url = urls_to_visit.pop()\n",
    "        if current_url in visited_urls:\n",
    "            continue\n",
    "        print(f\"ğŸ” Scraping: {current_url}\")\n",
    "        visited_urls.add(current_url)\n",
    "        html_content = await get_page_source_hybrid(current_url, page)\n",
    "        if not html_content:\n",
    "            continue\n",
    "        email_regex = r'\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b'\n",
    "        potential_emails = re.findall(email_regex, html_content)\n",
    "        emails_on_page = [\n",
    "            email for email in potential_emails \n",
    "            if not any(email.lower().endswith(ext) for ext in IMAGE_EXTENSIONS)\n",
    "        ]\n",
    "        if emails_on_page:\n",
    "            new_emails_found = set(emails_on_page) - found_emails\n",
    "            if new_emails_found:\n",
    "                print(f\"  âœ… Found {len(new_emails_found)} new email(s): {', '.join(new_emails_found)}\")\n",
    "                found_emails.update(new_emails_found)\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        for link_tag in soup.find_all('a', href=True):\n",
    "            href = link_tag['href']\n",
    "            if href.lower().startswith(('mailto:', 'tel:', 'javascript:')):\n",
    "                continue\n",
    "            absolute_link = urljoin(current_url, href)\n",
    "            parsed_link = urlparse(absolute_link)\n",
    "            path = parsed_link.path\n",
    "            if any(path.lower().endswith(ext) for ext in IGNORED_EXTENSIONS):\n",
    "                continue\n",
    "            if parsed_link.netloc == base_netloc and parsed_link.scheme in ['http', 'https']:\n",
    "                clean_link = parsed_link._replace(query=\"\", fragment=\"\").geturl()\n",
    "                if clean_link not in visited_urls:\n",
    "                    urls_to_visit.add(clean_link)\n",
    "    return found_emails, visited_urls\n",
    "\n",
    "async def main():\n",
    "    async with async_playwright() as p:\n",
    "        print(\"ğŸš€ Initializing Playwright browser...\")\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        context = await browser.new_context(\n",
    "            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "        )\n",
    "        page = await context.new_page()\n",
    "        print(\"âœ… Browser is ready.\")\n",
    "        try:\n",
    "            while True:\n",
    "                start_url = input(\"\\nEnter the full website URL to crawl (or 'quit' to exit): \").strip()\n",
    "                if start_url.lower() == 'quit':\n",
    "                    break\n",
    "                if not start_url.startswith('http'):\n",
    "                    print(\"âš ï¸ Please enter a full URL, including 'http://' or 'https://'.\")\n",
    "                    continue\n",
    "                start_time = time.time()\n",
    "                all_emails, all_crawled_urls = await crawl_and_extract(start_url, page)\n",
    "                end_time = time.time()\n",
    "                print(\"\\n\" + \"=\"*20 + \" CRAWL COMPLETE \" + \"=\"*20)\n",
    "                print(f\"Crawl finished in {end_time - start_time:.2f} seconds.\")\n",
    "                print(f\"Crawled a total of {len(all_crawled_urls)} pages.\")\n",
    "                if all_emails:\n",
    "                    print(f\"\\nâœ… SUCCESS! Found {len(all_emails)} unique email address(es):\")\n",
    "                    for i, email in enumerate(sorted(list(all_emails)), 1):\n",
    "                        print(f\"  {i}. {email}\")\n",
    "                else:\n",
    "                    print(\"\\nâŒ No email addresses were found on this website.\")\n",
    "                print(\"=\"*58 + \"\\n\")\n",
    "        finally:\n",
    "            print(\"\\nğŸ‘‹ Closing browser...\")\n",
    "            await browser.close()\n",
    "            print(\"âœ… Browser has been closed.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. FINAL EXECUTION (The only part that changed)\n",
    "# ==============================================================================\n",
    "# This will correctly run your main() function in a Jupyter/Colab environment.\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4013cea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7bc26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install requests beautifulsoup4 playwright tenacity\n",
    "! playwright install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518d6629",
   "metadata": {},
   "source": [
    "# More robust approach with playwright "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd81389b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Initializing Playwright browser...\n",
      "âœ… Browser is ready.\n",
      "âœ… Successfully read robots.txt from http://colesfrenchdip.com/robots.txt\n",
      "\n",
      "ğŸ•·ï¸ Starting crawl of 'colesfrenchdip.com' with up to 5 concurrent workers.\n",
      "ğŸ” [1/100 | Depth: 0] Scraping: http://colesfrenchdip.com/\n",
      "  âœ… Found 8 new email(s) on http://colesfrenchdip.com/: example@mysite.com, 605a7baede844d278b89dc95ae0a9123@sentry-next.wixpress.com, 78f7996315bc402f9dcb8a2f974b82d1@sentry.wixpress.com, info@colesfrenchdip.com, 88170cb0c9d64f94b5821ca7fd2d55a4@sentry-next.wixpress.com, 5d1795a2db124a268f1e1bd88f503500@sentry.wixpress.com, 18d2f96d279149989b95faf0a4b41882@sentry-next.wixpress.com, 9a65e97ebe8141fca0c4fd686f70996b@sentry.wixpress.com\n",
      "\n",
      "ğŸ Crawl finished or limit reached.\n",
      "\n",
      "==================== CRAWL COMPLETE ====================\n",
      "Crawl finished in 1.49 seconds.\n",
      "Crawled a total of 1 pages.\n",
      "\n",
      "âœ… SUCCESS! Found 8 unique email address(es):\n",
      "  1. 18d2f96d279149989b95faf0a4b41882@sentry-next.wixpress.com\n",
      "  2. 5d1795a2db124a268f1e1bd88f503500@sentry.wixpress.com\n",
      "  3. 605a7baede844d278b89dc95ae0a9123@sentry-next.wixpress.com\n",
      "  4. 78f7996315bc402f9dcb8a2f974b82d1@sentry.wixpress.com\n",
      "  5. 88170cb0c9d64f94b5821ca7fd2d55a4@sentry-next.wixpress.com\n",
      "  6. 9a65e97ebe8141fca0c4fd686f70996b@sentry.wixpress.com\n",
      "  7. example@mysite.com\n",
      "  8. info@colesfrenchdip.com\n",
      "==========================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "import requests\n",
    "import asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright, Page, Error, TimeoutError as PlaywrightTimeoutError\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from collections import deque\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed, retry_if_exception_type\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# ==============================================================================\n",
    "# NEW CONFIGURATION FLAG: Set to False to ignore robots.txt rules\n",
    "RESPECT_ROBOTS_TXT = True  # <-- CHANGE THIS TO False TO SCRAPE ANYWAY\n",
    "\n",
    "MAX_CONCURRENT_REQUESTS = 5\n",
    "MAX_PAGES_TO_CRAWL = 100\n",
    "MAX_CRAWL_DEPTH = 10\n",
    "REQUEST_DELAY = 1\n",
    "BROWSER_TIMEOUT = 25000\n",
    "USER_AGENT = \"My-Email-Scraper-Bot/1.0\"\n",
    "\n",
    "IGNORED_EXTENSIONS = {'.pdf', '.zip', '.rar', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx', '.mp3', '.mp4', '.avi', '.mov'}\n",
    "IMAGE_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.gif', '.svg', '.webp', '.bmp'}\n",
    "\n",
    "# ... get_page_source_hybrid function is unchanged ...\n",
    "@retry(\n",
    "    stop=stop_after_attempt(3),\n",
    "    wait=wait_fixed(2),\n",
    "    retry=retry_if_exception_type((requests.RequestException, PlaywrightTimeoutError)),\n",
    "    reraise=True\n",
    ")\n",
    "async def get_page_source_hybrid(url: str, page: Page) -> str:\n",
    "    html_source = \"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers={\"User-Agent\": USER_AGENT}, timeout=15, allow_redirects=True)\n",
    "        response.raise_for_status()\n",
    "        content_type = response.headers.get('content-type', '').lower()\n",
    "        if 'text/html' in content_type:\n",
    "            html_source = response.text\n",
    "        else:\n",
    "            return \"\"\n",
    "        if len(html_source) > 1000:\n",
    "            return html_source\n",
    "    except requests.RequestException:\n",
    "        pass\n",
    "    await page.goto(url, wait_until='networkidle', timeout=BROWSER_TIMEOUT)\n",
    "    return await page.content()\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. THE CRAWLER CLASS (MODIFIED)\n",
    "# ==============================================================================\n",
    "class Crawler:\n",
    "    def __init__(self, start_url: str):\n",
    "        self.start_url = start_url\n",
    "        self.base_netloc = urlparse(start_url).netloc\n",
    "        self.urls_to_visit = deque([(start_url, 0)])\n",
    "        self.visited_urls = set()\n",
    "        self.found_emails = set()\n",
    "        self.semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)\n",
    "        \n",
    "        # MODIFIED: Only initialize the parser if we respect robots.txt\n",
    "        if RESPECT_ROBOTS_TXT:\n",
    "            self.robot_parser = self._get_robot_parser(start_url)\n",
    "        else:\n",
    "            self.robot_parser = None\n",
    "            print(\"âš ï¸ robots.txt rules are being ignored.\")\n",
    "\n",
    "    def _get_robot_parser(self, url: str) -> RobotFileParser:\n",
    "        # ... this helper function is unchanged ...\n",
    "        rp = RobotFileParser()\n",
    "        robots_url = urljoin(url, '/robots.txt')\n",
    "        try:\n",
    "            rp.set_url(robots_url)\n",
    "            rp.read()\n",
    "            print(f\"âœ… Successfully read robots.txt from {robots_url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Could not read robots.txt, proceeding with caution. Error: {e}\")\n",
    "        return rp\n",
    "\n",
    "    async def _process_page(self, url: str, depth: int, page: Page):\n",
    "        # MODIFIED: Conditionally check robots.txt\n",
    "        if self.robot_parser and not self.robot_parser.can_fetch(USER_AGENT, url):\n",
    "            print(f\"ğŸš« Denied by robots.txt: {url}\")\n",
    "            return\n",
    "        \n",
    "        # ... rest of the function is unchanged ...\n",
    "        try:\n",
    "            html_content = await get_page_source_hybrid(url, page)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to fetch {url} after all retries: {e}\")\n",
    "            return\n",
    "        email_regex = r'\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b'\n",
    "        potential_emails = re.findall(email_regex, html_content, re.IGNORECASE)\n",
    "        clean_emails = {email for email in potential_emails if not any(email.lower().endswith(ext) for ext in IMAGE_EXTENSIONS)}\n",
    "        new_emails = clean_emails - self.found_emails\n",
    "        if new_emails:\n",
    "            print(f\"  âœ… Found {len(new_emails)} new email(s) on {url}: {', '.join(new_emails)}\")\n",
    "            self.found_emails.update(new_emails)\n",
    "        if depth < MAX_CRAWL_DEPTH:\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            for link_tag in soup.find_all('a', href=True):\n",
    "                href = link_tag['href']\n",
    "                if href.lower().startswith(('mailto:', 'tel:', 'javascript:')):\n",
    "                    continue\n",
    "                absolute_link = urljoin(url, href)\n",
    "                parsed_link = urlparse(absolute_link)\n",
    "                path = parsed_link.path.lower()\n",
    "                if (parsed_link.netloc == self.base_netloc and \n",
    "                    parsed_link.scheme in ['http', 'https'] and \n",
    "                    not any(path.endswith(ext) for ext in IGNORED_EXTENSIONS.union(IMAGE_EXTENSIONS))):\n",
    "                    clean_link = parsed_link._replace(query=\"\", fragment=\"\").geturl()\n",
    "                    if clean_link not in self.visited_urls and clean_link not in {u for u, d in self.urls_to_visit}:\n",
    "                        self.urls_to_visit.append((clean_link, depth + 1))\n",
    "    \n",
    "    # ... _worker and run methods are unchanged ...\n",
    "    async def _worker(self, page: Page):\n",
    "        while self.urls_to_visit:\n",
    "            if len(self.visited_urls) >= MAX_PAGES_TO_CRAWL:\n",
    "                break\n",
    "            url, depth = self.urls_to_visit.popleft()\n",
    "            if url in self.visited_urls:\n",
    "                continue\n",
    "            async with self.semaphore:\n",
    "                self.visited_urls.add(url)\n",
    "                print(f\"ğŸ” [{len(self.visited_urls)}/{MAX_PAGES_TO_CRAWL} | Depth: {depth}] Scraping: {url}\")\n",
    "                await self._process_page(url, depth, page)\n",
    "                await asyncio.sleep(REQUEST_DELAY)\n",
    "\n",
    "    async def run(self, page: Page):\n",
    "        print(f\"\\nğŸ•·ï¸ Starting crawl of '{self.base_netloc}' with up to {MAX_CONCURRENT_REQUESTS} concurrent workers.\")\n",
    "        await self._worker(page)\n",
    "        print(\"\\nğŸ Crawl finished or limit reached.\")\n",
    "\n",
    "# ... main execution block is unchanged ...\n",
    "async def main():\n",
    "    async with async_playwright() as p:\n",
    "        print(\"ğŸš€ Initializing Playwright browser...\")\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        context = await browser.new_context(user_agent=USER_AGENT)\n",
    "        page = await context.new_page()\n",
    "        print(\"âœ… Browser is ready.\")\n",
    "        try:\n",
    "            while True:\n",
    "                start_url = input(\"\\nEnter the full website URL to crawl (or 'quit' to exit): \").strip()\n",
    "                if start_url.lower() == 'quit':\n",
    "                    break\n",
    "                if not start_url.startswith('http'):\n",
    "                    print(\"âš ï¸ Please enter a full URL, including 'http://' or 'https://'.\")\n",
    "                    continue\n",
    "                crawler = Crawler(start_url)\n",
    "                start_time = time.time()\n",
    "                await crawler.run(page)\n",
    "                end_time = time.time()\n",
    "                print(\"\\n\" + \"=\"*20 + \" CRAWL COMPLETE \" + \"=\"*20)\n",
    "                print(f\"Crawl finished in {end_time - start_time:.2f} seconds.\")\n",
    "                print(f\"Crawled a total of {len(crawler.visited_urls)} pages.\")\n",
    "                if crawler.found_emails:\n",
    "                    print(f\"\\nâœ… SUCCESS! Found {len(crawler.found_emails)} unique email address(es):\")\n",
    "                    for i, email in enumerate(sorted(list(crawler.found_emails)), 1):\n",
    "                        print(f\"  {i}. {email}\")\n",
    "                else:\n",
    "                    print(\"\\nâŒ No email addresses were found on this website.\")\n",
    "                print(\"=\"*58 + \"\\n\")\n",
    "        finally:\n",
    "            print(\"\\nğŸ‘‹ Closing browser...\")\n",
    "            await browser.close()\n",
    "            print(\"âœ… Browser has been closed.\")\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4f80a6",
   "metadata": {},
   "source": [
    "# Using crawl4ai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4e6ce07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“¦ Install required packages\n",
    "!pip install -q crawl4ai nest-asyncio pandas requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d5e4055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting Advanced Email Scraper...\n",
      "ğŸ’¡ This script uses a hybrid approach to find and categorize emails.\n",
      "ğŸ”„ Using Crawl4AI as primary method for 'www.magnoliabakery.com'...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... â†’ Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.7</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. â†’ Crawl4AI \u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m0\u001b[0m\u001b[36m \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">FETCH</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">... â†“ </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://www.magnoliabakery.com/</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                                      |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">âœ“ | â±: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">6.</span><span style=\"color: #008000; text-decoration-color: #008000\">80s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mFETCH\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m...\u001b[0m\u001b[32m â†“ \u001b[0m\u001b[4;32mhttps://www.magnoliabakery.com/\u001b[0m\u001b[32m                                                                      |\u001b[0m\n",
       "\u001b[32mâœ“\u001b[0m\u001b[32m | â±: \u001b[0m\u001b[1;32m6.\u001b[0m\u001b[32m80s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">SCRAPE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\">.. â—† </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://www.magnoliabakery.com/</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                                      |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">âœ“ | â±: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.</span><span style=\"color: #008000; text-decoration-color: #008000\">32s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mSCRAPE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m.. â—† \u001b[0m\u001b[4;32mhttps://www.magnoliabakery.com/\u001b[0m\u001b[32m                                                                      |\u001b[0m\n",
       "\u001b[32mâœ“\u001b[0m\u001b[32m | â±: \u001b[0m\u001b[1;32m0.\u001b[0m\u001b[32m32s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">COMPLETE</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">]</span><span style=\"color: #008000; text-decoration-color: #008000\"> â— </span><span style=\"color: #008000; text-decoration-color: #008000; text-decoration: underline\">https://www.magnoliabakery.com/</span><span style=\"color: #008000; text-decoration-color: #008000\">                                                                      |</span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">âœ“ | â±: </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">7.</span><span style=\"color: #008000; text-decoration-color: #008000\">13s </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m[\u001b[0m\u001b[32mCOMPLETE\u001b[0m\u001b[1;32m]\u001b[0m\u001b[32m â— \u001b[0m\u001b[4;32mhttps://www.magnoliabakery.com/\u001b[0m\u001b[32m                                                                      |\u001b[0m\n",
       "\u001b[32mâœ“\u001b[0m\u001b[32m | â±: \u001b[0m\u001b[1;32m7.\u001b[0m\u001b[32m13s \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - âŒ Error with Crawl4AI: 'CrawlResult' object has no attribute 'pages_crawled'\n",
      "ğŸ”„ Crawl4AI found fewer emails than target. Trying fast method as fallback...\n",
      "ğŸ•·ï¸  Fast Crawling 'www.magnoliabakery.com'...\n",
      "  - Fast check: https://www.magnoliabakery.com/\n",
      "  - Fast check: https://www.magnoliabakery.com/products/glossier-banana-pudding-balm-dotcom\n",
      "  - Fast check: https://www.magnoliabakery.com/products/blueberry-crisp-pudding-cup\n",
      "  - Fast check: https://www.magnoliabakery.com/pages/us-franchise\n",
      "  - Fast check: https://www.magnoliabakery.com/products/the-magnolia-bakery-handbook-of-icebox-desserts\n",
      "  - Fast check: https://www.magnoliabakery.com/collections/grocery-banana-pudding\n",
      "  - Fast check: https://www.magnoliabakery.com/pages/us-franchising-help-center\n",
      "  - Fast check: https://www.magnoliabakery.com/pages/order-now\n",
      "  - Fast check: https://www.magnoliabakery.com/collections\n",
      "  - Fast check: https://www.magnoliabakery.com/pages/catering-events\n",
      "  - Fast check: https://www.magnoliabakery.com/products/grocery-banana-pudding-wafer-cookie-bits\n",
      "  - Fast check: https://www.magnoliabakery.com/products/grocery-banana-pudding-red-velvet-cookie-bits\n",
      "  - Fast check: https://www.magnoliabakery.com/products/grocery-banana-pudding-chocolatey-hazelnut-swirl\n",
      "  - Fast check: https://www.magnoliabakery.com/pages/gifting-events\n",
      "ğŸ“Š Fast Crawl: Visited 14 pages.\n",
      "\n",
      "==================== CRAWL RESULTS ====================\n",
      "â±ï¸ Time taken: 24.33 seconds\n",
      "ğŸ› ï¸ Method used: Hybrid (Crawl4AI + Fast Fallback)\n",
      "ğŸ“§ Total unique emails found: 0\n",
      "âŒ No email addresses were found.\n",
      "==========================================================\n",
      "\n",
      "âœ… Done.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "import requests\n",
    "import asyncio\n",
    "import csv\n",
    "from typing import Set, List, Tuple, Dict\n",
    "from dataclasses import dataclass\n",
    "from bs4 import BeautifulSoup\n",
    "from crawl4ai import AsyncWebCrawler\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio for Colab/Jupyter compatibility\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. DATA CLASSES & CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "@dataclass\n",
    "class CrawlConfig:\n",
    "    \"\"\"Configuration for crawling parameters.\"\"\"\n",
    "    max_pages: int = 50\n",
    "    stop_after_finding: int = 10  # Stop after finding this many *total* emails\n",
    "    batch_size: int = 5\n",
    "    timeout: int = 5\n",
    "    delay: float = 0.1\n",
    "    crawl4ai_max_pages: int = 30\n",
    "\n",
    "@dataclass\n",
    "class CrawlResult:\n",
    "    \"\"\"Structured result for a crawling operation.\"\"\"\n",
    "    business_url: str\n",
    "    categorized_emails: Dict[str, Set[str]]\n",
    "    pages_crawled: int\n",
    "    time_taken: float\n",
    "    method_used: str\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CORE LOGIC CLASSES\n",
    "# ==============================================================================\n",
    "\n",
    "class EmailDetector:\n",
    "    \"\"\"Handles detection and categorization of emails and priority pages.\"\"\"\n",
    "    IGNORED_EXTENSIONS = {'.pdf', '.jpg', '.jpeg', '.png', '.gif', '.zip', '.rar', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx'}\n",
    "    IMAGE_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.gif', '.svg', '.webp', '.bmp'}\n",
    "\n",
    "    # Keywords for prioritizing pages that are likely to contain contact info\n",
    "    PRIORITY_KEYWORDS = {\"contact\", \"about\", \"team\", \"staff\", \"directory\", \"locations\", \"faq\", \"help\", \"support\"}\n",
    "    HR_KEYWORDS = {\"career\", \"jobs\", \"hr\", \"human-resources\", \"employment\", \"hiring\"}\n",
    "    SALES_KEYWORDS = {\"sales\", \"inquiry\", \"inquiries\", \"info\", \"press\", \"media\", \"business\", \"contact-us\"}\n",
    "    \n",
    "    def get_url_priority(self, href: str, text: str) -> int:\n",
    "        \"\"\"Assigns a priority score to a URL based on its likelihood of containing emails.\"\"\"\n",
    "        content = (href + \" \" + text).lower()\n",
    "        score = 0\n",
    "        if any(key in content for key in self.PRIORITY_KEYWORDS): score += 100\n",
    "        if any(key in content for key in self.HR_KEYWORDS): score += 50\n",
    "        if any(key in content for key in self.SALES_KEYWORDS): score += 50\n",
    "        return score\n",
    "\n",
    "    def categorize_email(self, email: str, context_text: str) -> str:\n",
    "        \"\"\"Categorizes an email as HR, Sales, or Other based on its content and surrounding text.\"\"\"\n",
    "        email_lower = email.lower()\n",
    "        context_lower = context_text.lower()\n",
    "\n",
    "        # Check email address first\n",
    "        if any(key in email_lower for key in self.HR_KEYWORDS): return \"HR\"\n",
    "        if any(key in email_lower for key in self.SALES_KEYWORDS): return \"Sales/Contact\"\n",
    "        \n",
    "        # Check surrounding text for context\n",
    "        if any(key in context_lower for key in self.HR_KEYWORDS): return \"HR\"\n",
    "        if any(key in context_lower for key in self.SALES_KEYWORDS): return \"Sales/Contact\"\n",
    "        \n",
    "        return \"Other\"\n",
    "\n",
    "class LinkAndEmailExtractor:\n",
    "    \"\"\"Extracts links and categorized emails from a single HTML page.\"\"\"\n",
    "    def __init__(self, detector: EmailDetector):\n",
    "        self.detector = detector\n",
    "\n",
    "    def extract(self, html: str, base_url: str) -> Tuple[List[Tuple[str, int]], Dict[str, Set[str]]]:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        page_links_with_priority = []\n",
    "        categorized_emails = {\"HR\": set(), \"Sales/Contact\": set(), \"Other\": set()}\n",
    "        \n",
    "        # Extract Links with Priority\n",
    "        base_netloc = urlparse(base_url).netloc\n",
    "        for tag in soup.find_all(\"a\", href=True):\n",
    "            href, text = tag['href'], tag.get_text().lower().strip()\n",
    "            full_url = urljoin(base_url, href)\n",
    "            parsed = urlparse(full_url)\n",
    "            if parsed.netloc == base_netloc and parsed.scheme in [\"http\", \"https\"]:\n",
    "                if not any(parsed.path.lower().endswith(ext) for ext in self.detector.IGNORED_EXTENSIONS):\n",
    "                    clean_url = parsed._replace(query=\"\", fragment=\"\").geturl()\n",
    "                    priority = self.detector.get_url_priority(href, text)\n",
    "                    page_links_with_priority.append((clean_url, priority))\n",
    "\n",
    "        # Extract and Categorize Emails\n",
    "        page_text = soup.get_text(\" \", strip=True)\n",
    "        email_regex = r'\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b'\n",
    "        for match in re.finditer(email_regex, page_text, re.IGNORECASE):\n",
    "            email = match.group(0)\n",
    "            if any(email.lower().endswith(ext) for ext in self.detector.IMAGE_EXTENSIONS): continue\n",
    "            \n",
    "            context_text = page_text[max(0, match.start() - 75) : match.end() + 75]\n",
    "            category = self.detector.categorize_email(email, context_text)\n",
    "            categorized_emails[category].add(email)\n",
    "            \n",
    "        return page_links_with_priority, categorized_emails\n",
    "\n",
    "class FastEmailCrawler:\n",
    "    \"\"\"Fast, concurrent crawler using requests, designed as a fallback.\"\"\"\n",
    "    def __init__(self, config: CrawlConfig):\n",
    "        self.config = config\n",
    "        self.extractor = LinkAndEmailExtractor(EmailDetector())\n",
    "\n",
    "    async def crawl(self, start_url: str) -> Dict[str, Set[str]]:\n",
    "        urls_to_visit = [(0, start_url)]\n",
    "        visited_urls = set()\n",
    "        found_emails = {\"HR\": set(), \"Sales/Contact\": set(), \"Other\": set()}\n",
    "        \n",
    "        print(f\"ğŸ•·ï¸  Fast Crawling '{urlparse(start_url).netloc}'...\")\n",
    "        while urls_to_visit and len(visited_urls) < self.config.max_pages:\n",
    "            total_found = sum(len(s) for s in found_emails.values())\n",
    "            if total_found >= self.config.stop_after_finding:\n",
    "                print(f\"ğŸ¯ Found {total_found} emails! Stopping fast crawl early.\")\n",
    "                break\n",
    "\n",
    "            urls_to_visit.sort(key=lambda x: -x[0])\n",
    "            batch = [u for p, u in urls_to_visit[:self.config.batch_size] if u not in visited_urls]\n",
    "            urls_to_visit = urls_to_visit[self.config.batch_size:]\n",
    "\n",
    "            if not batch: continue\n",
    "            \n",
    "            tasks = [self._process_url(url, visited_urls) for url in batch]\n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "            for i, res in enumerate(results):\n",
    "                if isinstance(res, Exception): continue\n",
    "                new_links, new_emails = res\n",
    "                for category, emails in new_emails.items():\n",
    "                    found_emails[category].update(emails)\n",
    "                for link, priority in new_links:\n",
    "                    if link not in visited_urls and link not in {u for p, u in urls_to_visit}:\n",
    "                        urls_to_visit.append((priority, link))\n",
    "\n",
    "            await asyncio.sleep(self.config.delay)\n",
    "        \n",
    "        print(f\"ğŸ“Š Fast Crawl: Visited {len(visited_urls)} pages.\")\n",
    "        return found_emails\n",
    "\n",
    "    async def _process_url(self, url: str, visited: set) -> Tuple[List, Dict]:\n",
    "        print(f\"  - Fast check: {url}\")\n",
    "        visited.add(url)\n",
    "        loop = asyncio.get_event_loop()\n",
    "        try:\n",
    "            html = await loop.run_in_executor(None, lambda u: requests.get(u, timeout=self.config.timeout).text, url)\n",
    "            return self.extractor.extract(html, url)\n",
    "        except Exception:\n",
    "            return [], {}\n",
    "\n",
    "class Crawl4AIPrimaryEmailCrawler:\n",
    "    \"\"\"Primary crawler using Crawl4AI for its power and simplicity.\"\"\"\n",
    "    def __init__(self, config: CrawlConfig):\n",
    "        self.config = config\n",
    "        self.detector = EmailDetector()\n",
    "\n",
    "    async def crawl(self, start_url: str) -> Dict[str, Set[str]]:\n",
    "        print(f\"ğŸ”„ Using Crawl4AI as primary method for '{urlparse(start_url).netloc}'...\")\n",
    "        categorized_emails = {\"HR\": set(), \"Sales/Contact\": set(), \"Other\": set()}\n",
    "        \n",
    "        try:\n",
    "            async with AsyncWebCrawler(headless=True, verbose=False) as crawler:\n",
    "                result = await crawler.arun(url=start_url, max_pages=self.config.crawl4ai_max_pages)\n",
    "                \n",
    "                if not result.success:\n",
    "                    print(\"  - Crawl4AI did not succeed.\")\n",
    "                    return categorized_emails\n",
    "\n",
    "                print(f\"  - Crawl4AI finished. Processed {result.pages_crawled} pages.\")\n",
    "                clean_text = result.markdown\n",
    "\n",
    "                for match in re.finditer(EMAIL_REGEX, clean_text, re.IGNORECASE):\n",
    "                    email = match.group(0)\n",
    "                    if any(email.lower().endswith(ext) for ext in self.detector.IMAGE_EXTENSIONS): continue\n",
    "                    context_text = clean_text[max(0, match.start() - 75) : match.end() + 75]\n",
    "                    category = self.detector.categorize_email(email, context_text)\n",
    "                    categorized_emails[category].add(email)\n",
    "                return categorized_emails\n",
    "        except Exception as e:\n",
    "            print(f\"  - âŒ Error with Crawl4AI: {e}\")\n",
    "            return categorized_emails\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. MAIN ORCHESTRATOR CLASS\n",
    "# ==============================================================================\n",
    "\n",
    "class AdvancedEmailScraper:\n",
    "    \"\"\"Orchestrates the crawling process using a hybrid strategy.\"\"\"\n",
    "    def __init__(self, config: CrawlConfig):\n",
    "        self.config = config\n",
    "        self.primary_crawler = Crawl4AIPrimaryEmailCrawler(config)\n",
    "        self.fallback_crawler = FastEmailCrawler(config)\n",
    "\n",
    "    async def scrape_website(self, start_url: str) -> CrawlResult:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Attempt with the powerful Crawl4AI first\n",
    "        categorized_emails = await self.primary_crawler.crawl(start_url)\n",
    "        method_used = \"Crawl4AI\"\n",
    "        \n",
    "        total_found = sum(len(s) for s in categorized_emails.values())\n",
    "        if total_found < self.config.stop_after_finding:\n",
    "            print(\"ğŸ”„ Crawl4AI found fewer emails than target. Trying fast method as fallback...\")\n",
    "            fallback_emails = await self.fallback_crawler.crawl(start_url)\n",
    "            method_used = \"Hybrid (Crawl4AI + Fast Fallback)\"\n",
    "            # Merge results, giving precedence to the fallback's potentially deeper crawl\n",
    "            for category, emails in fallback_emails.items():\n",
    "                categorized_emails[category].update(emails)\n",
    "\n",
    "        return CrawlResult(\n",
    "            business_url=start_url,\n",
    "            categorized_emails=categorized_emails,\n",
    "            pages_crawled=0, # Note: Page count is complex in hybrid, focusing on results\n",
    "            time_taken=time.time() - start_time,\n",
    "            method_used=method_used\n",
    "        )\n",
    "\n",
    "    def display_results(self, result: CrawlResult):\n",
    "        print(\"\\n\" + \"=\"*20 + \" CRAWL RESULTS \" + \"=\"*20)\n",
    "        print(f\"â±ï¸ Time taken: {result.time_taken:.2f} seconds\")\n",
    "        print(f\"ğŸ› ï¸ Method used: {result.method_used}\")\n",
    "\n",
    "        total_emails = sum(len(s) for s in result.categorized_emails.values())\n",
    "        print(f\"ğŸ“§ Total unique emails found: {total_emails}\")\n",
    "        \n",
    "        if total_emails > 0:\n",
    "            print(\"\\n--- Categorized Emails ---\")\n",
    "            for category, emails in result.categorized_emails.items():\n",
    "                if emails:\n",
    "                    print(f\"  - {category}:\")\n",
    "                    for email in sorted(list(emails)):\n",
    "                        print(f\"    - {email}\")\n",
    "            \n",
    "            # Save to CSV\n",
    "            with open(\"scraped_emails.csv\", \"a\", newline='', encoding='utf-8') as f:\n",
    "                writer = csv.writer(f)\n",
    "                if f.tell() == 0:\n",
    "                    writer.writerow([\"Business Website\", \"Email Category\", \"Email Address\"])\n",
    "                for category, emails in result.categorized_emails.items():\n",
    "                    for email in emails:\n",
    "                        writer.writerow([result.business_url, category, email])\n",
    "            print(f\"\\nğŸ“„ Saved {total_emails} emails to scraped_emails.csv\")\n",
    "        else:\n",
    "            print(\"âŒ No email addresses were found.\")\n",
    "        print(\"=\"*58 + \"\\n\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. INTERACTIVE EXECUTION LOOP\n",
    "# ==============================================================================\n",
    "\n",
    "async def main():\n",
    "    print(\"ğŸš€ Starting Advanced Email Scraper...\")\n",
    "    print(\"ğŸ’¡ This script uses a hybrid approach to find and categorize emails.\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            start_url = input(\"\\nEnter website URL (or 'quit' to exit): \").strip()\n",
    "            if start_url.lower() == \"quit\": break\n",
    "            if not start_url.startswith(\"http\"):\n",
    "                print(\"âš ï¸ Please enter a valid full URL with http/https.\")\n",
    "                continue\n",
    "            \n",
    "            stop_count_input = input(f\"How many total emails to find before stopping? (default: 10): \").strip()\n",
    "            stop_after_finding = int(stop_count_input) if stop_count_input else 10\n",
    "\n",
    "            config = CrawlConfig(stop_after_finding=stop_after_finding)\n",
    "            scraper = AdvancedEmailScraper(config)\n",
    "            result = await scraper.scrape_website(start_url)\n",
    "            scraper.display_results(result)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nğŸ‘‹ Interrupted by user.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ An unexpected error occurred: {e}\")\n",
    "\n",
    "    print(\"âœ… Done.\")\n",
    "\n",
    "# Run the main loop\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8dd13c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
