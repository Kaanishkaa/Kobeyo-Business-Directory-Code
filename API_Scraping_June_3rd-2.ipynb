{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install selenium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odvCqKmQdid4",
        "outputId": "a8bec45a-d53c-4e2c-9b3c-c3c7e9d05846"
      },
      "id": "odvCqKmQdid4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (4.33.0)\n",
            "Requirement already satisfied: urllib3~=2.4.0 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: trio~=0.30.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.30.0)\n",
            "Requirement already satisfied: trio-websocket~=0.12.2 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: certifi>=2025.4.26 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.4.26)\n",
            "Requirement already satisfied: typing_extensions~=4.13.2 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.13.2)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bdcdfe7",
      "metadata": {
        "id": "9bdcdfe7"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "import time\n",
        "from urllib.parse import urlparse, urljoin\n",
        "from collections import deque\n",
        "import random\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# API key - replace with your actual API key\n",
        "GOOGLE_MAPS_API_KEY = 'enter your key'\n",
        "# APIs and search parameters\n",
        "TEXT_SEARCH_URL = \"https://maps.googleapis.com/maps/api/place/textsearch/json\"\n",
        "PLACE_DETAILS_URL = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
        "SEARCH_TAGS = [\"pet grooming\"]  # Add more tags as needed\n",
        "SOCIAL_PLATFORMS = [\"facebook.com\", \"instagram.com\", \"twitter.com\", \"linkedin.com\"]\n",
        "\n",
        "# User agents for web scraping\n",
        "USER_AGENTS = [\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
        "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15\",\n",
        "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\",\n",
        "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1\",\n",
        "    \"Mozilla/5.0 (iPad; CPU OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1\"\n",
        "]\n",
        "\n",
        "# ----------------- GOOGLE PLACES API FUNCTIONS -----------------\n",
        "\n",
        "def search_places(query, location=\"Los Angeles\"):\n",
        "    \"\"\"\n",
        "    Search for places using Google Places API\n",
        "    \"\"\"\n",
        "    print(f\"\\nğŸ” Searching for: {query} in {location}\")\n",
        "    params = {\"query\": f\"{query} in {location}\", \"key\": GOOGLE_MAPS_API_KEY}\n",
        "    response = requests.get(TEXT_SEARCH_URL, params=params)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"âŒ API Error: {response.status_code}, {response.text}\")\n",
        "        return []\n",
        "\n",
        "    results = response.json().get(\"results\", [])\n",
        "    place_ids = [r.get(\"place_id\") for r in results]\n",
        "    print(f\"âœ… Found {len(place_ids)} places\")\n",
        "    return place_ids\n",
        "\n",
        "def get_place_details(place_id):\n",
        "    \"\"\"\n",
        "    Get detailed information about a place using its place_id\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        \"place_id\": place_id,\n",
        "        \"fields\": \"name,formatted_address,website,url,formatted_phone_number\",\n",
        "        \"key\": GOOGLE_MAPS_API_KEY\n",
        "    }\n",
        "    response = requests.get(PLACE_DETAILS_URL, params=params)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"âŒ API Error: {response.status_code}, {response.text}\")\n",
        "        return {}\n",
        "\n",
        "    return response.json().get(\"result\", {})\n",
        "\n",
        "# ----------------- WEB SCRAPING FUNCTIONS -----------------\n",
        "\n",
        "def setup_chrome_driver():\n",
        "    \"\"\"\n",
        "    Setup headless Chrome driver for web scraping\n",
        "    \"\"\"\n",
        "    options = Options()\n",
        "    options.add_argument(\"--headless\")\n",
        "    options.add_argument(\"--disable-gpu\")\n",
        "    options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.binary_location = \"/usr/bin/google-chrome\"  # For Colab\n",
        "    options.add_argument(f\"user-agent={random.choice(USER_AGENTS)}\")\n",
        "    return webdriver.Chrome(options=options)\n",
        "\n",
        "\n",
        "    #return webdriver.Chrome(service=Service(\"/usr/bin/chromedriver\"), options=options)\n",
        "def extract_links_and_content(url):\n",
        "    \"\"\"\n",
        "    Extract links and content from a webpage\n",
        "    \"\"\"\n",
        "    try:\n",
        "        headers = {\"User-Agent\": random.choice(USER_AGENTS)}\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # Extract text content\n",
        "        all_text = soup.get_text(separator=\" \", strip=True)\n",
        "\n",
        "        # Extract links\n",
        "        links = [a['href'] for a in soup.find_all('a', href=True)]\n",
        "\n",
        "        return links, all_text\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Scrape error for {url}: {e}\")\n",
        "        return [], \"\"\n",
        "\n",
        "def is_internal_link(link, domain):\n",
        "    \"\"\"\n",
        "    Check if a link is internal to the domain\n",
        "    \"\"\"\n",
        "    return domain in link and not any(bad in link for bad in [\"indeed\", \"glassdoor\", \"ziprecruiter\", \"monster\", \"linkedin\"])\n",
        "\n",
        "def find_social_links(links):\n",
        "    \"\"\"\n",
        "    Find social media links in a list of links\n",
        "    \"\"\"\n",
        "    social_links = []\n",
        "    for link in links:\n",
        "        if any(social in link for social in SOCIAL_PLATFORMS):\n",
        "            social_links.append(link)\n",
        "    return social_links\n",
        "\n",
        "def get_social_from_google(place_result):\n",
        "    \"\"\"\n",
        "    Try to extract social links from Google Places result\n",
        "    \"\"\"\n",
        "    social_links = []\n",
        "    for field in ['url', 'website']:\n",
        "        if field in place_result and place_result[field]:\n",
        "            url = place_result[field]\n",
        "            if any(social in url for social in SOCIAL_PLATFORMS):\n",
        "                social_links.append(url)\n",
        "    return social_links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06074542",
      "metadata": {
        "id": "06074542"
      },
      "outputs": [],
      "source": [
        "def extract_emails(text):\n",
        "    \"\"\"\n",
        "    Extract valid email addresses from text\n",
        "    \"\"\"\n",
        "    pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
        "    emails = re.findall(pattern, text)\n",
        "\n",
        "    # Filter for business/hiring related emails\n",
        "    valid_emails = []\n",
        "    for email in emails:\n",
        "        email = email.lower()\n",
        "        # Include hiring-related and general contact emails\n",
        "        if any(email.startswith(prefix) for prefix in [\"hr@\", \"jobs@\", \"careers@\", \"hiring@\", \"recruiting@\", \"info@\", \"contact@\", \"hello@\"]):\n",
        "            valid_emails.append(email)\n",
        "        # Include Gmail accounts (often used by small businesses)\n",
        "        elif email.endswith(\"@gmail.com\"):\n",
        "            valid_emails.append(email)\n",
        "\n",
        "    return list(set(valid_emails))\n",
        "\n",
        "def detect_job_content(text):\n",
        "    \"\"\"\n",
        "    Detect job-related content in text\n",
        "    \"\"\"\n",
        "    job_keywords = [\n",
        "        \"we're hiring\", \"now hiring\", \"join our team\", \"career\", \"careers\", \"job opening\",\n",
        "        \"position available\", \"apply now\", \"job application\", \"employment\", \"work with us\",\n",
        "        \"job opportunity\", \"open position\", \"join us\"\n",
        "    ]\n",
        "\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    for keyword in job_keywords:\n",
        "        if keyword in text_lower:\n",
        "            # Find the context around the keyword\n",
        "            start = max(0, text_lower.find(keyword) - 100)\n",
        "            end = min(len(text_lower), text_lower.find(keyword) + 100)\n",
        "            context = text[start:end].strip()\n",
        "            return True, context\n",
        "\n",
        "    return False, \"\"\n",
        "\n",
        "def crawl_site(site_url, max_pages=5):\n",
        "    \"\"\"\n",
        "    Crawl a website to find job-related content and contact information\n",
        "    \"\"\"\n",
        "    print(f\"\\nğŸ” Crawling: {site_url}\")\n",
        "\n",
        "    # Setup\n",
        "    visited = set()\n",
        "    queue = deque([site_url])\n",
        "    emails = set()\n",
        "    job_content = []\n",
        "    social_links = []\n",
        "\n",
        "    # Parse domain for filtering internal links\n",
        "    try:\n",
        "        domain = urlparse(site_url).netloc\n",
        "    except:\n",
        "        return [], [], [], \"Invalid URL\"\n",
        "\n",
        "    # Process pages\n",
        "    page_count = 0\n",
        "    while queue and page_count < max_pages:\n",
        "        try:\n",
        "            current_url = queue.popleft()\n",
        "            if current_url in visited:\n",
        "                continue\n",
        "\n",
        "            # Mark as visited\n",
        "            visited.add(current_url)\n",
        "            page_count += 1\n",
        "\n",
        "            print(f\"  Processing page {page_count}/{max_pages}: {current_url}\")\n",
        "\n",
        "            # Extract content\n",
        "            links, content = extract_links_and_content(current_url)\n",
        "\n",
        "            # Extract emails\n",
        "            page_emails = extract_emails(content)\n",
        "            emails.update(page_emails)\n",
        "\n",
        "            # Check for job content\n",
        "            has_job, job_context = detect_job_content(content)\n",
        "            if has_job:\n",
        "                job_content.append({\n",
        "                    \"url\": current_url,\n",
        "                    \"context\": job_context\n",
        "                })\n",
        "\n",
        "            # Find social links\n",
        "            page_social = [link for link in links if any(social in link for social in SOCIAL_PLATFORMS)]\n",
        "            social_links.extend(page_social)\n",
        "\n",
        "            # Add internal links to queue\n",
        "            for link in links:\n",
        "                full_link = urljoin(current_url, link)\n",
        "                parsed_link = urlparse(full_link)\n",
        "\n",
        "                # Only add internal links\n",
        "                if parsed_link.netloc == domain and full_link not in visited:\n",
        "                    queue.append(full_link)\n",
        "\n",
        "            # Add some delay to avoid being blocked\n",
        "            time.sleep(random.uniform(1, 2))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Error processing {current_url}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return list(emails), job_content, list(set(social_links)), \"Success\"\n",
        "\n",
        "# ----------------- MAIN EXECUTION -----------------\n",
        "\n",
        "def process_business(place_id):\n",
        "    \"\"\"\n",
        "    Process a business by its place_id\n",
        "    \"\"\"\n",
        "    # Get place details\n",
        "    details = get_place_details(place_id)\n",
        "    if not details:\n",
        "        return None\n",
        "\n",
        "    name = details.get(\"name\", \"Unknown\")\n",
        "    address = details.get(\"formatted_address\", \"Unknown\")\n",
        "    phone = details.get(\"formatted_phone_number\", \"Unknown\")\n",
        "    website = details.get(\"website\", \"\")\n",
        "    hr_emails = extract_hr_emails(website) if website else [\"N/A\"]\n",
        "\n",
        "    print(f\"\\nğŸ“‹ Processing: {name}\")\n",
        "    print(f\"  ğŸ“ Address: {address}\")\n",
        "    print(f\"  ğŸ“ Phone: {phone}\")\n",
        "    print(f\"  ğŸŒ Website: {website}\")\n",
        "    print(f\"  ğŸŒ HR Mail: {hr_emails}\")\n",
        "\n",
        "    emails = []\n",
        "    job_content = []\n",
        "    social_links = []\n",
        "    crawl_status = \"Not crawled\"\n",
        "\n",
        "    # Crawl website if available\n",
        "    if website:\n",
        "        try:\n",
        "            emails, job_content, social_links, crawl_status = crawl_site(website)\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Error crawling website: {e}\")\n",
        "\n",
        "    # Check Google-provided social links if no website or job content found\n",
        "    if not website or not job_content:\n",
        "        google_social = get_social_from_google(details)\n",
        "\n",
        "        # Add to social links list\n",
        "        social_links.extend(google_social)\n",
        "\n",
        "        # Crawl first social link if available and no job content found\n",
        "        if google_social and not job_content:\n",
        "            try:\n",
        "                social_emails, social_job_content, _, _ = crawl_site(google_social[0])\n",
        "                emails.extend(social_emails)\n",
        "                job_content.extend(social_job_content)\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Error crawling social link: {e}\")\n",
        "\n",
        "    # Format job content for CSV\n",
        "    has_job_posting = len(job_content) > 0\n",
        "    job_urls = [item[\"url\"] for item in job_content]\n",
        "    job_contexts = [item[\"context\"] for item in job_content]\n",
        "\n",
        "    result = {\n",
        "        \"name\": name,\n",
        "        \"address\": address,\n",
        "        \"phone\": phone,\n",
        "        \"website\": website,\n",
        "        \"emails\": \"; \".join(emails) if emails else \"None found\",\n",
        "        \"HR Emails\": \",\".join(hr_emails),\n",
        "        \"has_job_posting\": \"Yes\" if has_job_posting else \"No\",\n",
        "        \"job_urls\": \"; \".join(job_urls) if job_urls else \"None found\",\n",
        "        \"job_contexts\": \"; \".join(job_contexts) if job_contexts else \"None found\",\n",
        "        \"social_links\": \"; \".join(social_links) if social_links else \"None found\",\n",
        "        \"crawl_status\": crawl_status\n",
        "    }\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from selenium.common.exceptions import WebDriverException\n",
        "\n",
        "def extract_hr_emails(website_url):\n",
        "    \"\"\"\n",
        "    Extract HR-related emails from a given website URL\n",
        "    \"\"\"\n",
        "    try:\n",
        "        driver = setup_chrome_driver()\n",
        "        driver.get(website_url)\n",
        "        page_text = driver.page_source\n",
        "        driver.quit()\n",
        "\n",
        "        # Find all email addresses\n",
        "        all_emails = re.findall(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", page_text)\n",
        "        unique_emails = list(set(all_emails))\n",
        "\n",
        "        # Filter HR-related emails\n",
        "        hr_keywords = ['hr', 'people', 'talent', 'careers', 'recruit', 'jobs']\n",
        "        hr_emails = [email for email in unique_emails if any(kw in email.lower() for kw in hr_keywords)]\n",
        "\n",
        "        return hr_emails if hr_emails else [\"Not Found\"]\n",
        "\n",
        "    except WebDriverException as e:\n",
        "        print(f\"Error accessing {website_url}: {e}\")\n",
        "        return [\"Error\"]"
      ],
      "metadata": {
        "id": "x1GNcHdIIzQT"
      },
      "id": "x1GNcHdIIzQT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6471acee",
      "metadata": {
        "id": "6471acee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50a49659-7d89-4701-b8f8-648a710bef59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ” Searching for: pet grooming in Los Angeles\n",
            "âœ… Found 20 places\n",
            "Error accessing https://www.21pooch.com/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "ğŸ“‹ Processing: 21Pooch Hollywood\n",
            "  ğŸ“ Address: 5137 W Sunset Blvd Unit 213, 2nd floor, Los Angeles, CA 90027, USA\n",
            "  ğŸ“ Phone: (310) 290-3905\n",
            "  ğŸŒ Website: https://www.21pooch.com/\n",
            "  ğŸŒ HR Mail: ['Error']\n",
            "\n",
            "ğŸ” Crawling: https://www.21pooch.com/\n",
            "  Processing page 1/5: https://www.21pooch.com/\n",
            "  Processing page 2/5: https://www.21pooch.com\n",
            "  Processing page 3/5: https://www.21pooch.com/price\n",
            "  Processing page 4/5: https://www.21pooch.com/book-now\n",
            "  Processing page 5/5: https://www.21pooch.com/contacts\n",
            "Error accessing https://www.21pooch.com/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "ğŸ“‹ Processing: 21Pooch Dog & Cat grooming salon\n",
            "  ğŸ“ Address: 1061 S Broadway, Los Angeles, CA 90015, USA\n",
            "  ğŸ“ Phone: (213) 589-8304\n",
            "  ğŸŒ Website: https://www.21pooch.com/\n",
            "  ğŸŒ HR Mail: ['Error']\n",
            "\n",
            "ğŸ” Crawling: https://www.21pooch.com/\n",
            "  Processing page 1/5: https://www.21pooch.com/\n",
            "  Processing page 2/5: https://www.21pooch.com\n",
            "  Processing page 3/5: https://www.21pooch.com/price\n",
            "  Processing page 4/5: https://www.21pooch.com/book-now\n",
            "  Processing page 5/5: https://www.21pooch.com/contacts\n",
            "Error accessing https://groomla.com/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "ğŸ“‹ Processing: Groom - Dog Care With Love\n",
            "  ğŸ“ Address: 8749 Holloway Dr, West Hollywood, CA 90069, USA\n",
            "  ğŸ“ Phone: (310) 492-5222\n",
            "  ğŸŒ Website: https://groomla.com/\n",
            "  ğŸŒ HR Mail: ['Error']\n",
            "\n",
            "ğŸ” Crawling: https://groomla.com/\n",
            "  Processing page 1/5: https://groomla.com/\n",
            "  Processing page 2/5: https://groomla.com/health-and-safety\n",
            "  Processing page 3/5: https://groomla.com/services/dog-daycare\n",
            "  Processing page 4/5: https://groomla.com/services/dog-grooming\n",
            "  Processing page 5/5: https://groomla.com/services/deluxe-dog-bath\n",
            "Error accessing https://www.theurbanpet.net/self-wash-pods-and-full-service-grooming-at-moonshine-grooming/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "ğŸ“‹ Processing: Moon Shine Grooming & Pet Wash\n",
            "  ğŸ“ Address: 4473 Santa Monica Blvd, Los Angeles, CA 90029, USA\n",
            "  ğŸ“ Phone: (323) 664-4600\n",
            "  ğŸŒ Website: https://www.theurbanpet.net/self-wash-pods-and-full-service-grooming-at-moonshine-grooming/\n",
            "  ğŸŒ HR Mail: ['Error']\n",
            "\n",
            "ğŸ” Crawling: https://www.theurbanpet.net/self-wash-pods-and-full-service-grooming-at-moonshine-grooming/\n",
            "  Processing page 1/5: https://www.theurbanpet.net/self-wash-pods-and-full-service-grooming-at-moonshine-grooming/\n",
            "  Processing page 2/5: https://www.theurbanpet.net/self-wash-pods-and-full-service-grooming-at-moonshine-grooming/contact-for-pet-grooming\n",
            "  Processing page 3/5: https://www.theurbanpet.net/self-wash-pods-and-full-service-grooming-at-moonshine-grooming/upcoming-events\n",
            "  Processing page 4/5: https://www.theurbanpet.net/self-wash-pods-and-full-service-grooming-at-moonshine-grooming/membership\n",
            "  Processing page 5/5: https://www.theurbanpet.net/\n",
            "Error accessing http://pourlapooch.com/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "ğŸ“‹ Processing: Pour La Pooch\n",
            "  ğŸ“ Address: 7617 Beverly Blvd, Los Angeles, CA 90036, USA\n",
            "  ğŸ“ Phone: (323) 934-0940\n",
            "  ğŸŒ Website: http://pourlapooch.com/\n",
            "  ğŸŒ HR Mail: ['Error']\n",
            "\n",
            "ğŸ” Crawling: http://pourlapooch.com/\n",
            "  Processing page 1/5: http://pourlapooch.com/\n",
            "  Processing page 2/5: http://pourlapooch.com/dog-daycare.html\n",
            "  Processing page 3/5: http://pourlapooch.com/photos.html\n",
            "  Processing page 4/5: http://pourlapooch.com/map--hours.html\n",
            "  Processing page 5/5: http://pourlapooch.com/contact-us.html\n",
            "Error accessing https://www.alessandropetspa.com/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "ğŸ“‹ Processing: Alessandro Pet Spa\n",
            "  ğŸ“ Address: 7221 Melrose Ave, Los Angeles, CA 90046, USA\n",
            "  ğŸ“ Phone: (323) 644-4946\n",
            "  ğŸŒ Website: https://www.alessandropetspa.com/\n",
            "  ğŸŒ HR Mail: ['Error']\n",
            "\n",
            "ğŸ” Crawling: https://www.alessandropetspa.com/\n",
            "  Processing page 1/5: https://www.alessandropetspa.com/\n",
            "  Processing page 2/5: https://www.alessandropetspa.com/about\n",
            "  Processing page 3/5: https://www.alessandropetspa.com/dog-grooming\n",
            "  Processing page 4/5: https://www.alessandropetspa.com/cat-grooming\n",
            "  Processing page 5/5: https://www.alessandropetspa.com/dog-bathing\n",
            "Error accessing http://www.bubblepupsalon.com/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "ğŸ“‹ Processing: Bubble Pup Salon\n",
            "  ğŸ“ Address: 333 S Alameda St STE 201, Los Angeles, CA 90013, USA\n",
            "  ğŸ“ Phone: (213) 558-9355\n",
            "  ğŸŒ Website: http://www.bubblepupsalon.com/\n",
            "  ğŸŒ HR Mail: ['Error']\n",
            "\n",
            "ğŸ” Crawling: http://www.bubblepupsalon.com/\n",
            "  Processing page 1/5: http://www.bubblepupsalon.com/\n",
            "  Processing page 2/5: https://www.bubblepupsalon.com/book-online-2\n",
            "  Processing page 3/5: https://www.bubblepupsalon.com\n",
            "  Processing page 4/5: https://www.bubblepupsalon.com/book-online-1\n",
            "  Processing page 5/5: https://www.bubblepupsalon.com/general-5\n",
            "Error accessing https://www.instagram.com/losangelespetgrooming/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "ğŸ“‹ Processing: Los Angeles Pet Grooming\n",
            "  ğŸ“ Address: 6022 Santa Fe Ave, Huntington Park, CA 90255, USA\n",
            "  ğŸ“ Phone: (323) 818-6741\n",
            "  ğŸŒ Website: https://www.instagram.com/losangelespetgrooming/\n",
            "  ğŸŒ HR Mail: ['Error']\n",
            "\n",
            "ğŸ” Crawling: https://www.instagram.com/losangelespetgrooming/\n",
            "  Processing page 1/5: https://www.instagram.com/losangelespetgrooming/\n",
            "\n",
            "ğŸ” Crawling: https://www.instagram.com/losangelespetgrooming/\n",
            "  Processing page 1/5: https://www.instagram.com/losangelespetgrooming/\n",
            "Error accessing http://www.hollywoodpetwash.com/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "ğŸ“‹ Processing: Pet Wash\n",
            "  ğŸ“ Address: 7357 W Sunset Blvd, Los Angeles, CA 90046, USA\n",
            "  ğŸ“ Phone: (323) 882-6855\n",
            "  ğŸŒ Website: http://www.hollywoodpetwash.com/\n",
            "  ğŸŒ HR Mail: ['Error']\n",
            "\n",
            "ğŸ” Crawling: http://www.hollywoodpetwash.com/\n",
            "  Processing page 1/5: http://www.hollywoodpetwash.com/\n",
            "  Processing page 2/5: https://www.hollywoodpetwash.com\n",
            "  Processing page 3/5: https://www.hollywoodpetwash.com/dog-grooming\n",
            "  Processing page 4/5: https://www.hollywoodpetwash.com/cat-grooming\n",
            "  Processing page 5/5: https://www.hollywoodpetwash.com/book-online\n",
            "Error accessing https://www.wagnswaggrooming.com/?fbclid=PAZXh0bgNhZW0CMTEAAabu939-UBsVpaou6rcwkhQaLUHCUsgTF3-N21m3oqC30J7StqlmBRSQ58M_aem_U2ilWEBXSr-zbMCRNfceDA: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "ğŸ“‹ Processing: Wag n' Swag\n",
            "  ğŸ“ Address: 608 N Hoover St, Los Angeles, CA 90004, USA\n",
            "  ğŸ“ Phone: (323) 522-3600\n",
            "  ğŸŒ Website: https://www.wagnswaggrooming.com/?fbclid=PAZXh0bgNhZW0CMTEAAabu939-UBsVpaou6rcwkhQaLUHCUsgTF3-N21m3oqC30J7StqlmBRSQ58M_aem_U2ilWEBXSr-zbMCRNfceDA\n",
            "  ğŸŒ HR Mail: ['Error']\n",
            "\n",
            "ğŸ” Crawling: https://www.wagnswaggrooming.com/?fbclid=PAZXh0bgNhZW0CMTEAAabu939-UBsVpaou6rcwkhQaLUHCUsgTF3-N21m3oqC30J7StqlmBRSQ58M_aem_U2ilWEBXSr-zbMCRNfceDA\n",
            "  Processing page 1/5: https://www.wagnswaggrooming.com/?fbclid=PAZXh0bgNhZW0CMTEAAabu939-UBsVpaou6rcwkhQaLUHCUsgTF3-N21m3oqC30J7StqlmBRSQ58M_aem_U2ilWEBXSr-zbMCRNfceDA\n",
            "  Processing page 2/5: https://www.wagnswaggrooming.com/cart\n",
            "  Processing page 3/5: https://www.wagnswaggrooming.com/?fbclid=PAZXh0bgNhZW0CMTEAAabu939-UBsVpaou6rcwkhQaLUHCUsgTF3-N21m3oqC30J7StqlmBRSQ58M_aem_U2ilWEBXSr-zbMCRNfceDA#page\n",
            "  Processing page 4/5: https://www.wagnswaggrooming.com/new-dropdown\n",
            "  Processing page 5/5: https://www.wagnswaggrooming.com/services\n",
            "Error accessing https://lovelynosegrooming.top/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "ğŸ“‹ Processing: Lovely Nose Pet Grooming\n",
            "  ğŸ“ Address: 4209 W Washington Blvd, Los Angeles, CA 90016, USA\n",
            "  ğŸ“ Phone: (323) 402-0440\n",
            "  ğŸŒ Website: https://lovelynosegrooming.top/\n",
            "  ğŸŒ HR Mail: ['Error']\n",
            "\n",
            "ğŸ” Crawling: https://lovelynosegrooming.top/\n",
            "  Processing page 1/5: https://lovelynosegrooming.top/\n",
            "  Processing page 2/5: https://lovelynosegrooming.top/#top\n",
            "  Processing page 3/5: https://lovelynosegrooming.top/#about\n",
            "  Processing page 4/5: https://lovelynosegrooming.top/#reviews\n",
            "  Processing page 5/5: https://lovelynosegrooming.top/getting-started/\n",
            "Error accessing http://www.bowiebarker.com/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "ğŸ“‹ Processing: Bowie Barker West Hollywood\n",
            "  ğŸ“ Address: 7609 Beverly Blvd, Los Angeles, CA 90036, USA\n",
            "  ğŸ“ Phone: (323) 372-0622\n",
            "  ğŸŒ Website: http://www.bowiebarker.com/\n",
            "  ğŸŒ HR Mail: ['Error']\n",
            "\n",
            "ğŸ” Crawling: http://www.bowiebarker.com/\n",
            "  Processing page 1/5: http://www.bowiebarker.com/\n",
            "Error accessing http://www.melrosegrooming.com/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "ğŸ“‹ Processing: Melrose Pet Grooming\n",
            "  ğŸ“ Address: 5770 Melrose Ave Children's Reading, Los Angeles, CA 90038, USA\n",
            "  ğŸ“ Phone: (323) 993-1959\n",
            "  ğŸŒ Website: http://www.melrosegrooming.com/\n",
            "  ğŸŒ HR Mail: ['Error']\n",
            "\n",
            "ğŸ” Crawling: http://www.melrosegrooming.com/\n",
            "  Processing page 1/5: http://www.melrosegrooming.com/\n",
            "  Processing page 2/5: https://www.melrosegrooming.com\n",
            "Error accessing http://dragonpetgrooming.com/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "ğŸ“‹ Processing: Dragon in Gaecheon Pet Grooming\n",
            "  ğŸ“ Address: 2825 W 8th St #101, Los Angeles, CA 90005, USA\n",
            "  ğŸ“ Phone: (323) 449-4881\n",
            "  ğŸŒ Website: http://dragonpetgrooming.com/\n",
            "  ğŸŒ HR Mail: ['Error']\n",
            "\n",
            "ğŸ” Crawling: http://dragonpetgrooming.com/\n",
            "  Processing page 1/5: http://dragonpetgrooming.com/\n",
            "  Processing page 2/5: http://dragonpetgrooming.com/index.html\n",
            "  Processing page 3/5: http://dragonpetgrooming.com/#about-home\n",
            "  Processing page 4/5: http://dragonpetgrooming.com/#gallery-home\n",
            "  Processing page 5/5: http://dragonpetgrooming.com/#feature-section\n",
            "Error accessing https://puppylovepet.top/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "ğŸ“‹ Processing: Puppy Love Pet Spa\n",
            "  ğŸ“ Address: 1608 Colorado Blvd, Los Angeles, CA 90041, USA\n",
            "  ğŸ“ Phone: (323) 256-2000\n",
            "  ğŸŒ Website: https://puppylovepet.top/\n",
            "  ğŸŒ HR Mail: ['Error']\n",
            "\n",
            "ğŸ” Crawling: https://puppylovepet.top/\n",
            "  Processing page 1/5: https://puppylovepet.top/\n",
            "  Processing page 2/5: https://puppylovepet.top/#top\n",
            "  Processing page 3/5: https://puppylovepet.top/#about\n",
            "  Processing page 4/5: https://puppylovepet.top/#reviews\n",
            "  Processing page 5/5: https://puppylovepet.top/getting-started/\n",
            "Error accessing http://www.petpurrspective.com/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "ğŸ“‹ Processing: Pet Purrspective\n",
            "  ğŸ“ Address: 5834 Santa Monica Blvd, Los Angeles, CA 90038, USA\n",
            "  ğŸ“ Phone: (323) 600-5913\n",
            "  ğŸŒ Website: http://www.petpurrspective.com/\n",
            "  ğŸŒ HR Mail: ['Error']\n",
            "\n",
            "ğŸ” Crawling: http://www.petpurrspective.com/\n",
            "  Processing page 1/5: http://www.petpurrspective.com/\n",
            "\n",
            "ğŸ“‹ Processing: Tofu Mom Pet Grooming\n",
            "  ğŸ“ Address: 4003 W Olympic Blvd, Los Angeles, CA 90019, USA\n",
            "  ğŸ“ Phone: (323) 578-3181\n",
            "  ğŸŒ Website: \n",
            "  ğŸŒ HR Mail: ['N/A']\n",
            "Error accessing http://www.pawlalaclub.com/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "ğŸ“‹ Processing: PAW LA LA\n",
            "  ğŸ“ Address: 1521 S Vermont Ave, Los Angeles, CA 90006, USA\n",
            "  ğŸ“ Phone: (213) 738-7297\n",
            "  ğŸŒ Website: http://www.pawlalaclub.com/\n",
            "  ğŸŒ HR Mail: ['Error']\n",
            "\n",
            "ğŸ” Crawling: http://www.pawlalaclub.com/\n",
            "  Processing page 1/5: http://www.pawlalaclub.com/\n",
            "  Processing page 2/5: http://www.pawlalaclub.com/cart\n",
            "  Processing page 3/5: http://www.pawlalaclub.com/search\n",
            "  Processing page 4/5: http://www.pawlalaclub.com/gallery\n",
            "  Processing page 5/5: http://www.pawlalaclub.com/contactus\n",
            "\n",
            "ğŸ“‹ Processing: Alex The Happy Dog\n",
            "  ğŸ“ Address: 5011 San Vicente Blvd, Los Angeles, CA 90019, USA\n",
            "  ğŸ“ Phone: (323) 933-9708\n",
            "  ğŸŒ Website: \n",
            "  ğŸŒ HR Mail: ['N/A']\n",
            "\n",
            "ğŸ“‹ Processing: Bubbles Dog Grooming\n",
            "  ğŸ“ Address: 4511 Beverly Blvd, Los Angeles, CA 90004, USA\n",
            "  ğŸ“ Phone: (213) 736-0068\n",
            "  ğŸŒ Website: \n",
            "  ğŸŒ HR Mail: ['N/A']\n",
            "\n",
            "âœ… Results saved to /content/Kobeyo/Scraped CSV/business_job_listings.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main execution function for scraping + saving to Colab filesystem\n",
        "    \"\"\"\n",
        "    all_results = []\n",
        "\n",
        "    # Ensure output directory exists\n",
        "    output_dir = \"/content/Kobeyo/Scraped CSV\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Process each search tag\n",
        "    for tag in SEARCH_TAGS:\n",
        "        place_ids = search_places(tag)\n",
        "\n",
        "        for place_id in place_ids:\n",
        "            result = process_business(place_id)\n",
        "            if result:\n",
        "                all_results.append(result)\n",
        "            time.sleep(2)  # Respect API rate limits\n",
        "\n",
        "    # Save results to CSV\n",
        "    if all_results:\n",
        "        df = pd.DataFrame(all_results)\n",
        "        output_file = f\"{output_dir}/business_job_listings.csv\"\n",
        "        df.to_csv(output_file, index=False)\n",
        "        print(f\"\\nâœ… Results saved to {output_file}\")\n",
        "        return df\n",
        "    else:\n",
        "        print(\"\\nâŒ No results found\")\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa278e62",
      "metadata": {
        "id": "fa278e62"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
