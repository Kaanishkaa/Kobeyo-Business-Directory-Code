{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install selenium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odvCqKmQdid4",
        "outputId": "a8bec45a-d53c-4e2c-9b3c-c3c7e9d05846"
      },
      "id": "odvCqKmQdid4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (4.33.0)\n",
            "Requirement already satisfied: urllib3~=2.4.0 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: trio~=0.30.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.30.0)\n",
            "Requirement already satisfied: trio-websocket~=0.12.2 in /usr/local/lib/python3.11/dist-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: certifi>=2025.4.26 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.4.26)\n",
            "Requirement already satisfied: typing_extensions~=4.13.2 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.13.2)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bdcdfe7",
      "metadata": {
        "id": "9bdcdfe7"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "import time\n",
        "from urllib.parse import urlparse, urljoin\n",
        "from collections import deque\n",
        "import random\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# API key - replace with your actual API key\n",
        "GOOGLE_MAPS_API_KEY = 'enter your key'\n",
        "# APIs and search parameters\n",
        "TEXT_SEARCH_URL = \"https://maps.googleapis.com/maps/api/place/textsearch/json\"\n",
        "PLACE_DETAILS_URL = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
        "SEARCH_TAGS = [\"pet grooming\"]  # Add more tags as needed\n",
        "SOCIAL_PLATFORMS = [\"facebook.com\", \"instagram.com\", \"twitter.com\", \"linkedin.com\"]\n",
        "\n",
        "# User agents for web scraping\n",
        "USER_AGENTS = [\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
        "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15\",\n",
        "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\",\n",
        "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1\",\n",
        "    \"Mozilla/5.0 (iPad; CPU OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1\"\n",
        "]\n",
        "\n",
        "# ----------------- GOOGLE PLACES API FUNCTIONS -----------------\n",
        "\n",
        "def search_places(query, location=\"Los Angeles\"):\n",
        "    \"\"\"\n",
        "    Search for places using Google Places API\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîç Searching for: {query} in {location}\")\n",
        "    params = {\"query\": f\"{query} in {location}\", \"key\": GOOGLE_MAPS_API_KEY}\n",
        "    response = requests.get(TEXT_SEARCH_URL, params=params)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"‚ùå API Error: {response.status_code}, {response.text}\")\n",
        "        return []\n",
        "\n",
        "    results = response.json().get(\"results\", [])\n",
        "    place_ids = [r.get(\"place_id\") for r in results]\n",
        "    print(f\"‚úÖ Found {len(place_ids)} places\")\n",
        "    return place_ids\n",
        "\n",
        "def get_place_details(place_id):\n",
        "    \"\"\"\n",
        "    Get detailed information about a place using its place_id\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        \"place_id\": place_id,\n",
        "        \"fields\": \"name,formatted_address,website,url,formatted_phone_number\",\n",
        "        \"key\": GOOGLE_MAPS_API_KEY\n",
        "    }\n",
        "    response = requests.get(PLACE_DETAILS_URL, params=params)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"‚ùå API Error: {response.status_code}, {response.text}\")\n",
        "        return {}\n",
        "\n",
        "    return response.json().get(\"result\", {})\n",
        "\n",
        "# ----------------- WEB SCRAPING FUNCTIONS -----------------\n",
        "\n",
        "def setup_chrome_driver():\n",
        "    \"\"\"\n",
        "    Setup headless Chrome driver for web scraping\n",
        "    \"\"\"\n",
        "    options = Options()\n",
        "    options.add_argument(\"--headless\")\n",
        "    options.add_argument(\"--disable-gpu\")\n",
        "    options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.binary_location = \"/usr/bin/google-chrome\"  # For Colab\n",
        "    options.add_argument(f\"user-agent={random.choice(USER_AGENTS)}\")\n",
        "    return webdriver.Chrome(options=options)\n",
        "\n",
        "\n",
        "    #return webdriver.Chrome(service=Service(\"/usr/bin/chromedriver\"), options=options)\n",
        "def extract_links_and_content(url):\n",
        "    \"\"\"\n",
        "    Extract links and content from a webpage\n",
        "    \"\"\"\n",
        "    try:\n",
        "        headers = {\"User-Agent\": random.choice(USER_AGENTS)}\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # Extract text content\n",
        "        all_text = soup.get_text(separator=\" \", strip=True)\n",
        "\n",
        "        # Extract links\n",
        "        links = [a['href'] for a in soup.find_all('a', href=True)]\n",
        "\n",
        "        return links, all_text\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Scrape error for {url}: {e}\")\n",
        "        return [], \"\"\n",
        "\n",
        "def is_internal_link(link, domain):\n",
        "    \"\"\"\n",
        "    Check if a link is internal to the domain\n",
        "    \"\"\"\n",
        "    return domain in link and not any(bad in link for bad in [\"indeed\", \"glassdoor\", \"ziprecruiter\", \"monster\", \"linkedin\"])\n",
        "\n",
        "def find_social_links(links):\n",
        "    \"\"\"\n",
        "    Find social media links in a list of links\n",
        "    \"\"\"\n",
        "    social_links = []\n",
        "    for link in links:\n",
        "        if any(social in link for social in SOCIAL_PLATFORMS):\n",
        "            social_links.append(link)\n",
        "    return social_links\n",
        "\n",
        "def get_social_from_google(place_result):\n",
        "    \"\"\"\n",
        "    Try to extract social links from Google Places result\n",
        "    \"\"\"\n",
        "    social_links = []\n",
        "    for field in ['url', 'website']:\n",
        "        if field in place_result and place_result[field]:\n",
        "            url = place_result[field]\n",
        "            if any(social in url for social in SOCIAL_PLATFORMS):\n",
        "                social_links.append(url)\n",
        "    return social_links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06074542",
      "metadata": {
        "id": "06074542"
      },
      "outputs": [],
      "source": [
        "def extract_emails(text):\n",
        "    \"\"\"\n",
        "    Extract valid email addresses from text\n",
        "    \"\"\"\n",
        "    pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
        "    emails = re.findall(pattern, text)\n",
        "\n",
        "    # Filter for business/hiring related emails\n",
        "    valid_emails = []\n",
        "    for email in emails:\n",
        "        email = email.lower()\n",
        "        # Include hiring-related and general contact emails\n",
        "        if any(email.startswith(prefix) for prefix in [\"hr@\", \"jobs@\", \"careers@\", \"hiring@\", \"recruiting@\", \"info@\", \"contact@\", \"hello@\"]):\n",
        "            valid_emails.append(email)\n",
        "        # Include Gmail accounts (often used by small businesses)\n",
        "        elif email.endswith(\"@gmail.com\"):\n",
        "            valid_emails.append(email)\n",
        "\n",
        "    return list(set(valid_emails))\n",
        "\n",
        "def detect_job_content(text):\n",
        "    \"\"\"\n",
        "    Detect job-related content in text\n",
        "    \"\"\"\n",
        "    job_keywords = [\n",
        "        \"we're hiring\", \"now hiring\", \"join our team\", \"career\", \"careers\", \"job opening\",\n",
        "        \"position available\", \"apply now\", \"job application\", \"employment\", \"work with us\",\n",
        "        \"job opportunity\", \"open position\", \"join us\"\n",
        "    ]\n",
        "\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    for keyword in job_keywords:\n",
        "        if keyword in text_lower:\n",
        "            # Find the context around the keyword\n",
        "            start = max(0, text_lower.find(keyword) - 100)\n",
        "            end = min(len(text_lower), text_lower.find(keyword) + 100)\n",
        "            context = text[start:end].strip()\n",
        "            return True, context\n",
        "\n",
        "    return False, \"\"\n",
        "\n",
        "def crawl_site(site_url, max_pages=5):\n",
        "    \"\"\"\n",
        "    Crawl a website to find job-related content and contact information\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîç Crawling: {site_url}\")\n",
        "\n",
        "    # Setup\n",
        "    visited = set()\n",
        "    queue = deque([site_url])\n",
        "    emails = set()\n",
        "    job_content = []\n",
        "    social_links = []\n",
        "\n",
        "    # Parse domain for filtering internal links\n",
        "    try:\n",
        "        domain = urlparse(site_url).netloc\n",
        "    except:\n",
        "        return [], [], [], \"Invalid URL\"\n",
        "\n",
        "    # Process pages\n",
        "    page_count = 0\n",
        "    while queue and page_count < max_pages:\n",
        "        try:\n",
        "            current_url = queue.popleft()\n",
        "            if current_url in visited:\n",
        "                continue\n",
        "\n",
        "            # Mark as visited\n",
        "            visited.add(current_url)\n",
        "            page_count += 1\n",
        "\n",
        "            print(f\"  Processing page {page_count}/{max_pages}: {current_url}\")\n",
        "\n",
        "            # Extract content\n",
        "            links, content = extract_links_and_content(current_url)\n",
        "\n",
        "            # Extract emails\n",
        "            page_emails = extract_emails(content)\n",
        "            emails.update(page_emails)\n",
        "\n",
        "            # Check for job content\n",
        "            has_job, job_context = detect_job_content(content)\n",
        "            if has_job:\n",
        "                job_content.append({\n",
        "                    \"url\": current_url,\n",
        "                    \"context\": job_context\n",
        "                })\n",
        "\n",
        "            # Find social links\n",
        "            page_social = [link for link in links if any(social in link for social in SOCIAL_PLATFORMS)]\n",
        "            social_links.extend(page_social)\n",
        "\n",
        "            # Add internal links to queue\n",
        "            for link in links:\n",
        "                full_link = urljoin(current_url, link)\n",
        "                parsed_link = urlparse(full_link)\n",
        "\n",
        "                # Only add internal links\n",
        "                if parsed_link.netloc == domain and full_link not in visited:\n",
        "                    queue.append(full_link)\n",
        "\n",
        "            # Add some delay to avoid being blocked\n",
        "            time.sleep(random.uniform(1, 2))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error processing {current_url}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return list(emails), job_content, list(set(social_links)), \"Success\"\n",
        "\n",
        "# ----------------- MAIN EXECUTION -----------------\n",
        "\n",
        "def process_business(place_id):\n",
        "    \"\"\"\n",
        "    Process a business by its place_id\n",
        "    \"\"\"\n",
        "    # Get place details\n",
        "    details = get_place_details(place_id)\n",
        "    if not details:\n",
        "        return None\n",
        "\n",
        "    name = details.get(\"name\", \"Unknown\")\n",
        "    address = details.get(\"formatted_address\", \"Unknown\")\n",
        "    phone = details.get(\"formatted_phone_number\", \"Unknown\")\n",
        "    website = details.get(\"website\", \"\")\n",
        "    hr_emails = extract_hr_emails(website) if website else [\"N/A\"]\n",
        "\n",
        "    print(f\"\\nüìã Processing: {name}\")\n",
        "    print(f\"  üìç Address: {address}\")\n",
        "    print(f\"  üìû Phone: {phone}\")\n",
        "    print(f\"  üåê Website: {website}\")\n",
        "    print(f\"  üåê HR Mail: {hr_emails}\")\n",
        "\n",
        "    emails = []\n",
        "    job_content = []\n",
        "    social_links = []\n",
        "    crawl_status = \"Not crawled\"\n",
        "\n",
        "    # Crawl website if available\n",
        "    if website:\n",
        "        try:\n",
        "            emails, job_content, social_links, crawl_status = crawl_site(website)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error crawling website: {e}\")\n",
        "\n",
        "    # Check Google-provided social links if no website or job content found\n",
        "    if not website or not job_content:\n",
        "        google_social = get_social_from_google(details)\n",
        "\n",
        "        # Add to social links list\n",
        "        social_links.extend(google_social)\n",
        "\n",
        "        # Crawl first social link if available and no job content found\n",
        "        if google_social and not job_content:\n",
        "            try:\n",
        "                social_emails, social_job_content, _, _ = crawl_site(google_social[0])\n",
        "                emails.extend(social_emails)\n",
        "                job_content.extend(social_job_content)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error crawling social link: {e}\")\n",
        "\n",
        "    # Format job content for CSV\n",
        "    has_job_posting = len(job_content) > 0\n",
        "    job_urls = [item[\"url\"] for item in job_content]\n",
        "    job_contexts = [item[\"context\"] for item in job_content]\n",
        "\n",
        "    result = {\n",
        "        \"name\": name,\n",
        "        \"address\": address,\n",
        "        \"phone\": phone,\n",
        "        \"website\": website,\n",
        "        \"emails\": \"; \".join(emails) if emails else \"None found\",\n",
        "        \"HR Emails\": \",\".join(hr_emails),\n",
        "        \"has_job_posting\": \"Yes\" if has_job_posting else \"No\",\n",
        "        \"job_urls\": \"; \".join(job_urls) if job_urls else \"None found\",\n",
        "        \"job_contexts\": \"; \".join(job_contexts) if job_contexts else \"None found\",\n",
        "        \"social_links\": \"; \".join(social_links) if social_links else \"None found\",\n",
        "        \"crawl_status\": crawl_status\n",
        "    }\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from selenium.common.exceptions import WebDriverException\n",
        "\n",
        "def extract_hr_emails(website_url):\n",
        "    \"\"\"\n",
        "    Extract HR-related emails from a given website URL\n",
        "    \"\"\"\n",
        "    try:\n",
        "        driver = setup_chrome_driver()\n",
        "        driver.get(website_url)\n",
        "        page_text = driver.page_source\n",
        "        driver.quit()\n",
        "\n",
        "        # Find all email addresses\n",
        "        all_emails = re.findall(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", page_text)\n",
        "        unique_emails = list(set(all_emails))\n",
        "\n",
        "        # Filter HR-related emails\n",
        "        hr_keywords = ['hr', 'people', 'talent', 'careers', 'recruit', 'jobs']\n",
        "        hr_emails = [email for email in unique_emails if any(kw in email.lower() for kw in hr_keywords)]\n",
        "\n",
        "        return hr_emails if hr_emails else [\"Not Found\"]\n",
        "\n",
        "    except WebDriverException as e:\n",
        "        print(f\"Error accessing {website_url}: {e}\")\n",
        "        return [\"Error\"]"
      ],
      "metadata": {
        "id": "x1GNcHdIIzQT"
      },
      "id": "x1GNcHdIIzQT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6471acee",
      "metadata": {
        "id": "6471acee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50a49659-7d89-4701-b8f8-648a710bef59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Searching for: pet grooming in Los Angeles\n",
            "‚úÖ Found 20 places\n",
            "Error accessing https://www.21pooch.com/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "üìã Processing: 21Pooch Hollywood\n",
            "  üìç Address: 5137 W Sunset Blvd Unit 213, 2nd floor, Los Angeles, CA 90027, USA\n",
            "  üìû Phone: (310) 290-3905\n",
            "  üåê Website: https://www.21pooch.com/\n",
            "  üåê HR Mail: ['Error']\n",
            "\n",
            "üîç Crawling: https://www.21pooch.com/\n",
            "  Processing page 1/5: https://www.21pooch.com/\n",
            "  Processing page 2/5: https://www.21pooch.com\n",
            "  Processing page 3/5: https://www.21pooch.com/price\n",
            "  Processing page 4/5: https://www.21pooch.com/book-now\n",
            "  Processing page 5/5: https://www.21pooch.com/contacts\n",
            "Error accessing https://www.21pooch.com/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "üìã Processing: 21Pooch Dog & Cat grooming salon\n",
            "  üìç Address: 1061 S Broadway, Los Angeles, CA 90015, USA\n",
            "  üìû Phone: (213) 589-8304\n",
            "  üåê Website: https://www.21pooch.com/\n",
            "  üåê HR Mail: ['Error']\n",
            "\n",
            "üîç Crawling: https://www.21pooch.com/\n",
            "  Processing page 1/5: https://www.21pooch.com/\n",
            "  Processing page 2/5: https://www.21pooch.com\n",
            "  Processing page 3/5: https://www.21pooch.com/price\n",
            "  Processing page 4/5: https://www.21pooch.com/book-now\n",
            "  Processing page 5/5: https://www.21pooch.com/contacts\n",
            "Error accessing https://groomla.com/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "üìã Processing: Groom - Dog Care With Love\n",
            "  üìç Address: 8749 Holloway Dr, West Hollywood, CA 90069, USA\n",
            "  üìû Phone: (310) 492-5222\n",
            "  üåê Website: https://groomla.com/\n",
            "  üåê HR Mail: ['Error']\n",
            "\n",
            "üîç Crawling: https://groomla.com/\n",
            "  Processing page 1/5: https://groomla.com/\n",
            "  Processing page 2/5: https://groomla.com/health-and-safety\n",
            "  Processing page 3/5: https://groomla.com/services/dog-daycare\n",
            "  Processing page 4/5: https://groomla.com/services/dog-grooming\n",
            "  Processing page 5/5: https://groomla.com/services/deluxe-dog-bath\n",
            "Error accessing https://www.theurbanpet.net/self-wash-pods-and-full-service-grooming-at-moonshine-grooming/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "üìã Processing: Moon Shine Grooming & Pet Wash\n",
            "  üìç Address: 4473 Santa Monica Blvd, Los Angeles, CA 90029, USA\n",
            "  üìû Phone: (323) 664-4600\n",
            "  üåê Website: https://www.theurbanpet.net/self-wash-pods-and-full-service-grooming-at-moonshine-grooming/\n",
            "  üåê HR Mail: ['Error']\n",
            "\n",
            "üîç Crawling: https://www.theurbanpet.net/self-wash-pods-and-full-service-grooming-at-moonshine-grooming/\n",
            "  Processing page 1/5: https://www.theurbanpet.net/self-wash-pods-and-full-service-grooming-at-moonshine-grooming/\n",
            "  Processing page 2/5: https://www.theurbanpet.net/self-wash-pods-and-full-service-grooming-at-moonshine-grooming/contact-for-pet-grooming\n",
            "  Processing page 3/5: https://www.theurbanpet.net/self-wash-pods-and-full-service-grooming-at-moonshine-grooming/upcoming-events\n",
            "  Processing page 4/5: https://www.theurbanpet.net/self-wash-pods-and-full-service-grooming-at-moonshine-grooming/membership\n",
            "  Processing page 5/5: https://www.theurbanpet.net/\n",
            "Error accessing http://pourlapooch.com/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "üìã Processing: Pour La Pooch\n",
            "  üìç Address: 7617 Beverly Blvd, Los Angeles, CA 90036, USA\n",
            "  üìû Phone: (323) 934-0940\n",
            "  üåê Website: http://pourlapooch.com/\n",
            "  üåê HR Mail: ['Error']\n",
            "\n",
            "üîç Crawling: http://pourlapooch.com/\n",
            "  Processing page 1/5: http://pourlapooch.com/\n",
            "  Processing page 2/5: http://pourlapooch.com/dog-daycare.html\n",
            "  Processing page 3/5: http://pourlapooch.com/photos.html\n",
            "  Processing page 4/5: http://pourlapooch.com/map--hours.html\n",
            "  Processing page 5/5: http://pourlapooch.com/contact-us.html\n",
            "Error accessing https://www.alessandropetspa.com/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "üìã Processing: Alessandro Pet Spa\n",
            "  üìç Address: 7221 Melrose Ave, Los Angeles, CA 90046, USA\n",
            "  üìû Phone: (323) 644-4946\n",
            "  üåê Website: https://www.alessandropetspa.com/\n",
            "  üåê HR Mail: ['Error']\n",
            "\n",
            "üîç Crawling: https://www.alessandropetspa.com/\n",
            "  Processing page 1/5: https://www.alessandropetspa.com/\n",
            "  Processing page 2/5: https://www.alessandropetspa.com/about\n",
            "  Processing page 3/5: https://www.alessandropetspa.com/dog-grooming\n",
            "  Processing page 4/5: https://www.alessandropetspa.com/cat-grooming\n",
            "  Processing page 5/5: https://www.alessandropetspa.com/dog-bathing\n",
            "Error accessing http://www.bubblepupsalon.com/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "üìã Processing: Bubble Pup Salon\n",
            "  üìç Address: 333 S Alameda St STE 201, Los Angeles, CA 90013, USA\n",
            "  üìû Phone: (213) 558-9355\n",
            "  üåê Website: http://www.bubblepupsalon.com/\n",
            "  üåê HR Mail: ['Error']\n",
            "\n",
            "üîç Crawling: http://www.bubblepupsalon.com/\n",
            "  Processing page 1/5: http://www.bubblepupsalon.com/\n",
            "  Processing page 2/5: https://www.bubblepupsalon.com/book-online-2\n",
            "  Processing page 3/5: https://www.bubblepupsalon.com\n",
            "  Processing page 4/5: https://www.bubblepupsalon.com/book-online-1\n",
            "  Processing page 5/5: https://www.bubblepupsalon.com/general-5\n",
            "Error accessing https://www.instagram.com/losangelespetgrooming/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "üìã Processing: Los Angeles Pet Grooming\n",
            "  üìç Address: 6022 Santa Fe Ave, Huntington Park, CA 90255, USA\n",
            "  üìû Phone: (323) 818-6741\n",
            "  üåê Website: https://www.instagram.com/losangelespetgrooming/\n",
            "  üåê HR Mail: ['Error']\n",
            "\n",
            "üîç Crawling: https://www.instagram.com/losangelespetgrooming/\n",
            "  Processing page 1/5: https://www.instagram.com/losangelespetgrooming/\n",
            "\n",
            "üîç Crawling: https://www.instagram.com/losangelespetgrooming/\n",
            "  Processing page 1/5: https://www.instagram.com/losangelespetgrooming/\n",
            "Error accessing http://www.hollywoodpetwash.com/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "üìã Processing: Pet Wash\n",
            "  üìç Address: 7357 W Sunset Blvd, Los Angeles, CA 90046, USA\n",
            "  üìû Phone: (323) 882-6855\n",
            "  üåê Website: http://www.hollywoodpetwash.com/\n",
            "  üåê HR Mail: ['Error']\n",
            "\n",
            "üîç Crawling: http://www.hollywoodpetwash.com/\n",
            "  Processing page 1/5: http://www.hollywoodpetwash.com/\n",
            "  Processing page 2/5: https://www.hollywoodpetwash.com\n",
            "  Processing page 3/5: https://www.hollywoodpetwash.com/dog-grooming\n",
            "  Processing page 4/5: https://www.hollywoodpetwash.com/cat-grooming\n",
            "  Processing page 5/5: https://www.hollywoodpetwash.com/book-online\n",
            "Error accessing https://www.wagnswaggrooming.com/?fbclid=PAZXh0bgNhZW0CMTEAAabu939-UBsVpaou6rcwkhQaLUHCUsgTF3-N21m3oqC30J7StqlmBRSQ58M_aem_U2ilWEBXSr-zbMCRNfceDA: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "üìã Processing: Wag n' Swag\n",
            "  üìç Address: 608 N Hoover St, Los Angeles, CA 90004, USA\n",
            "  üìû Phone: (323) 522-3600\n",
            "  üåê Website: https://www.wagnswaggrooming.com/?fbclid=PAZXh0bgNhZW0CMTEAAabu939-UBsVpaou6rcwkhQaLUHCUsgTF3-N21m3oqC30J7StqlmBRSQ58M_aem_U2ilWEBXSr-zbMCRNfceDA\n",
            "  üåê HR Mail: ['Error']\n",
            "\n",
            "üîç Crawling: https://www.wagnswaggrooming.com/?fbclid=PAZXh0bgNhZW0CMTEAAabu939-UBsVpaou6rcwkhQaLUHCUsgTF3-N21m3oqC30J7StqlmBRSQ58M_aem_U2ilWEBXSr-zbMCRNfceDA\n",
            "  Processing page 1/5: https://www.wagnswaggrooming.com/?fbclid=PAZXh0bgNhZW0CMTEAAabu939-UBsVpaou6rcwkhQaLUHCUsgTF3-N21m3oqC30J7StqlmBRSQ58M_aem_U2ilWEBXSr-zbMCRNfceDA\n",
            "  Processing page 2/5: https://www.wagnswaggrooming.com/cart\n",
            "  Processing page 3/5: https://www.wagnswaggrooming.com/?fbclid=PAZXh0bgNhZW0CMTEAAabu939-UBsVpaou6rcwkhQaLUHCUsgTF3-N21m3oqC30J7StqlmBRSQ58M_aem_U2ilWEBXSr-zbMCRNfceDA#page\n",
            "  Processing page 4/5: https://www.wagnswaggrooming.com/new-dropdown\n",
            "  Processing page 5/5: https://www.wagnswaggrooming.com/services\n",
            "Error accessing https://lovelynosegrooming.top/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "üìã Processing: Lovely Nose Pet Grooming\n",
            "  üìç Address: 4209 W Washington Blvd, Los Angeles, CA 90016, USA\n",
            "  üìû Phone: (323) 402-0440\n",
            "  üåê Website: https://lovelynosegrooming.top/\n",
            "  üåê HR Mail: ['Error']\n",
            "\n",
            "üîç Crawling: https://lovelynosegrooming.top/\n",
            "  Processing page 1/5: https://lovelynosegrooming.top/\n",
            "  Processing page 2/5: https://lovelynosegrooming.top/#top\n",
            "  Processing page 3/5: https://lovelynosegrooming.top/#about\n",
            "  Processing page 4/5: https://lovelynosegrooming.top/#reviews\n",
            "  Processing page 5/5: https://lovelynosegrooming.top/getting-started/\n",
            "Error accessing http://www.bowiebarker.com/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "üìã Processing: Bowie Barker West Hollywood\n",
            "  üìç Address: 7609 Beverly Blvd, Los Angeles, CA 90036, USA\n",
            "  üìû Phone: (323) 372-0622\n",
            "  üåê Website: http://www.bowiebarker.com/\n",
            "  üåê HR Mail: ['Error']\n",
            "\n",
            "üîç Crawling: http://www.bowiebarker.com/\n",
            "  Processing page 1/5: http://www.bowiebarker.com/\n",
            "Error accessing http://www.melrosegrooming.com/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "üìã Processing: Melrose Pet Grooming\n",
            "  üìç Address: 5770 Melrose Ave Children's Reading, Los Angeles, CA 90038, USA\n",
            "  üìû Phone: (323) 993-1959\n",
            "  üåê Website: http://www.melrosegrooming.com/\n",
            "  üåê HR Mail: ['Error']\n",
            "\n",
            "üîç Crawling: http://www.melrosegrooming.com/\n",
            "  Processing page 1/5: http://www.melrosegrooming.com/\n",
            "  Processing page 2/5: https://www.melrosegrooming.com\n",
            "Error accessing http://dragonpetgrooming.com/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "üìã Processing: Dragon in Gaecheon Pet Grooming\n",
            "  üìç Address: 2825 W 8th St #101, Los Angeles, CA 90005, USA\n",
            "  üìû Phone: (323) 449-4881\n",
            "  üåê Website: http://dragonpetgrooming.com/\n",
            "  üåê HR Mail: ['Error']\n",
            "\n",
            "üîç Crawling: http://dragonpetgrooming.com/\n",
            "  Processing page 1/5: http://dragonpetgrooming.com/\n",
            "  Processing page 2/5: http://dragonpetgrooming.com/index.html\n",
            "  Processing page 3/5: http://dragonpetgrooming.com/#about-home\n",
            "  Processing page 4/5: http://dragonpetgrooming.com/#gallery-home\n",
            "  Processing page 5/5: http://dragonpetgrooming.com/#feature-section\n",
            "Error accessing https://puppylovepet.top/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "üìã Processing: Puppy Love Pet Spa\n",
            "  üìç Address: 1608 Colorado Blvd, Los Angeles, CA 90041, USA\n",
            "  üìû Phone: (323) 256-2000\n",
            "  üåê Website: https://puppylovepet.top/\n",
            "  üåê HR Mail: ['Error']\n",
            "\n",
            "üîç Crawling: https://puppylovepet.top/\n",
            "  Processing page 1/5: https://puppylovepet.top/\n",
            "  Processing page 2/5: https://puppylovepet.top/#top\n",
            "  Processing page 3/5: https://puppylovepet.top/#about\n",
            "  Processing page 4/5: https://puppylovepet.top/#reviews\n",
            "  Processing page 5/5: https://puppylovepet.top/getting-started/\n",
            "Error accessing http://www.petpurrspective.com/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "üìã Processing: Pet Purrspective\n",
            "  üìç Address: 5834 Santa Monica Blvd, Los Angeles, CA 90038, USA\n",
            "  üìû Phone: (323) 600-5913\n",
            "  üåê Website: http://www.petpurrspective.com/\n",
            "  üåê HR Mail: ['Error']\n",
            "\n",
            "üîç Crawling: http://www.petpurrspective.com/\n",
            "  Processing page 1/5: http://www.petpurrspective.com/\n",
            "\n",
            "üìã Processing: Tofu Mom Pet Grooming\n",
            "  üìç Address: 4003 W Olympic Blvd, Los Angeles, CA 90019, USA\n",
            "  üìû Phone: (323) 578-3181\n",
            "  üåê Website: \n",
            "  üåê HR Mail: ['N/A']\n",
            "Error accessing http://www.pawlalaclub.com/: Message: Unable to obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
            "\n",
            "\n",
            "üìã Processing: PAW LA LA\n",
            "  üìç Address: 1521 S Vermont Ave, Los Angeles, CA 90006, USA\n",
            "  üìû Phone: (213) 738-7297\n",
            "  üåê Website: http://www.pawlalaclub.com/\n",
            "  üåê HR Mail: ['Error']\n",
            "\n",
            "üîç Crawling: http://www.pawlalaclub.com/\n",
            "  Processing page 1/5: http://www.pawlalaclub.com/\n",
            "  Processing page 2/5: http://www.pawlalaclub.com/cart\n",
            "  Processing page 3/5: http://www.pawlalaclub.com/search\n",
            "  Processing page 4/5: http://www.pawlalaclub.com/gallery\n",
            "  Processing page 5/5: http://www.pawlalaclub.com/contactus\n",
            "\n",
            "üìã Processing: Alex The Happy Dog\n",
            "  üìç Address: 5011 San Vicente Blvd, Los Angeles, CA 90019, USA\n",
            "  üìû Phone: (323) 933-9708\n",
            "  üåê Website: \n",
            "  üåê HR Mail: ['N/A']\n",
            "\n",
            "üìã Processing: Bubbles Dog Grooming\n",
            "  üìç Address: 4511 Beverly Blvd, Los Angeles, CA 90004, USA\n",
            "  üìû Phone: (213) 736-0068\n",
            "  üåê Website: \n",
            "  üåê HR Mail: ['N/A']\n",
            "\n",
            "‚úÖ Results saved to /content/Kobeyo/Scraped CSV/business_job_listings.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main execution function for scraping + saving to Colab filesystem\n",
        "    \"\"\"\n",
        "    all_results = []\n",
        "\n",
        "    # Ensure output directory exists\n",
        "    output_dir = \"/content/Kobeyo/Scraped CSV\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Process each search tag\n",
        "    for tag in SEARCH_TAGS:\n",
        "        place_ids = search_places(tag)\n",
        "\n",
        "        for place_id in place_ids:\n",
        "            result = process_business(place_id)\n",
        "            if result:\n",
        "                all_results.append(result)\n",
        "            time.sleep(2)  # Respect API rate limits\n",
        "\n",
        "    # Save results to CSV\n",
        "    if all_results:\n",
        "        df = pd.DataFrame(all_results)\n",
        "        output_file = f\"{output_dir}/business_job_listings.csv\"\n",
        "        df.to_csv(output_file, index=False)\n",
        "        print(f\"\\n‚úÖ Results saved to {output_file}\")\n",
        "        return df\n",
        "    else:\n",
        "        print(\"\\n‚ùå No results found\")\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa278e62",
      "metadata": {
        "id": "fa278e62"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
