{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "odvCqKmQdid4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odvCqKmQdid4",
        "outputId": "a8bec45a-d53c-4e2c-9b3c-c3c7e9d05846"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: selenium in /Users/varma/my_folder/summer_2025/internship/KOBEYO/.venv/lib/python3.9/site-packages (4.33.0)\n",
            "Requirement already satisfied: urllib3~=2.4.0 in /Users/varma/my_folder/summer_2025/internship/KOBEYO/.venv/lib/python3.9/site-packages (from urllib3[socks]~=2.4.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: trio~=0.30.0 in /Users/varma/my_folder/summer_2025/internship/KOBEYO/.venv/lib/python3.9/site-packages (from selenium) (0.30.0)\n",
            "Requirement already satisfied: trio-websocket~=0.12.2 in /Users/varma/my_folder/summer_2025/internship/KOBEYO/.venv/lib/python3.9/site-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: certifi>=2025.4.26 in /Users/varma/my_folder/summer_2025/internship/KOBEYO/.venv/lib/python3.9/site-packages (from selenium) (2025.6.15)\n",
            "Requirement already satisfied: typing_extensions~=4.13.2 in /Users/varma/my_folder/summer_2025/internship/KOBEYO/.venv/lib/python3.9/site-packages (from selenium) (4.13.2)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /Users/varma/my_folder/summer_2025/internship/KOBEYO/.venv/lib/python3.9/site-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /Users/varma/my_folder/summer_2025/internship/KOBEYO/.venv/lib/python3.9/site-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /Users/varma/my_folder/summer_2025/internship/KOBEYO/.venv/lib/python3.9/site-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /Users/varma/my_folder/summer_2025/internship/KOBEYO/.venv/lib/python3.9/site-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /Users/varma/my_folder/summer_2025/internship/KOBEYO/.venv/lib/python3.9/site-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /Users/varma/my_folder/summer_2025/internship/KOBEYO/.venv/lib/python3.9/site-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /Users/varma/my_folder/summer_2025/internship/KOBEYO/.venv/lib/python3.9/site-packages (from trio~=0.30.0->selenium) (1.3.0)\n",
            "Requirement already satisfied: wsproto>=0.14 in /Users/varma/my_folder/summer_2025/internship/KOBEYO/.venv/lib/python3.9/site-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /Users/varma/my_folder/summer_2025/internship/KOBEYO/.venv/lib/python3.9/site-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /Users/varma/my_folder/summer_2025/internship/KOBEYO/.venv/lib/python3.9/site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install selenium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bdcdfe7",
      "metadata": {
        "id": "9bdcdfe7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/varma/my_folder/summer_2025/internship/KOBEYO/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "import time\n",
        "from urllib.parse import urlparse, urljoin\n",
        "from collections import deque\n",
        "import random\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# API key - replace with your actual API key\n",
        "GOOGLE_MAPS_API_KEY = ''\n",
        "# APIs and search parameters\n",
        "TEXT_SEARCH_URL = \"https://maps.googleapis.com/maps/api/place/textsearch/json\"\n",
        "PLACE_DETAILS_URL = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
        "SEARCH_TAGS = [\"pet grooming\"]  # Add more tags as needed\n",
        "SOCIAL_PLATFORMS = [\"facebook.com\", \"instagram.com\", \"twitter.com\", \"linkedin.com\"]\n",
        "\n",
        "# User agents for web scraping\n",
        "USER_AGENTS = [\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
        "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15\",\n",
        "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\",\n",
        "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1\",\n",
        "    \"Mozilla/5.0 (iPad; CPU OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1\"\n",
        "]\n",
        "\n",
        "# ----------------- GOOGLE PLACES API FUNCTIONS -----------------\n",
        "\n",
        "def search_places(query, location=\"Los Angeles\"):\n",
        "    \"\"\"\n",
        "    Search for places using Google Places API\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîç Searching for: {query} in {location}\")\n",
        "    params = {\"query\": f\"{query} in {location}\", \"key\": GOOGLE_MAPS_API_KEY}\n",
        "    response = requests.get(TEXT_SEARCH_URL, params=params)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"‚ùå API Error: {response.status_code}, {response.text}\")\n",
        "        return []\n",
        "\n",
        "    results = response.json().get(\"results\", [])\n",
        "    place_ids = [r.get(\"place_id\") for r in results]\n",
        "    print(f\"‚úÖ Found {len(place_ids)} places\")\n",
        "    return place_ids\n",
        "\n",
        "def get_place_details(place_id):\n",
        "    \"\"\"\n",
        "    Get detailed information about a place using its place_id\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        \"place_id\": place_id,\n",
        "        \"fields\": \"name,formatted_address,website,url,formatted_phone_number\",\n",
        "        \"key\": GOOGLE_MAPS_API_KEY\n",
        "    }\n",
        "    response = requests.get(PLACE_DETAILS_URL, params=params)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"‚ùå API Error: {response.status_code}, {response.text}\")\n",
        "        return {}\n",
        "\n",
        "    return response.json().get(\"result\", {})\n",
        "\n",
        "# ----------------- WEB SCRAPING FUNCTIONS -----------------\n",
        "\n",
        "def setup_chrome_driver():\n",
        "    \"\"\"\n",
        "    Setup headless Chrome driver for web scraping\n",
        "    \"\"\"\n",
        "    options = Options()\n",
        "    options.add_argument(\"--headless\")\n",
        "    options.add_argument(\"--disable-gpu\")\n",
        "    options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.binary_location = \"/usr/bin/google-chrome\"  # For Colab\n",
        "    options.add_argument(f\"user-agent={random.choice(USER_AGENTS)}\")\n",
        "    return webdriver.Chrome(options=options)\n",
        "\n",
        "\n",
        "    #return webdriver.Chrome(service=Service(\"/usr/bin/chromedriver\"), options=options)\n",
        "def extract_links_and_content(url):\n",
        "    \"\"\"\n",
        "    Extract links and content from a webpage\n",
        "    \"\"\"\n",
        "    try:\n",
        "        headers = {\"User-Agent\": random.choice(USER_AGENTS)}\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # Extract text content\n",
        "        all_text = soup.get_text(separator=\" \", strip=True)\n",
        "\n",
        "        # Extract links\n",
        "        links = [a['href'] for a in soup.find_all('a', href=True)]\n",
        "\n",
        "        return links, all_text\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Scrape error for {url}: {e}\")\n",
        "        return [], \"\"\n",
        "\n",
        "def is_internal_link(link, domain):\n",
        "    \"\"\"\n",
        "    Check if a link is internal to the domain\n",
        "    \"\"\"\n",
        "    return domain in link and not any(bad in link for bad in [\"indeed\", \"glassdoor\", \"ziprecruiter\", \"monster\", \"linkedin\"])\n",
        "\n",
        "def find_social_links(links):\n",
        "    \"\"\"\n",
        "    Find social media links in a list of links\n",
        "    \"\"\"\n",
        "    social_links = []\n",
        "    for link in links:\n",
        "        if any(social in link for social in SOCIAL_PLATFORMS):\n",
        "            social_links.append(link)\n",
        "    return social_links\n",
        "\n",
        "def get_social_from_google(place_result):\n",
        "    \"\"\"\n",
        "    Try to extract social links from Google Places result\n",
        "    \"\"\"\n",
        "    social_links = []\n",
        "    for field in ['url', 'website']:\n",
        "        if field in place_result and place_result[field]:\n",
        "            url = place_result[field]\n",
        "            if any(social in url for social in SOCIAL_PLATFORMS):\n",
        "                social_links.append(url)\n",
        "    return social_links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "06074542",
      "metadata": {
        "id": "06074542"
      },
      "outputs": [],
      "source": [
        "def extract_emails(text):\n",
        "    \"\"\"\n",
        "    Extract valid email addresses from text\n",
        "    \"\"\"\n",
        "    pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
        "    emails = re.findall(pattern, text)\n",
        "\n",
        "    # Filter for business/hiring related emails\n",
        "    valid_emails = []\n",
        "    for email in emails:\n",
        "        email = email.lower()\n",
        "        # Include hiring-related and general contact emails\n",
        "        if any(email.startswith(prefix) for prefix in [\"hr@\", \"jobs@\", \"careers@\", \"hiring@\", \"recruiting@\", \"info@\", \"contact@\", \"hello@\"]):\n",
        "            valid_emails.append(email)\n",
        "        # Include Gmail accounts (often used by small businesses)\n",
        "        elif email.endswith(\"@gmail.com\"):\n",
        "            valid_emails.append(email)\n",
        "\n",
        "    return list(set(valid_emails))\n",
        "\n",
        "def detect_job_content(text):\n",
        "    \"\"\"\n",
        "    Detect job-related content in text\n",
        "    \"\"\"\n",
        "    job_keywords = [\n",
        "        \"we're hiring\", \"now hiring\", \"join our team\", \"career\", \"careers\", \"job opening\",\n",
        "        \"position available\", \"apply now\", \"job application\", \"employment\", \"work with us\",\n",
        "        \"job opportunity\", \"open position\", \"join us\"\n",
        "    ]\n",
        "\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    for keyword in job_keywords:\n",
        "        if keyword in text_lower:\n",
        "            # Find the context around the keyword\n",
        "            start = max(0, text_lower.find(keyword) - 100)\n",
        "            end = min(len(text_lower), text_lower.find(keyword) + 100)\n",
        "            context = text[start:end].strip()\n",
        "            return True, context\n",
        "\n",
        "    return False, \"\"\n",
        "\n",
        "def crawl_site(site_url, max_pages=5):\n",
        "    \"\"\"\n",
        "    Crawl a website to find job-related content and contact information\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîç Crawling: {site_url}\")\n",
        "\n",
        "    # Setup\n",
        "    visited = set()\n",
        "    queue = deque([site_url])\n",
        "    emails = set()\n",
        "    job_content = []\n",
        "    social_links = []\n",
        "\n",
        "    # Parse domain for filtering internal links\n",
        "    try:\n",
        "        domain = urlparse(site_url).netloc\n",
        "    except:\n",
        "        return [], [], [], \"Invalid URL\"\n",
        "\n",
        "    # Process pages\n",
        "    page_count = 0\n",
        "    while queue and page_count < max_pages:\n",
        "        try:\n",
        "            current_url = queue.popleft()\n",
        "            if current_url in visited:\n",
        "                continue\n",
        "\n",
        "            # Mark as visited\n",
        "            visited.add(current_url)\n",
        "            page_count += 1\n",
        "\n",
        "            print(f\"  Processing page {page_count}/{max_pages}: {current_url}\")\n",
        "\n",
        "            # Extract content\n",
        "            links, content = extract_links_and_content(current_url)\n",
        "\n",
        "            # Extract emails\n",
        "            page_emails = extract_emails(content)\n",
        "            emails.update(page_emails)\n",
        "\n",
        "            # Check for job content\n",
        "            has_job, job_context = detect_job_content(content)\n",
        "            if has_job:\n",
        "                job_content.append({\n",
        "                    \"url\": current_url,\n",
        "                    \"context\": job_context\n",
        "                })\n",
        "\n",
        "            # Find social links\n",
        "            page_social = [link for link in links if any(social in link for social in SOCIAL_PLATFORMS)]\n",
        "            social_links.extend(page_social)\n",
        "\n",
        "            # Add internal links to queue\n",
        "            for link in links:\n",
        "                full_link = urljoin(current_url, link)\n",
        "                parsed_link = urlparse(full_link)\n",
        "\n",
        "                # Only add internal links\n",
        "                if parsed_link.netloc == domain and full_link not in visited:\n",
        "                    queue.append(full_link)\n",
        "\n",
        "            # Add some delay to avoid being blocked\n",
        "            time.sleep(random.uniform(1, 2))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error processing {current_url}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return list(emails), job_content, list(set(social_links)), \"Success\"\n",
        "\n",
        "# ----------------- MAIN EXECUTION -----------------\n",
        "\n",
        "def process_business(place_id):\n",
        "    \"\"\"\n",
        "    Process a business by its place_id\n",
        "    \"\"\"\n",
        "    # Get place details\n",
        "    details = get_place_details(place_id)\n",
        "    if not details:\n",
        "        return None\n",
        "\n",
        "    name = details.get(\"name\", \"Unknown\")\n",
        "    address = details.get(\"formatted_address\", \"Unknown\")\n",
        "    phone = details.get(\"formatted_phone_number\", \"Unknown\")\n",
        "    website = details.get(\"website\", \"\")\n",
        "    hr_emails = extract_hr_emails(website) if website else [\"N/A\"]\n",
        "\n",
        "    print(f\"\\nüìã Processing: {name}\")\n",
        "    print(f\"  üìç Address: {address}\")\n",
        "    print(f\"  üìû Phone: {phone}\")\n",
        "    print(f\"  üåê Website: {website}\")\n",
        "    print(f\"  üåê HR Mail: {hr_emails}\")\n",
        "\n",
        "    emails = []\n",
        "    job_content = []\n",
        "    social_links = []\n",
        "    crawl_status = \"Not crawled\"\n",
        "\n",
        "    # Crawl website if available\n",
        "    if website:\n",
        "        try:\n",
        "            emails, job_content, social_links, crawl_status = crawl_site(website)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error crawling website: {e}\")\n",
        "\n",
        "    # Check Google-provided social links if no website or job content found\n",
        "    if not website or not job_content:\n",
        "        google_social = get_social_from_google(details)\n",
        "\n",
        "        # Add to social links list\n",
        "        social_links.extend(google_social)\n",
        "\n",
        "        # Crawl first social link if available and no job content found\n",
        "        if google_social and not job_content:\n",
        "            try:\n",
        "                social_emails, social_job_content, _, _ = crawl_site(google_social[0])\n",
        "                emails.extend(social_emails)\n",
        "                job_content.extend(social_job_content)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error crawling social link: {e}\")\n",
        "\n",
        "    # Format job content for CSV\n",
        "    has_job_posting = len(job_content) > 0\n",
        "    job_urls = [item[\"url\"] for item in job_content]\n",
        "    job_contexts = [item[\"context\"] for item in job_content]\n",
        "\n",
        "    result = {\n",
        "        \"name\": name,\n",
        "        \"address\": address,\n",
        "        \"phone\": phone,\n",
        "        \"website\": website,\n",
        "        \"emails\": \"; \".join(emails) if emails else \"None found\",\n",
        "        \"HR Emails\": \",\".join(hr_emails),\n",
        "        \"has_job_posting\": \"Yes\" if has_job_posting else \"No\",\n",
        "        \"job_urls\": \"; \".join(job_urls) if job_urls else \"None found\",\n",
        "        \"job_contexts\": \"; \".join(job_contexts) if job_contexts else \"None found\",\n",
        "        \"social_links\": \"; \".join(social_links) if social_links else \"None found\",\n",
        "        \"crawl_status\": crawl_status\n",
        "    }\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "x1GNcHdIIzQT",
      "metadata": {
        "id": "x1GNcHdIIzQT"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from selenium.common.exceptions import WebDriverException\n",
        "\n",
        "def extract_hr_emails(website_url):\n",
        "    \"\"\"\n",
        "    Extract HR-related emails from a given website URL\n",
        "    \"\"\"\n",
        "    try:\n",
        "        driver = setup_chrome_driver()\n",
        "        driver.get(website_url)\n",
        "        page_text = driver.page_source\n",
        "        driver.quit()\n",
        "\n",
        "        # Find all email addresses\n",
        "        all_emails = re.findall(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", page_text)\n",
        "        unique_emails = list(set(all_emails))\n",
        "\n",
        "        # Filter HR-related emails\n",
        "        hr_keywords = ['hr', 'people', 'talent', 'careers', 'recruit', 'jobs']\n",
        "        hr_emails = [email for email in unique_emails if any(kw in email.lower() for kw in hr_keywords)]\n",
        "\n",
        "        return hr_emails if hr_emails else [\"Not Found\"]\n",
        "\n",
        "    except WebDriverException as e:\n",
        "        print(f\"Error accessing {website_url}: {e}\")\n",
        "        return [\"Error\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6471acee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6471acee",
        "outputId": "50a49659-7d89-4701-b8f8-648a710bef59"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "[Errno 30] Read-only file system: '/content'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[5], line 9\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Ensure output directory exists\u001b[39;00m\n\u001b[1;32m      8\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/Kobeyo/Scraped CSV\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Process each search tag\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m SEARCH_TAGS:\n",
            "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/content'"
          ]
        }
      ],
      "source": [
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main execution function for scraping + saving to Colab filesystem\n",
        "    \"\"\"\n",
        "    all_results = []\n",
        "\n",
        "    # Ensure output directory exists\n",
        "    output_dir = \"/content/Kobeyo/Scraped CSV\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Process each search tag\n",
        "    for tag in SEARCH_TAGS:\n",
        "        place_ids = search_places(tag)\n",
        "\n",
        "        for place_id in place_ids:\n",
        "            result = process_business(place_id)\n",
        "            if result:\n",
        "                all_results.append(result)\n",
        "            time.sleep(2)  # Respect API rate limits\n",
        "\n",
        "    # Save results to CSV\n",
        "    if all_results:\n",
        "        df = pd.DataFrame(all_results)\n",
        "        output_file = f\"{output_dir}/business_job_listings.csv\"\n",
        "        df.to_csv(output_file, index=False)\n",
        "        print(f\"\\n‚úÖ Results saved to {output_file}\")\n",
        "        return df\n",
        "    else:\n",
        "        print(\"\\n‚ùå No results found\")\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa278e62",
      "metadata": {
        "id": "fa278e62"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
