{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69668ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import threading\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import deque\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Constants\n",
    "GOOGLE_API_KEY = \"PLACE YOUR GOOGLE API KEY HERE\"\n",
    "TEXT_SEARCH_URL = \"https://maps.googleapis.com/maps/api/place/textsearch/json\"\n",
    "PLACE_DETAILS_URL = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
    "SEARCH_TAGS = [\"restaurant\"]\n",
    "SEARCH_LOCATIONS = [\"Los Angeles\"]\n",
    "MAX_PAGES_PER_SITE = 15\n",
    "CAREERS_MAX_PAGES = 5\n",
    "THREAD_POOL_WORKERS = 5\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fc560d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third-party job sites to exclude\n",
    "THIRD_PARTY_JOB_SITES = [\n",
    "    \"indeed.com\", \"ziprecruiter.com\", \"linkedin.com/jobs\", \"monster.com\",\n",
    "    \"glassdoor.com\", \"careerbuilder.com\", \"simplyhired.com\", \"dice.com\",\n",
    "    \"flexjobs.com\", \"upwork.com\", \"freelancer.com\", \"fiverr.com\",\n",
    "    \"snagajob.com\", \"workday.com\", \"bamboohr.com\", \"greenhouse.io\"\n",
    "]\n",
    "\n",
    "# URL Shortening Configuration\n",
    "MAX_URL_LENGTH = 80\n",
    "ELLIPSIS = \"...\"\n",
    "\n",
    "# Setup headless Chrome for Selenium\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "driver = webdriver.Chrome(options=chrome_options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6a1b378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= URL PROCESSING FUNCTIONS =================\n",
    "\n",
    "def shorten_url(url, max_length=MAX_URL_LENGTH):\n",
    "    \"\"\"Shorten a URL if it exceeds the maximum length\"\"\"\n",
    "    if len(url) <= max_length:\n",
    "        return url\n",
    "\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        domain = parsed.netloc\n",
    "        scheme = parsed.scheme\n",
    "        base_url = f\"{scheme}://{domain}\"\n",
    "\n",
    "        if len(base_url) >= max_length - len(ELLIPSIS):\n",
    "            return base_url[:max_length - len(ELLIPSIS)] + ELLIPSIS\n",
    "\n",
    "        remaining_space = max_length - len(base_url) - len(ELLIPSIS)\n",
    "        if remaining_space > 0 and parsed.path:\n",
    "            path_part = parsed.path[:remaining_space]\n",
    "            return f\"{base_url}{path_part}{ELLIPSIS}\"\n",
    "        else:\n",
    "            return base_url\n",
    "\n",
    "    except Exception:\n",
    "        return url[:max_length - len(ELLIPSIS)] + ELLIPSIS\n",
    "\n",
    "def check_url_status(url, timeout=5):\n",
    "    \"\"\"Check if a URL is working/accessible (returns True if working)\"\"\"\n",
    "    if not url or url == \"None found\":\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        headers = {\"User-Agent\": random.choice(USER_AGENTS)}\n",
    "        response = requests.head(url, headers=headers, timeout=timeout, allow_redirects=True)\n",
    "        return response.status_code < 400\n",
    "    except:\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)\n",
    "            return response.status_code < 400\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "def process_urls(url_string):\n",
    "    \"\"\"Process and shorten URLs, filter out broken ones\"\"\"\n",
    "    if not url_string or url_string == \"None found\":\n",
    "        return url_string\n",
    "\n",
    "    urls = [url.strip() for url in url_string.split(\";\")]\n",
    "    working_urls = []\n",
    "\n",
    "    for url in urls:\n",
    "        if url and check_url_status(url):\n",
    "            working_urls.append(shorten_url(url))\n",
    "\n",
    "    return \"; \".join(working_urls) if working_urls else \"None found\"\n",
    "\n",
    "def is_third_party_job_site(url):\n",
    "    \"\"\"Check if URL is from a third-party job site\"\"\"\n",
    "    return any(site in url.lower() for site in THIRD_PARTY_JOB_SITES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41ba158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= GOOGLE MAPS API FUNCTIONS =================\n",
    "\n",
    "def search_places(query, location, max_results=50):\n",
    "    all_place_ids = []\n",
    "    all_names = []\n",
    "    all_addresses = []\n",
    "\n",
    "    params = {\n",
    "        \"query\": f\"{query} in {location}\",\n",
    "        \"key\": GOOGLE_API_KEY,\n",
    "    }\n",
    "\n",
    "    url = \"https://maps.googleapis.com/maps/api/place/textsearch/json\"\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(all_place_ids) < max_results:\n",
    "        if next_page_token:\n",
    "            params[\"pagetoken\"] = next_page_token\n",
    "            time.sleep(2)  # Required delay for next_page_token to become active\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        for result in data.get(\"results\", []):\n",
    "            all_place_ids.append(result[\"place_id\"])\n",
    "            all_names.append(result[\"name\"])\n",
    "            all_addresses.append(result.get(\"formatted_address\", \"\"))\n",
    "\n",
    "            if len(all_place_ids) >= max_results:\n",
    "                break\n",
    "\n",
    "        next_page_token = data.get(\"next_page_token\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return all_place_ids, all_names, all_addresses\n",
    "\n",
    "\n",
    "def get_place_details(place_id):\n",
    "    \"\"\"Get detailed information about a place\"\"\"\n",
    "    params = {\n",
    "        \"place_id\": place_id,\n",
    "        \"fields\": \"name,formatted_address,website\",\n",
    "        \"key\": GOOGLE_API_KEY,\n",
    "    }\n",
    "    response = requests.get(PLACE_DETAILS_URL, params=params)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    return response.json().get(\"result\", {})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d78380bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emails(text, soup=None):\n",
    "    pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
    "    emails = set(re.findall(pattern, text))\n",
    "\n",
    "    if soup:\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            href = a['href'].lower()\n",
    "            if href.startswith(\"mailto:\"):\n",
    "                email = href[7:].split(\"?\")[0].strip()\n",
    "                if re.match(pattern, email):\n",
    "                    emails.add(email)\n",
    "            visible = a.get_text(strip=True)\n",
    "            if re.match(pattern, visible):\n",
    "                emails.add(visible)\n",
    "\n",
    "    return list(emails)\n",
    "\n",
    "def categorize_emails(email_list):\n",
    "    best_keywords = [\n",
    "        \"careers@\", \"joinus@\", \"hr@\", \"ta@\", \"talentaquisition@\", \"humanresources@\", \"apply@\", \"jobs@\", \n",
    "        \"hiring@\", \"recruiting@\", \"recruitment@\", \"talent@\", \"talentteam@\", \"people@\", \"peopleops@\", \n",
    "        \"applications@\", \"submit@\", \"cv@\", \"resume@\", \"workwithus@\", \"jobshr@\", \"hrteam@\", \"recruiters@\", \n",
    "        \"talentmgmt@\", \"hiringteam@\", \"teamhr@\", \"opportunities@\", \"team@\", \"staffing@\", \"onboarding@\"\n",
    "    ]\n",
    "\n",
    "    better_keywords = [\n",
    "        \"info@\", \"contact@\", \"contactus@\", \"hello@\", \"admin@\", \"mail@\", \"hola@\", \"store@\", \"clinic@\", \"office@\"\n",
    "    ]\n",
    "\n",
    "    exclude_keywords = [\n",
    "        \"support@\", \"invoices@\", \"billing@\", \"guestservices@\", \"estimates@\", \n",
    "        \"sales@\", \"orders@\", \"customerservice@\"\n",
    "    ]\n",
    "\n",
    "    categorized = {\"BEST\": [], \"BETTER\": [], \"EXCLUDE\": []}\n",
    "\n",
    "    for email in email_list:\n",
    "        e = email.lower()\n",
    "\n",
    "        if any(x in e for x in exclude_keywords):\n",
    "            categorized[\"EXCLUDE\"].append(e)\n",
    "        elif any(x in e for x in best_keywords):\n",
    "            categorized[\"BEST\"].append(e)\n",
    "        elif any(x in e for x in better_keywords):\n",
    "            categorized[\"BETTER\"].append(e)\n",
    "        elif len(e.split(\"@\")[0]) <= 4:  # <== only now apply short-length heuristic (very conservative)\n",
    "            categorized[\"EXCLUDE\"].append(e)\n",
    "        else:\n",
    "            categorized[\"BETTER\"].append(e)  # fallback if unknown, but likely org email\n",
    "\n",
    "    return categorized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77ccb2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= PAGE DETECTION FUNCTIONS =================\n",
    "\n",
    "def detect_careers_page(url, text, soup):\n",
    "    \"\"\"Detect if this is a careers/jobs page and if it's internal\"\"\"\n",
    "    if is_third_party_job_site(url):\n",
    "        return False, \"\"\n",
    "\n",
    "    careers_indicators = [\n",
    "        \"career\", \"careers\", \"job\", \"jobs\", \"employment\", \"hiring\", \"positions\",\n",
    "        \"join our team\", \"work with us\", \"apply now\", \"open positions\"\n",
    "    ]\n",
    "\n",
    "    url_lower = url.lower()\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Check URL path\n",
    "    url_has_careers = any(indicator in url_lower for indicator in careers_indicators)\n",
    "\n",
    "    # Check page content\n",
    "    content_has_careers = any(indicator in text_lower for indicator in careers_indicators)\n",
    "\n",
    "    # Look for application forms\n",
    "    has_application_form = False\n",
    "    if soup:\n",
    "        forms = soup.find_all(\"form\")\n",
    "        for form in forms:\n",
    "            form_text = form.get_text().lower()\n",
    "            if any(word in form_text for word in [\"apply\", \"application\", \"resume\", \"cv\", \"position\"]):\n",
    "                has_application_form = True\n",
    "                break\n",
    "\n",
    "    if url_has_careers or (content_has_careers and has_application_form):\n",
    "        return True, url\n",
    "\n",
    "    return False, \"\"\n",
    "\n",
    "def detect_products_services_page(url, text, soup):\n",
    "    \"\"\"Detect pages showing products, services, or industries served\"\"\"\n",
    "    if not text:\n",
    "        return False, \"\"\n",
    "\n",
    "    # Keywords that indicate products/services pages\n",
    "    product_service_indicators = [\n",
    "        \"products\", \"services\", \"solutions\", \"offerings\", \"what we do\",\n",
    "        \"industries\", \"sectors\", \"specialties\", \"capabilities\", \"expertise\",\n",
    "        \"portfolio\", \"catalog\", \"menu\", \"pricing\", \"packages\"\n",
    "    ]\n",
    "\n",
    "    url_lower = url.lower()\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Check URL path\n",
    "    url_indicates_products = any(indicator in url_lower for indicator in product_service_indicators)\n",
    "\n",
    "    # Check if content has substantial product/service information\n",
    "    content_score = sum(1 for indicator in product_service_indicators if indicator in text_lower)\n",
    "\n",
    "    # Look for structured content (lists, grids, etc.)\n",
    "    has_structured_content = False\n",
    "    if soup:\n",
    "        lists = soup.find_all(['ul', 'ol', 'div'])\n",
    "        for element in lists:\n",
    "            element_text = element.get_text().lower()\n",
    "            if any(indicator in element_text for indicator in product_service_indicators):\n",
    "                has_structured_content = True\n",
    "                break\n",
    "\n",
    "    if url_indicates_products or (content_score >= 2 and has_structured_content):\n",
    "        return True, url\n",
    "\n",
    "    return False, \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25afc845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= EXTRACT SOCIAL MEDIA LINKS =================\n",
    "\n",
    "def extract_socials(soup):\n",
    "    \"\"\"Extract Instagram, Facebook, and X (Twitter) links \"\"\"\n",
    "    social_links = {\"Instagram\": \"\", \"Facebook\": \"\", \"X\": \"\"}\n",
    "\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        if \"instagram.com\" in href and not social_links[\"Instagram\"]:\n",
    "            social_links[\"Instagram\"] = href\n",
    "        elif \"facebook.com\" in href and not social_links[\"Facebook\"]:\n",
    "            social_links[\"Facebook\"] = href\n",
    "        elif \"twitter.com\" in href or \"x.com\" in href:\n",
    "            social_links[\"X\"] = href\n",
    "\n",
    "    return social_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f38a64d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= WEB CRAWLING =================\n",
    "\n",
    "def crawl_site_comprehensive(base_url, max_pages=15):\n",
    "    \"\"\"Comprehensive website crawling with categorized emails, social media, and Selenium fallback\"\"\"\n",
    "    visited = set()\n",
    "    queue = deque([base_url])\n",
    "    emails = set()\n",
    "    social_links = {\"Instagram\": \"\", \"Facebook\": \"\", \"X\": \"\"}\n",
    "    careers_pages = set()\n",
    "    products_services_pages = set()\n",
    "    debug_info = \"\"\n",
    "\n",
    "    domain = urlparse(base_url).netloc.replace(\"www.\", \"\")\n",
    "    pages_crawled = 0\n",
    "\n",
    "    try:\n",
    "        while queue and pages_crawled < max_pages:\n",
    "            url = queue.popleft()\n",
    "            if url in visited:\n",
    "                continue\n",
    "            visited.add(url)\n",
    "\n",
    "            headers = {\"User-Agent\": random.choice(USER_AGENTS)}\n",
    "            resp = requests.get(url, headers=headers, timeout=10)\n",
    "            resp.raise_for_status()\n",
    "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "            text = soup.get_text(\" \", strip=True)\n",
    "\n",
    "            emails.update(extract_emails(text, soup))\n",
    "\n",
    "            if not all(social_links.values()):\n",
    "                new_socials = extract_socials(soup)\n",
    "                for key in social_links:\n",
    "                    if not social_links[key] and new_socials[key]:\n",
    "                        social_links[key] = new_socials[key]\n",
    "\n",
    "            is_careers, careers_url = detect_careers_page(url, text, soup)\n",
    "            if is_careers:\n",
    "                careers_pages.add(careers_url)\n",
    "\n",
    "            is_products, products_url = detect_products_services_page(url, text, soup)\n",
    "            if is_products:\n",
    "                products_services_pages.add(products_url)\n",
    "\n",
    "            for a in soup.find_all(\"a\", href=True):\n",
    "                new_url = urljoin(url, a['href'])\n",
    "                parsed_url = urlparse(new_url)\n",
    "                if domain in parsed_url.netloc and new_url not in visited:\n",
    "                    queue.append(new_url)\n",
    "\n",
    "            pages_crawled += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        debug_info += f\"Requests failed: {type(e).__name__}: {str(e).split(':')[0]}. \"\n",
    "\n",
    "    # Retry with Selenium\n",
    "    if not emails or not all(social_links.values()):\n",
    "        try:\n",
    "            driver.set_page_load_timeout(30)\n",
    "            driver.get(base_url)\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            text = soup.get_text(\" \", strip=True)\n",
    "\n",
    "            new_emails = extract_emails(text, soup)\n",
    "            emails.update(new_emails)\n",
    "\n",
    "            new_socials = extract_socials(soup)\n",
    "            for key in social_links:\n",
    "                if not social_links[key] and new_socials[key]:\n",
    "                    social_links[key] = new_socials[key]\n",
    "\n",
    "            if not new_emails and not debug_info:\n",
    "                debug_info = \"Selenium used but still no emails found.\"\n",
    "\n",
    "        except Exception as e:\n",
    "            error_type = type(e).__name__\n",
    "            short_message = str(e).split(\":\")[0][:100]\n",
    "            debug_info += f\"Selenium failed ({error_type}): {short_message.strip()}.\"\n",
    "\n",
    "    if emails:\n",
    "        debug_info = \"\"\n",
    "    elif not debug_info:\n",
    "        debug_info = \"No emails found from requests or Selenium.\"\n",
    "\n",
    "    categorized = categorize_emails(list(emails))\n",
    "\n",
    "    return {\n",
    "        \"best_email\": categorized[\"BEST\"],\n",
    "        \"better_email\": categorized[\"BETTER\"],\n",
    "        \"excluded_email\": categorized[\"EXCLUDE\"],\n",
    "        \"careers_pages\": list(careers_pages),\n",
    "        \"products_services_pages\": list(products_services_pages),\n",
    "        \"social_links\": social_links,\n",
    "        \"debug_info\": debug_info,\n",
    "        \"pages_crawled\": pages_crawled\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5dc5dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= BUSINESS PROCESSING =================\n",
    "\n",
    "def process_business_comprehensive(place_id, name, fallback_address):\n",
    "    \"\"\"Process a business with categorized emails and links\"\"\"\n",
    "    print(f\"Processing business: {name}\")\n",
    "\n",
    "    details = get_place_details(place_id)\n",
    "    if not details:\n",
    "        return None\n",
    "\n",
    "    website = details.get(\"website\", \"\")\n",
    "    address = details.get(\"formatted_address\", fallback_address)\n",
    "\n",
    "    result = {\n",
    "        \"Company Name\": name,\n",
    "        \"Company Address\": address,\n",
    "        \"Company Website\": website,\n",
    "        \"Best Email\": \"\",\n",
    "        \"Better Email\": \"\",\n",
    "        \"Excluded Email\": \"\",\n",
    "        \"Company Careers Page\": \"\",\n",
    "        \"Company Products/Services Page\": \"\",\n",
    "        \"Instagram\": \"\",\n",
    "        \"Facebook\": \"\",\n",
    "        \"X\": \"\"\n",
    "    }\n",
    "\n",
    "    if website:\n",
    "        try:\n",
    "            crawl_results = crawl_site_comprehensive(website, MAX_PAGES_PER_SITE)\n",
    "\n",
    "            if crawl_results[\"best_email\"]:\n",
    "                result[\"Best Email\"] = \"; \".join(crawl_results[\"best_email\"])\n",
    "\n",
    "            if crawl_results[\"better_email\"]:\n",
    "                result[\"Better Email\"] = \"; \".join(crawl_results[\"better_email\"])\n",
    "\n",
    "            if crawl_results[\"excluded_email\"]:\n",
    "                result[\"Excluded Email\"] = \"; \".join(crawl_results[\"excluded_email\"])\n",
    "\n",
    "            if crawl_results[\"careers_pages\"]:\n",
    "                result[\"Company Careers Page\"] = \"; \".join(crawl_results[\"careers_pages\"])\n",
    "\n",
    "            if crawl_results[\"products_services_pages\"]:\n",
    "                result[\"Company Products/Services Page\"] = \"; \".join(crawl_results[\"products_services_pages\"])\n",
    "\n",
    "            socials = crawl_results[\"social_links\"]\n",
    "            result[\"Instagram\"] = socials.get(\"Instagram\", \"\")\n",
    "            result[\"Facebook\"] = socials.get(\"Facebook\", \"\")\n",
    "            result[\"X\"] = socials.get(\"X\", \"\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to crawl {website}: {e}\")\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "715b3863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= MAIN FUNCTION =================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"ðŸš€ Starting comprehensive business data scraper\")\n",
    "\n",
    "    lock = threading.Lock()\n",
    "    checkpoint_file = \"business_directory.csv\"\n",
    "\n",
    "    # Define CSV columns\n",
    "    csv_columns = [\n",
    "        \"Company Name\",\n",
    "        \"Company Address\",\n",
    "        \"Company Website\",\n",
    "        \"HR Email\",\n",
    "        \"Company Careers Page\",\n",
    "        \"Company Products/Services Page\",\n",
    "        \"Instagram\",\n",
    "        \"Facebook\",\n",
    "        \"X\"\n",
    "    ]\n",
    "\n",
    "    # Load existing data\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        existing_df = pd.read_csv(checkpoint_file)\n",
    "        print(f\"ðŸ“„ Loaded {len(existing_df)} existing records\")\n",
    "    else:\n",
    "        existing_df = pd.DataFrame(columns=csv_columns)\n",
    "\n",
    "    existing_data = {row['Company Name']: row for _, row in existing_df.iterrows()} if not existing_df.empty else {}\n",
    "\n",
    "    # crawl_times = []\n",
    "    metrics = {\n",
    "        \"total_businesses\": 0,\n",
    "        \"emails_found\": 0,\n",
    "        \"best_emails\": 0,\n",
    "        \"better_emails\": 0,\n",
    "        \"excluded_emails\": 0,\n",
    "        \"priority_email_used\": 0,\n",
    "        \"social_found\": 0,\n",
    "        \"all_socials_found\": 0,\n",
    "        \"nothing_scraped\": 0\n",
    "    }\n",
    "\n",
    "    for location in SEARCH_LOCATIONS:\n",
    "        for tag in SEARCH_TAGS:\n",
    "            print(f\"\\nðŸ” Searching for {tag} in {location}\")\n",
    "            place_ids, names, addresses  = search_places(tag, location)\n",
    "            print(f\"ðŸ“ Found {len(place_ids)} businesses\")\n",
    "\n",
    "            with ThreadPoolExecutor(max_workers=THREAD_POOL_WORKERS) as executor:\n",
    "                futures = {\n",
    "                    executor.submit(process_business_comprehensive, pid, name, addr): name\n",
    "                    for pid, name, addr in zip(place_ids, names, addresses)\n",
    "                    if name not in existing_data\n",
    "                }\n",
    "\n",
    "                for future in as_completed(futures):\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        with lock:\n",
    "                            # Apply email priority logic\n",
    "                            priority_email = \"\"\n",
    "                            if result[\"Best Email\"] and result[\"Best Email\"] != \"None found\":\n",
    "                                priority_email = result[\"Best Email\"].split(\";\")[0]\n",
    "                            elif result[\"Better Email\"] and result[\"Better Email\"] != \"None found\":\n",
    "                                priority_email = result[\"Better Email\"].split(\";\")[0]\n",
    "                            elif result[\"Excluded Email\"] and any(\"@\" in e and any(char.isalpha() for char in e.split(\"@\")[0]) for e in result[\"Excluded Email\"].split(\";\")):\n",
    "                                for e in result[\"Excluded Email\"].split(\";\"):\n",
    "                                    if \"@\" in e and any(char.isalpha() for char in e.split(\"@\")[0]):\n",
    "                                        priority_email = e\n",
    "                                        break\n",
    "\n",
    "                            result[\"HR Email\"] = priority_email\n",
    "\n",
    "                            # Clean URLs\n",
    "                            for field in [\"Company Careers Page\", \"Company Products/Services Page\"]:\n",
    "                                if result[field]:\n",
    "                                    result[field] = process_urls(result[field])\n",
    "                                # else:\n",
    "                                #     result[field] = \"None found\"\n",
    "\n",
    "                            # Fill blanks\n",
    "                            # for field in [\"Best Email\", \"Better Email\", \"Excluded Email\", \"Instagram\", \"Facebook\", \"X\", \"Priority Email\"]:\n",
    "                            #     if not result[field]:\n",
    "                            #         result[field] = \"None found\"\n",
    "\n",
    "                            # Update metrics\n",
    "                            metrics[\"total_businesses\"] += 1\n",
    "                            if any(result[f] != \"None found\" for f in [\"Best Email\", \"Better Email\", \"Excluded Email\"]):\n",
    "                                metrics[\"emails_found\"] += 1\n",
    "                            if result[\"Best Email\"] != \"None found\":\n",
    "                                metrics[\"best_emails\"] += 1\n",
    "                            if result[\"Better Email\"] != \"None found\":\n",
    "                                metrics[\"better_emails\"] += 1\n",
    "                            if result[\"Excluded Email\"] != \"None found\":\n",
    "                                metrics[\"excluded_emails\"] += 1\n",
    "                            if result[\"HR Email\"] != \"None found\":\n",
    "                                metrics[\"priority_email_used\"] += 1\n",
    "                            if any(result[f] != \"None found\" for f in [\"Instagram\", \"Facebook\", \"X\"]):\n",
    "                                metrics[\"social_found\"] += 1\n",
    "                            if all(result[f] != \"None found\" for f in [\"Instagram\", \"Facebook\", \"X\"]):\n",
    "                                metrics[\"all_socials_found\"] += 1\n",
    "                            if all(result[f] == \"None found\" for f in [\"Best Email\", \"Better Email\", \"Excluded Email\", \"Instagram\", \"Facebook\", \"X\"]):\n",
    "                                metrics[\"nothing_scraped\"] += 1\n",
    "\n",
    "                            # Save result\n",
    "                            existing_df = pd.concat([existing_df, pd.DataFrame([result])], ignore_index=True)\n",
    "                            existing_df.to_csv(checkpoint_file, index=False)\n",
    "                            print(f\"âœ… Saved: {result['Company Name']}\")\n",
    "\n",
    "    # Report\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ðŸ“Š FINAL METRICS REPORT\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    total = metrics[\"total_businesses\"]\n",
    "    def pct(v): return f\"{(v/total*100):.1f}%\" if total else \"0.0%\"\n",
    "\n",
    "    print(f\"ðŸ“ˆ Total businesses processed: {total}\")\n",
    "    print(f\"ðŸŒ Businesses with any emails: {metrics['emails_found']} ({pct(metrics['emails_found'])})\")\n",
    "    print(f\"ðŸ‘” Businesses with BEST emails: {metrics['best_emails']} ({pct(metrics['best_emails'])})\")\n",
    "    print(f\"ðŸ“§ Businesses with BETTER emails: {metrics['better_emails']} ({pct(metrics['better_emails'])})\")\n",
    "    print(f\"ðŸš« Businesses with EXCLUDED emails: {metrics['excluded_emails']} ({pct(metrics['excluded_emails'])})\")\n",
    "    print(f\"ðŸŽ¯ Businesses where Priority Email used: {metrics['priority_email_used']} ({pct(metrics['priority_email_used'])})\")\n",
    "    print(f\"ðŸ“± Businesses with any social media: {metrics['social_found']} ({pct(metrics['social_found'])})\")\n",
    "    print(f\"âœ”ï¸ Businesses with all 3 socials: {metrics['all_socials_found']} ({pct(metrics['all_socials_found'])})\")\n",
    "    print(f\"âŒ Businesses with no contact info: {metrics['nothing_scraped']} ({pct(metrics['nothing_scraped'])})\")\n",
    "    print(\"ðŸŽ‰ Scraping completed successfully!\")\n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2979bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting comprehensive business data scraper\n",
      "\n",
      "ðŸ” Searching for restaurant in Los Angeles\n",
      "ðŸ“ Found 50 businesses\n",
      "Processing business: Girl & the Goat Los Angeles\n",
      "Processing business: Perch\n",
      "Processing business: The Little Door\n",
      "Processing business: Bottega Louie\n",
      "Processing business: Water Grill\n",
      "Processing business: Bestia\n",
      "âœ… Saved: The Little Door\n",
      "Processing business: Chi Spacca\n",
      "Processing business: 71Above\n",
      "Processing business: RÃ©publique CafÃ© Bakery & RÃ©publique Restaurant\n",
      "âœ… Saved: Bottega Louie\n",
      "Processing business: Flemingâ€™s Prime Steakhouse & Wine Bar\n",
      "âœ… Saved: Water Grill\n",
      "âœ… Saved: 71Above\n",
      "Processing business: Redbird\n",
      "Processing business: JOEY DTLA\n",
      "âœ… Saved: Perch\n",
      "Processing business: Providence\n",
      "âœ… Saved: Girl & the Goat Los Angeles\n",
      "âœ… Saved: Bestia\n",
      "Processing business: Majordomo\n",
      "Processing business: Le Grand Restaurant\n",
      "âœ… Saved: RÃ©publique CafÃ© Bakery & RÃ©publique Restaurant\n",
      "Processing business: Guelaguetza Restaurant\n",
      "Processing business: Le Petit Paris\n",
      "Processing business: Cabra Los Angeles\n",
      "âœ… Saved: Chi Spacca\n",
      "âœ… Saved: Providence\n",
      "âœ… Saved: Flemingâ€™s Prime Steakhouse & Wine Bar\n",
      "âœ… Saved: Redbird\n",
      "âœ… Saved: Guelaguetza Restaurant\n",
      "Processing business: Moonlark's Dinette\n",
      "âœ… Saved: Le Grand Restaurant\n",
      "Processing business: Bavel\n",
      "âœ… Saved: JOEY DTLA\n",
      "Processing business: DAMA Fashion District - Downtown LA Restaurant & Bar\n",
      "Processing business: Eggslut\n",
      "Processing business: De La Nonna\n",
      "âœ… Saved: Cabra Los Angeles\n",
      "Processing business: Home Restaurant\n",
      "Processing business: Camphor\n",
      "âœ… Saved: Majordomo\n",
      "Processing business: Here's Looking At You\n",
      "âœ… Saved: Le Petit Paris\n",
      "Processing business: Orsa & Winston\n",
      "âœ… Saved: Moonlark's Dinette\n",
      "âœ… Saved: Bavel\n",
      "âœ… Saved: Home Restaurant\n",
      "Processing business: San Laurel\n",
      "âœ… Saved: DAMA Fashion District - Downtown LA Restaurant & Bar\n",
      "âœ… Saved: Eggslut\n",
      "Processing business: Maccheroni Republic\n",
      "âœ… Saved: Orsa & Winston\n",
      "Processing business: Damian\n",
      "âœ… Saved: Here's Looking At You\n",
      "Processing business: District\n",
      "Processing business: LA Prime Steakhouse\n",
      "Processing business: BROKEN MOUTH | Lee's Homestyle\n",
      "âœ… Saved: San Laurel\n",
      "âœ… Saved: Camphor\n",
      "Processing business: Ristorante Per Lâ€™Ora\n",
      "Processing business: L'APPART by AIR FOOD\n",
      "âœ… Saved: Damian\n",
      "âœ… Saved: LA Prime Steakhouse\n",
      "âœ… Saved: BROKEN MOUTH | Lee's Homestyle\n",
      "Processing business: Rossoblu\n",
      "âœ… Saved: Maccheroni Republic\n",
      "Processing business: Little Beast Restaurant\n",
      "Processing business: Hill Street Bar & Restaurant\n",
      "Processing business: The Exchange Restaurant\n",
      "âœ… Saved: District\n",
      "âœ… Saved: L'APPART by AIR FOOD\n",
      "âœ… Saved: Hill Street Bar & Restaurant\n",
      "Processing business: El Coyote\n",
      "Processing business: Barton G. The Restaurant Los Angeles\n",
      "âœ… Saved: Ristorante Per Lâ€™Ora\n",
      "âœ… Saved: Rossoblu\n",
      "Processing business: Americano\n",
      "âœ… Saved: El Coyote\n",
      "Processing business: SORA Craft Kitchen\n",
      "âœ… Saved: The Exchange Restaurant\n",
      "Processing business: Kendall's Brasserie\n",
      "Processing business: Cicada Restaurant and Lounge\n",
      "âœ… Saved: Little Beast Restaurant\n",
      "âœ… Saved: SORA Craft Kitchen\n",
      "Processing business: The L.A. Cafe\n",
      "Processing business: LALA'S Argentine Grill Downtown Los Angeles\n",
      "Processing business: Urth Caffe\n",
      "âœ… Saved: De La Nonna\n",
      "âœ… Saved: Cicada Restaurant and Lounge\n",
      "Processing business: Yang Chow Restaurant\n",
      "âœ… Saved: Kendall's Brasserie\n",
      "Processing business: n/naka\n",
      "âœ… Saved: The L.A. Cafe\n",
      "âœ… Saved: Urth Caffe\n",
      "âœ… Saved: Americano\n",
      "âœ… Saved: Barton G. The Restaurant Los Angeles\n",
      "âœ… Saved: n/naka\n",
      "âœ… Saved: LALA'S Argentine Grill Downtown Los Angeles\n",
      "âœ… Saved: Yang Chow Restaurant\n",
      "\n",
      "==================================================\n",
      "ðŸ“Š FINAL METRICS REPORT\n",
      "==================================================\n",
      "ðŸ“ˆ Total businesses processed: 50\n",
      "ðŸŒ Businesses with any emails: 50 (100.0%)\n",
      "ðŸ‘” Businesses with BEST emails: 50 (100.0%)\n",
      "ðŸ“§ Businesses with BETTER emails: 50 (100.0%)\n",
      "ðŸš« Businesses with EXCLUDED emails: 50 (100.0%)\n",
      "ðŸŽ¯ Businesses where Priority Email used: 50 (100.0%)\n",
      "ðŸ“± Businesses with any social media: 50 (100.0%)\n",
      "âœ”ï¸ Businesses with all 3 socials: 50 (100.0%)\n",
      "âŒ Businesses with no contact info: 0 (0.0%)\n",
      "ðŸŽ‰ Scraping completed successfully!\n",
      "==================================================\n",
      "ðŸ”§ Selenium driver closed successfully\n"
     ]
    }
   ],
   "source": [
    "# ================= CODE DRIVER METHOD =================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    finally:\n",
    "        try:\n",
    "            driver.quit()\n",
    "            print(\"ðŸ”§ Selenium driver closed successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error while closing Selenium driver: {type(e).__name__} - {str(e)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django--rgjyWHQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
