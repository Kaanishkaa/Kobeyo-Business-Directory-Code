{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "69668ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import threading\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import deque\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Constants\n",
    "GOOGLE_API_KEY = \"AIzaSyBXbfRXA9eJxdL3DmLt3TuDbCtvzP6RWLA\"\n",
    "TEXT_SEARCH_URL = \"https://maps.googleapis.com/maps/api/place/textsearch/json\"\n",
    "PLACE_DETAILS_URL = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
    "SEARCH_TAGS = [\"catering service\"]\n",
    "SEARCH_LOCATIONS = [\"Los Angeles\"]\n",
    "MAX_PAGES_PER_SITE = 15\n",
    "CAREERS_MAX_PAGES = 5\n",
    "THREAD_POOL_WORKERS = 5\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4fc560d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third-party job sites to exclude\n",
    "THIRD_PARTY_JOB_SITES = [\n",
    "    \"indeed.com\", \"ziprecruiter.com\", \"linkedin.com/jobs\", \"monster.com\",\n",
    "    \"glassdoor.com\", \"careerbuilder.com\", \"simplyhired.com\", \"dice.com\",\n",
    "    \"flexjobs.com\", \"upwork.com\", \"freelancer.com\", \"fiverr.com\",\n",
    "    \"snagajob.com\", \"workday.com\", \"bamboohr.com\", \"greenhouse.io\"\n",
    "]\n",
    "\n",
    "# URL Shortening Configuration\n",
    "MAX_URL_LENGTH = 80\n",
    "ELLIPSIS = \"...\"\n",
    "\n",
    "# Setup headless Chrome for Selenium\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "driver = webdriver.Chrome(options=chrome_options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d6a1b378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= URL PROCESSING FUNCTIONS =================\n",
    "\n",
    "def shorten_url(url, max_length=MAX_URL_LENGTH):\n",
    "    \"\"\"Shorten a URL if it exceeds the maximum length\"\"\"\n",
    "    if len(url) <= max_length:\n",
    "        return url\n",
    "\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        domain = parsed.netloc\n",
    "        scheme = parsed.scheme\n",
    "        base_url = f\"{scheme}://{domain}\"\n",
    "\n",
    "        if len(base_url) >= max_length - len(ELLIPSIS):\n",
    "            return base_url[:max_length - len(ELLIPSIS)] + ELLIPSIS\n",
    "\n",
    "        remaining_space = max_length - len(base_url) - len(ELLIPSIS)\n",
    "        if remaining_space > 0 and parsed.path:\n",
    "            path_part = parsed.path[:remaining_space]\n",
    "            return f\"{base_url}{path_part}{ELLIPSIS}\"\n",
    "        else:\n",
    "            return base_url\n",
    "\n",
    "    except Exception:\n",
    "        return url[:max_length - len(ELLIPSIS)] + ELLIPSIS\n",
    "\n",
    "def check_url_status(url, timeout=5):\n",
    "    \"\"\"Check if a URL is working/accessible (returns True if working)\"\"\"\n",
    "    if not url or url == \"None found\":\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        headers = {\"User-Agent\": random.choice(USER_AGENTS)}\n",
    "        response = requests.head(url, headers=headers, timeout=timeout, allow_redirects=True)\n",
    "        return response.status_code < 400\n",
    "    except:\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)\n",
    "            return response.status_code < 400\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "def process_urls(url_string):\n",
    "    \"\"\"Process and shorten URLs, filter out broken ones\"\"\"\n",
    "    if not url_string or url_string == \"None found\":\n",
    "        return url_string\n",
    "\n",
    "    urls = [url.strip() for url in url_string.split(\";\")]\n",
    "    working_urls = []\n",
    "\n",
    "    for url in urls:\n",
    "        if url and check_url_status(url):\n",
    "            working_urls.append(shorten_url(url))\n",
    "\n",
    "    return \"; \".join(working_urls) if working_urls else \"None found\"\n",
    "\n",
    "def is_third_party_job_site(url):\n",
    "    \"\"\"Check if URL is from a third-party job site\"\"\"\n",
    "    return any(site in url.lower() for site in THIRD_PARTY_JOB_SITES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "41ba158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= GOOGLE MAPS API FUNCTIONS =================\n",
    "\n",
    "def search_places(query, location, max_results=50):\n",
    "    all_place_ids = []\n",
    "    all_names = []\n",
    "    all_addresses = []\n",
    "\n",
    "    params = {\n",
    "        \"query\": f\"{query} in {location}\",\n",
    "        \"key\": GOOGLE_API_KEY,\n",
    "    }\n",
    "\n",
    "    url = \"https://maps.googleapis.com/maps/api/place/textsearch/json\"\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(all_place_ids) < max_results:\n",
    "        if next_page_token:\n",
    "            params[\"pagetoken\"] = next_page_token\n",
    "            time.sleep(2)  # Required delay for next_page_token to become active\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        for result in data.get(\"results\", []):\n",
    "            all_place_ids.append(result[\"place_id\"])\n",
    "            all_names.append(result[\"name\"])\n",
    "            all_addresses.append(result.get(\"formatted_address\", \"\"))\n",
    "\n",
    "            if len(all_place_ids) >= max_results:\n",
    "                break\n",
    "\n",
    "        next_page_token = data.get(\"next_page_token\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return all_place_ids, all_names, all_addresses\n",
    "\n",
    "\n",
    "def get_place_details(place_id):\n",
    "    \"\"\"Get detailed information about a place\"\"\"\n",
    "    params = {\n",
    "        \"place_id\": place_id,\n",
    "        \"fields\": \"name,formatted_address,website\",\n",
    "        \"key\": GOOGLE_API_KEY,\n",
    "    }\n",
    "    response = requests.get(PLACE_DETAILS_URL, params=params)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    return response.json().get(\"result\", {})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5bc6b355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_sales_emails(email_list):\n",
    "    best_keywords = [\n",
    "        \"sales@\", \"orders@\"\n",
    "    ]\n",
    "\n",
    "    better_keywords = [\n",
    "        \"info@\", \"contact@\", \"contactus@\", \"hello@\", \"admin@\", \"mail@\", \"support@\",\n",
    "        \"store@\", \"clinic@\", \"hola@\",  # generic or store-specific emails\n",
    "    ]\n",
    "\n",
    "    exclude_keywords = [\n",
    "        \"invoices@\", \"billing@\", \"guestservices@\", \"estimates@\"\n",
    "    ]\n",
    "\n",
    "    categorized = {\"BEST\": [], \"BETTER\": [], \"EXCLUDE\": []}\n",
    "\n",
    "    for email in email_list:\n",
    "        e = email.lower()\n",
    "\n",
    "        if any(x in e for x in exclude_keywords):\n",
    "            categorized[\"EXCLUDE\"].append(e)\n",
    "        elif any(x in e for x in best_keywords):\n",
    "            categorized[\"BEST\"].append(e)\n",
    "        elif any(x in e for x in better_keywords):\n",
    "            categorized[\"BETTER\"].append(e)\n",
    "        elif len(e.split(\"@\")[0]) <= 4 or re.match(r\".*@(gmail|yahoo|hotmail|outlook)\\.\", e):\n",
    "            categorized[\"EXCLUDE\"].append(e)\n",
    "        else:\n",
    "            categorized[\"BETTER\"].append(e)  # fallback if unknown\n",
    "\n",
    "    return categorized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d78380bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emails(text, soup=None):\n",
    "    pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
    "    emails = set(re.findall(pattern, text))\n",
    "\n",
    "    if soup:\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            href = a['href'].lower()\n",
    "            if href.startswith(\"mailto:\"):\n",
    "                email = href[7:].split(\"?\")[0].strip()\n",
    "                if re.match(pattern, email):\n",
    "                    emails.add(email)\n",
    "            visible = a.get_text(strip=True)\n",
    "            if re.match(pattern, visible):\n",
    "                emails.add(visible)\n",
    "\n",
    "    return list(emails)\n",
    "\n",
    "def categorize_emails(email_list):\n",
    "    best_keywords = [\n",
    "        \"careers@\", \"joinus@\", \"hr@\", \"ta@\", \"talentaquisition@\", \"humanresources@\", \"apply@\", \"jobs@\", \n",
    "        \"hiring@\", \"recruiting@\", \"recruitment@\", \"talent@\", \"talentteam@\", \"people@\", \"peopleops@\", \n",
    "        \"applications@\", \"submit@\", \"cv@\", \"resume@\", \"workwithus@\", \"jobshr@\", \"hrteam@\", \"recruiters@\", \n",
    "        \"talentmgmt@\", \"hiringteam@\", \"teamhr@\", \"opportunities@\", \"team@\", \"staffing@\", \"onboarding@\"\n",
    "    ]\n",
    "\n",
    "    better_keywords = [\n",
    "        \"info@\", \"contact@\", \"contactus@\", \"hello@\", \"admin@\", \"mail@\", \"hola@\", \"store@\", \"clinic@\", \"office@\"\n",
    "    ]\n",
    "\n",
    "    exclude_keywords = [\n",
    "        \"support@\", \"invoices@\", \"billing@\", \"guestservices@\", \"estimates@\", \n",
    "        \"sales@\", \"orders@\", \"customerservice@\"\n",
    "    ]\n",
    "\n",
    "    categorized = {\"BEST\": [], \"BETTER\": [], \"EXCLUDE\": []}\n",
    "\n",
    "    for email in email_list:\n",
    "        e = email.lower()\n",
    "\n",
    "        if any(x in e for x in exclude_keywords):\n",
    "            categorized[\"EXCLUDE\"].append(e)\n",
    "        elif any(x in e for x in best_keywords):\n",
    "            categorized[\"BEST\"].append(e)\n",
    "        elif any(x in e for x in better_keywords):\n",
    "            categorized[\"BETTER\"].append(e)\n",
    "        elif len(e.split(\"@\")[0]) <= 4:  # <== only now apply short-length heuristic (very conservative)\n",
    "            categorized[\"EXCLUDE\"].append(e)\n",
    "        else:\n",
    "            categorized[\"BETTER\"].append(e)  # fallback if unknown, but likely org email\n",
    "\n",
    "    return categorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "77ccb2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= PAGE DETECTION FUNCTIONS =================\n",
    "\n",
    "def detect_careers_page(url, text, soup):\n",
    "    \"\"\"Detect if this is a careers/jobs page and if it's internal\"\"\"\n",
    "    if is_third_party_job_site(url):\n",
    "        return False, \"\"\n",
    "\n",
    "    careers_indicators = [\n",
    "        \"career\", \"careers\", \"job\", \"jobs\", \"employment\", \"hiring\", \"positions\",\n",
    "        \"join our team\", \"work with us\", \"apply now\", \"open positions\"\n",
    "    ]\n",
    "\n",
    "    url_lower = url.lower()\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Check URL path\n",
    "    url_has_careers = any(indicator in url_lower for indicator in careers_indicators)\n",
    "\n",
    "    # Check page content\n",
    "    content_has_careers = any(indicator in text_lower for indicator in careers_indicators)\n",
    "\n",
    "    # Look for application forms\n",
    "    has_application_form = False\n",
    "    if soup:\n",
    "        forms = soup.find_all(\"form\")\n",
    "        for form in forms:\n",
    "            form_text = form.get_text().lower()\n",
    "            if any(word in form_text for word in [\"apply\", \"application\", \"resume\", \"cv\", \"position\"]):\n",
    "                has_application_form = True\n",
    "                break\n",
    "\n",
    "    if url_has_careers or (content_has_careers and has_application_form):\n",
    "        return True, url\n",
    "\n",
    "    return False, \"\"\n",
    "\n",
    "def detect_products_services_page(url, text, soup):\n",
    "    \"\"\"Detect pages showing products, services, or industries served\"\"\"\n",
    "    if not text:\n",
    "        return False, \"\"\n",
    "\n",
    "    # Keywords that indicate products/services pages\n",
    "    product_service_indicators = [\n",
    "        \"products\", \"services\", \"solutions\", \"offerings\", \"what we do\",\n",
    "        \"industries\", \"sectors\", \"specialties\", \"capabilities\", \"expertise\",\n",
    "        \"portfolio\", \"catalog\", \"menu\", \"pricing\", \"packages\"\n",
    "    ]\n",
    "\n",
    "    url_lower = url.lower()\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Check URL path\n",
    "    url_indicates_products = any(indicator in url_lower for indicator in product_service_indicators)\n",
    "\n",
    "    # Check if content has substantial product/service information\n",
    "    content_score = sum(1 for indicator in product_service_indicators if indicator in text_lower)\n",
    "\n",
    "    # Look for structured content (lists, grids, etc.)\n",
    "    has_structured_content = False\n",
    "    if soup:\n",
    "        lists = soup.find_all(['ul', 'ol', 'div'])\n",
    "        for element in lists:\n",
    "            element_text = element.get_text().lower()\n",
    "            if any(indicator in element_text for indicator in product_service_indicators):\n",
    "                has_structured_content = True\n",
    "                break\n",
    "\n",
    "    if url_indicates_products or (content_score >= 2 and has_structured_content):\n",
    "        return True, url\n",
    "\n",
    "    return False, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "25afc845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= EXTRACT SOCIAL MEDIA LINKS =================\n",
    "\n",
    "def extract_socials(soup):\n",
    "    \"\"\"Extract Instagram, Facebook, and X (Twitter) links \"\"\"\n",
    "    social_links = {\"Instagram\": \"\", \"Facebook\": \"\", \"X\": \"\"}\n",
    "\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        if \"instagram.com\" in href and not social_links[\"Instagram\"]:\n",
    "            social_links[\"Instagram\"] = href\n",
    "        elif \"facebook.com\" in href and not social_links[\"Facebook\"]:\n",
    "            social_links[\"Facebook\"] = href\n",
    "        elif \"twitter.com\" in href or \"x.com\" in href:\n",
    "            social_links[\"X\"] = href\n",
    "\n",
    "    return social_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f38a64d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= WEB CRAWLING =================\n",
    "\n",
    "def crawl_site_comprehensive(base_url, max_pages=15):\n",
    "    \"\"\"Comprehensive website crawling with categorized emails, social media, and Selenium fallback\"\"\"\n",
    "    visited = set()\n",
    "    queue = deque([base_url])\n",
    "    emails = set()\n",
    "    social_links = {\"Instagram\": \"\", \"Facebook\": \"\", \"X\": \"\"}\n",
    "    careers_pages = set()\n",
    "    products_services_pages = set()\n",
    "    debug_info = \"\"\n",
    "\n",
    "    domain = urlparse(base_url).netloc.replace(\"www.\", \"\")\n",
    "    pages_crawled = 0\n",
    "\n",
    "    try:\n",
    "        while queue and pages_crawled < max_pages:\n",
    "            url = queue.popleft()\n",
    "            if url in visited:\n",
    "                continue\n",
    "            visited.add(url)\n",
    "\n",
    "            headers = {\"User-Agent\": random.choice(USER_AGENTS)}\n",
    "            resp = requests.get(url, headers=headers, timeout=10)\n",
    "            resp.raise_for_status()\n",
    "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "            text = soup.get_text(\" \", strip=True)\n",
    "\n",
    "            emails.update(extract_emails(text, soup))\n",
    "\n",
    "            if not all(social_links.values()):\n",
    "                new_socials = extract_socials(soup)\n",
    "                for key in social_links:\n",
    "                    if not social_links[key] and new_socials[key]:\n",
    "                        social_links[key] = new_socials[key]\n",
    "\n",
    "            is_careers, careers_url = detect_careers_page(url, text, soup)\n",
    "            if is_careers:\n",
    "                careers_pages.add(careers_url)\n",
    "\n",
    "            is_products, products_url = detect_products_services_page(url, text, soup)\n",
    "            if is_products:\n",
    "                products_services_pages.add(products_url)\n",
    "\n",
    "            for a in soup.find_all(\"a\", href=True):\n",
    "                new_url = urljoin(url, a['href'])\n",
    "                parsed_url = urlparse(new_url)\n",
    "                if domain in parsed_url.netloc and new_url not in visited:\n",
    "                    queue.append(new_url)\n",
    "\n",
    "            pages_crawled += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        debug_info += f\"Requests failed: {type(e).__name__}: {str(e).split(':')[0]}. \"\n",
    "\n",
    "    # Retry with Selenium\n",
    "    if not emails or not all(social_links.values()):\n",
    "        try:\n",
    "            driver.set_page_load_timeout(30)\n",
    "            driver.get(base_url)\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            text = soup.get_text(\" \", strip=True)\n",
    "\n",
    "            new_emails = extract_emails(text, soup)\n",
    "            emails.update(new_emails)\n",
    "\n",
    "            new_socials = extract_socials(soup)\n",
    "            for key in social_links:\n",
    "                if not social_links[key] and new_socials[key]:\n",
    "                    social_links[key] = new_socials[key]\n",
    "\n",
    "            if not new_emails and not debug_info:\n",
    "                debug_info = \"Selenium used but still no emails found.\"\n",
    "\n",
    "        except Exception as e:\n",
    "            error_type = type(e).__name__\n",
    "            short_message = str(e).split(\":\")[0][:100]\n",
    "            debug_info += f\"Selenium failed ({error_type}): {short_message.strip()}.\"\n",
    "\n",
    "    if emails:\n",
    "        debug_info = \"\"\n",
    "    elif not debug_info:\n",
    "        debug_info = \"No emails found from requests or Selenium.\"\n",
    "\n",
    "    categorized = categorize_emails(list(emails))\n",
    "    sales_categorized = categorize_sales_emails(list(emails))\n",
    "\n",
    "    return {\n",
    "    \"best_email\": categorized[\"BEST\"],\n",
    "    \"better_email\": categorized[\"BETTER\"],\n",
    "    \"excluded_email\": categorized[\"EXCLUDE\"],\n",
    "    \"sales_best\": sales_categorized[\"BEST\"],\n",
    "    \"sales_better\": sales_categorized[\"BETTER\"],\n",
    "    \"sales_exclude\": sales_categorized[\"EXCLUDE\"],\n",
    "    \"careers_pages\": list(careers_pages),\n",
    "    \"products_services_pages\": list(products_services_pages),\n",
    "    \"social_links\": social_links,\n",
    "    \"debug_info\": debug_info,\n",
    "    \"pages_crawled\": pages_crawled\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5dc5dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= BUSINESS PROCESSING =================\n",
    "\n",
    "def process_business_comprehensive(place_id, name, fallback_address):\n",
    "    \"\"\"Process a business with categorized emails and links\"\"\"\n",
    "    print(f\"Processing business: {name}\")\n",
    "\n",
    "    details = get_place_details(place_id)\n",
    "    if not details:\n",
    "        return None\n",
    "\n",
    "    website = details.get(\"website\", \"\")\n",
    "    address = details.get(\"formatted_address\", fallback_address)\n",
    "\n",
    "    result = {\n",
    "        \"Company Name\": name,\n",
    "        \"Company Address\": address,\n",
    "        \"Company Website\": website,\n",
    "        \"Best Email\": \"\",\n",
    "        \"Better Email\": \"\",\n",
    "        \"Excluded Email\": \"\",\n",
    "        \"Sales BEST\": \"\",\n",
    "        \"Sales BETTER\": \"\",\n",
    "        \"Sales EXCLUDE\": \"\",    \n",
    "        \"Company Careers Page\": \"\",\n",
    "        \"Company Products/Services Page\": \"\",\n",
    "        \"Instagram\": \"\",\n",
    "        \"Facebook\": \"\",\n",
    "        \"X\": \"\"\n",
    "    }\n",
    "\n",
    "    if website:\n",
    "        try:\n",
    "            crawl_results = crawl_site_comprehensive(website, MAX_PAGES_PER_SITE)\n",
    "\n",
    "            if crawl_results[\"best_email\"]:\n",
    "                result[\"Best Email\"] = \"; \".join(crawl_results[\"best_email\"])\n",
    "\n",
    "            if crawl_results[\"better_email\"]:\n",
    "                result[\"Better Email\"] = \"; \".join(crawl_results[\"better_email\"])\n",
    "\n",
    "            if crawl_results[\"excluded_email\"]:\n",
    "                result[\"Excluded Email\"] = \"; \".join(crawl_results[\"excluded_email\"])\n",
    "            \n",
    "            if crawl_results[\"sales_best\"]:\n",
    "                result[\"Sales BEST\"] = \"; \".join(crawl_results[\"sales_best\"])\n",
    "\n",
    "            if crawl_results[\"sales_better\"]:\n",
    "                result[\"Sales BETTER\"] = \"; \".join(crawl_results[\"sales_better\"])\n",
    "\n",
    "            if crawl_results[\"sales_exclude\"]:\n",
    "                result[\"Sales EXCLUDE\"] = \"; \".join(crawl_results[\"sales_exclude\"])\n",
    "\n",
    "            if crawl_results[\"careers_pages\"]:\n",
    "                result[\"Company Careers Page\"] = \"; \".join(crawl_results[\"careers_pages\"])\n",
    "\n",
    "            if crawl_results[\"products_services_pages\"]:\n",
    "                result[\"Company Products/Services Page\"] = \"; \".join(crawl_results[\"products_services_pages\"])\n",
    "\n",
    "\n",
    "            socials = crawl_results[\"social_links\"]\n",
    "            result[\"Instagram\"] = socials.get(\"Instagram\", \"\")\n",
    "            result[\"Facebook\"] = socials.get(\"Facebook\", \"\")\n",
    "            result[\"X\"] = socials.get(\"X\", \"\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to crawl {website}: {e}\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "715b3863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= MAIN FUNCTION =================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"üöÄ Starting comprehensive business data scraper\")\n",
    "\n",
    "    lock = threading.Lock()\n",
    "    checkpoint_file = \"business_directory.csv\"\n",
    "\n",
    "    # Define CSV columns\n",
    "    csv_columns = [\n",
    "        \"Company Name\",\n",
    "        \"Company Address\",\n",
    "        \"Company Website\",\n",
    "        \"HR Email\",\n",
    "        \"Sales BEST\",\n",
    "        \"Sales BETTER\",\n",
    "        \"Sales EXCLUDE\",\n",
    "        \"Company Careers Page\",\n",
    "        \"Company Products/Services Page\",\n",
    "        \"Instagram\",\n",
    "        \"Facebook\",\n",
    "        \"X\"\n",
    "    ]\n",
    "\n",
    "    # Load existing data\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        existing_df = pd.read_csv(checkpoint_file)\n",
    "        print(f\"üìÑ Loaded {len(existing_df)} existing records\")\n",
    "    else:\n",
    "        existing_df = pd.DataFrame(columns=csv_columns)\n",
    "\n",
    "    existing_data = {row['Company Name']: row for _, row in existing_df.iterrows()} if not existing_df.empty else {}\n",
    "\n",
    "    # crawl_times = []\n",
    "    metrics = {\n",
    "        \"total_businesses\": 0,\n",
    "        \"emails_found\": 0,\n",
    "        \"best_emails\": 0,\n",
    "        \"better_emails\": 0,\n",
    "        \"excluded_emails\": 0,\n",
    "        \"priority_email_used\": 0,\n",
    "        \"social_found\": 0,\n",
    "        \"all_socials_found\": 0,\n",
    "        \"nothing_scraped\": 0\n",
    "    }\n",
    "\n",
    "    for location in SEARCH_LOCATIONS:\n",
    "        for tag in SEARCH_TAGS:\n",
    "            print(f\"\\nüîç Searching for {tag} in {location}\")\n",
    "            place_ids, names, addresses  = search_places(tag, location)\n",
    "            print(f\"üìç Found {len(place_ids)} businesses\")\n",
    "\n",
    "            with ThreadPoolExecutor(max_workers=THREAD_POOL_WORKERS) as executor:\n",
    "                futures = {\n",
    "                    executor.submit(process_business_comprehensive, pid, name, addr): name\n",
    "                    for pid, name, addr in zip(place_ids, names, addresses)\n",
    "                    if name not in existing_data\n",
    "                }\n",
    "\n",
    "                for future in as_completed(futures):\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        with lock:\n",
    "                            # Apply email priority logic\n",
    "                            priority_email = \"\"\n",
    "                            if result[\"Best Email\"] and result[\"Best Email\"] != \"None found\":\n",
    "                                priority_email = result[\"Best Email\"].split(\";\")[0]\n",
    "                            elif result[\"Better Email\"] and result[\"Better Email\"] != \"None found\":\n",
    "                                priority_email = result[\"Better Email\"].split(\";\")[0]\n",
    "                            elif result[\"Excluded Email\"] and any(\"@\" in e and any(char.isalpha() for char in e.split(\"@\")[0]) for e in result[\"Excluded Email\"].split(\";\")):\n",
    "                                for e in result[\"Excluded Email\"].split(\";\"):\n",
    "                                    if \"@\" in e and any(char.isalpha() for char in e.split(\"@\")[0]):\n",
    "                                        priority_email = e\n",
    "                                        break\n",
    "\n",
    "                            result[\"HR Email\"] = priority_email\n",
    "\n",
    "                            # Clean URLs\n",
    "                            for field in [\"Company Careers Page\", \"Company Products/Services Page\"]:\n",
    "                                if result[field]:\n",
    "                                    result[field] = process_urls(result[field])\n",
    "                                # else:\n",
    "                                #     result[field] = \"None found\"\n",
    "\n",
    "                            # Fill blanks\n",
    "                            # for field in [\"Best Email\", \"Better Email\", \"Excluded Email\", \"Instagram\", \"Facebook\", \"X\", \"Priority Email\"]:\n",
    "                            #     if not result[field]:\n",
    "                            #         result[field] = \"None found\"\n",
    "\n",
    "                            # Update metrics\n",
    "                            metrics[\"total_businesses\"] += 1\n",
    "                            if any(result[f] != \"None found\" for f in [\"Best Email\", \"Better Email\", \"Excluded Email\"]):\n",
    "                                metrics[\"emails_found\"] += 1\n",
    "                            if result[\"Best Email\"] != \"None found\":\n",
    "                                metrics[\"best_emails\"] += 1\n",
    "                            if result[\"Better Email\"] != \"None found\":\n",
    "                                metrics[\"better_emails\"] += 1\n",
    "                            if result[\"Excluded Email\"] != \"None found\":\n",
    "                                metrics[\"excluded_emails\"] += 1\n",
    "                            if result[\"HR Email\"] != \"None found\":\n",
    "                                metrics[\"priority_email_used\"] += 1\n",
    "                            if any(result[f] != \"None found\" for f in [\"Instagram\", \"Facebook\", \"X\"]):\n",
    "                                metrics[\"social_found\"] += 1\n",
    "                            if all(result[f] != \"None found\" for f in [\"Instagram\", \"Facebook\", \"X\"]):\n",
    "                                metrics[\"all_socials_found\"] += 1\n",
    "                            if all(result[f] == \"None found\" for f in [\"Best Email\", \"Better Email\", \"Excluded Email\", \"Instagram\", \"Facebook\", \"X\"]):\n",
    "                                metrics[\"nothing_scraped\"] += 1\n",
    "\n",
    "                            for col in [\"Best Email\", \"Better Email\", \"Excluded Email\"]:\n",
    "                                if col in result:\n",
    "                                    del result[col]\n",
    "                            # Save result\n",
    "                            existing_df = pd.concat([existing_df, pd.DataFrame([result])], ignore_index=True)\n",
    "                            existing_df.to_csv(checkpoint_file, index=False)\n",
    "                            print(f\"‚úÖ Saved: {result['Company Name']}\")\n",
    "\n",
    "    # Report\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"üìä FINAL METRICS REPORT\")\n",
    "    print(\"=\" * 50)\n",
    "        # ‚úÖ Clean URLs by removing query strings\n",
    "    def clean_url_query(url):\n",
    "        if pd.isna(url):\n",
    "            return url\n",
    "        return \"; \".join(u.split(\"?\")[0] for u in url.split(\";\"))\n",
    "\n",
    "    columns_to_clean = [\n",
    "        \"Company Website\",\n",
    "        \"Company Careers Page\",\n",
    "        \"Company Products/Services Page\",\n",
    "        \"Instagram\",\n",
    "        \"Facebook\",\n",
    "        \"X\"\n",
    "    ]\n",
    "\n",
    "    for col in columns_to_clean:\n",
    "        if col in existing_df.columns:\n",
    "            existing_df[col] = existing_df[col].apply(clean_url_query)\n",
    "\n",
    "    # ‚úÖ Save cleaned file\n",
    "    existing_df.to_csv(\"business_directory_cleaned.csv\", index=False)\n",
    "    print(\"üßº Cleaned URLs and saved to 'business_directory_cleaned.csv'\")        \n",
    "\n",
    "    total = metrics[\"total_businesses\"]\n",
    "    def pct(v): return f\"{(v/total*100):.1f}%\" if total else \"0.0%\"\n",
    "\n",
    "    print(f\"üìà Total businesses processed: {total}\")\n",
    "    print(f\"üåê Businesses with any emails: {metrics['emails_found']} ({pct(metrics['emails_found'])})\")\n",
    "    print(f\"üëî Businesses with BEST emails: {metrics['best_emails']} ({pct(metrics['best_emails'])})\")\n",
    "    print(f\"üìß Businesses with BETTER emails: {metrics['better_emails']} ({pct(metrics['better_emails'])})\")\n",
    "    print(f\"üö´ Businesses with EXCLUDED emails: {metrics['excluded_emails']} ({pct(metrics['excluded_emails'])})\")\n",
    "    print(f\"üéØ Businesses where Priority Email used: {metrics['priority_email_used']} ({pct(metrics['priority_email_used'])})\")\n",
    "    print(f\"üì± Businesses with any social media: {metrics['social_found']} ({pct(metrics['social_found'])})\")\n",
    "    print(f\"‚úîÔ∏è Businesses with all 3 socials: {metrics['all_socials_found']} ({pct(metrics['all_socials_found'])})\")\n",
    "    print(f\"‚ùå Businesses with no contact info: {metrics['nothing_scraped']} ({pct(metrics['nothing_scraped'])})\")\n",
    "    print(\"üéâ Scraping completed successfully!\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e2979bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting comprehensive business data scraper\n",
      "\n",
      "üîç Searching for catering service in Los Angeles\n",
      "üìç Found 34 businesses\n",
      "Processing business: Bite Catering Couture\n",
      "Processing business: Marbled LA\n",
      "Processing business: Crateful Catering Los Angeles\n",
      "Processing business: Luxe Bites - LA's Best Charcuterie Boards and Event Catering\n",
      "Processing business: Good Heart Catering\n",
      "Processing business: Robert's Catering Services\n",
      "‚úÖ Saved: Marbled LA\n",
      "Processing business: Las Hermanas Catering\n",
      "Processing business: Chubby Fingers Catering Co\n",
      "‚úÖ Saved: Good Heart Catering\n",
      "Processing business: Heirloom LA\n",
      "Processing business: Spotted Hen Catering\n",
      "‚úÖ Saved: Luxe Bites - LA's Best Charcuterie Boards and Event Catering\n",
      "Processing business: Aloha Catering Services Inc\n",
      "‚úÖ Saved: Crateful Catering Los Angeles\n",
      "Processing business: Felice Italian Catering\n",
      "‚úÖ Saved: Bite Catering Couture\n",
      "‚úÖ Saved: Chubby Fingers Catering Co\n",
      "‚úÖ Saved: Aloha Catering Services Inc\n",
      "Processing business: TGIS Catering Services\n",
      "‚úÖ Saved: Heirloom LA\n",
      "Processing business: Bites and Bashes Catering\n",
      "Processing business: The Daily by HC\n",
      "Processing business: Catering by Emuna\n",
      "‚úÖ Saved: Las Hermanas Catering\n",
      "‚úÖ Saved: Spotted Hen Catering\n",
      "Processing business: Schaffer\n",
      "‚úÖ Saved: Felice Italian Catering\n",
      "Processing business: K Michelle's Kitchen Catering\n",
      "Processing business: Basil Pizza Bar Catering\n",
      "‚úÖ Saved: Robert's Catering Services\n",
      "Processing business: Ask 4 Tacos Catering\n",
      "Processing business: Simon's Caterers\n",
      "Processing business: Haute Chefs Los Angeles\n",
      "‚úÖ Saved: TGIS Catering Services\n",
      "Processing business: OFF THE SHELF CATERING\n",
      "Processing business: Paulina's Catering\n",
      "‚úÖ Saved: Bites and Bashes Catering\n",
      "‚úÖ Saved: Basil Pizza Bar Catering\n",
      "Processing business: Lalo's Catering Company\n",
      "Processing business: Taste And Company\n",
      "Processing business: Alfredo Catering Inc\n",
      "‚úÖ Saved: The Daily by HC\n",
      "Processing business: Castle's Southern and Creole Catering\n",
      "Processing business: Karla's Coffee Cart Catering\n",
      "‚úÖ Saved: Schaffer\n",
      "Processing business: Serves You Right! Catering\n",
      "‚úÖ Saved: Simon's Caterers\n",
      "Processing business: Four Seasons Catering\n",
      "‚úÖ Saved: K Michelle's Kitchen Catering\n",
      "‚úÖ Saved: Ask 4 Tacos Catering\n",
      "‚úÖ Saved: Paulina's Catering\n",
      "Processing business: Team Butler Inc. (DBA) The Butler Pantry Catering Co.\n",
      "‚úÖ Saved: OFF THE SHELF CATERING\n",
      "Processing business: LA Spice Catering\n",
      "‚úÖ Saved: Haute Chefs Los Angeles\n",
      "‚úÖ Saved: Castle's Southern and Creole Catering\n",
      "‚úÖ Saved: Lalo's Catering Company\n",
      "‚úÖ Saved: Karla's Coffee Cart Catering\n",
      "Processing business: Tr√®s LA Catering\n",
      "‚úÖ Saved: Catering by Emuna\n",
      "‚úÖ Saved: Taste And Company\n",
      "‚úÖ Saved: Alfredo Catering Inc\n",
      "‚úÖ Saved: Four Seasons Catering\n",
      "‚úÖ Saved: Serves You Right! Catering\n",
      "‚úÖ Saved: Tr√®s LA Catering\n",
      "‚úÖ Saved: LA Spice Catering\n",
      "‚úÖ Saved: Team Butler Inc. (DBA) The Butler Pantry Catering Co.\n",
      "\n",
      "==================================================\n",
      "üìä FINAL METRICS REPORT\n",
      "==================================================\n",
      "üßº Cleaned URLs and saved to 'business_directory_cleaned.csv'\n",
      "üìà Total businesses processed: 34\n",
      "üåê Businesses with any emails: 34 (100.0%)\n",
      "üëî Businesses with BEST emails: 34 (100.0%)\n",
      "üìß Businesses with BETTER emails: 34 (100.0%)\n",
      "üö´ Businesses with EXCLUDED emails: 34 (100.0%)\n",
      "üéØ Businesses where Priority Email used: 34 (100.0%)\n",
      "üì± Businesses with any social media: 34 (100.0%)\n",
      "‚úîÔ∏è Businesses with all 3 socials: 34 (100.0%)\n",
      "‚ùå Businesses with no contact info: 0 (0.0%)\n",
      "üéâ Scraping completed successfully!\n",
      "==================================================\n",
      "üîß Selenium driver closed successfully\n"
     ]
    }
   ],
   "source": [
    "# ================= CODE DRIVER METHOD =================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    finally:\n",
    "        try:\n",
    "            driver.quit()\n",
    "            print(\"üîß Selenium driver closed successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error while closing Selenium driver: {type(e).__name__} - {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb543d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25e8177d",
   "metadata": {},
   "source": [
    "# Tagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42edfa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "import threading\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "from thefuzz import fuzz\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cf85727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 1. Configuration\n",
    "# ==============================================================================\n",
    "\n",
    "# --- File Configuration ---\n",
    "INPUT_FILENAME = 'business_directory_cleaned.csv'\n",
    "OUTPUT_FILENAME = 'final_tagged_catering_businesses.csv'\n",
    "\n",
    "# --- Tagging Configuration ---\n",
    "CATERING_KEYWORDS = [\"catering\", \"cater\", \"catered\", \"private events\", \"functions\", \"banquets\", \"private dining\"]\n",
    "CATERING_SKILL_ID = 11\n",
    "\n",
    "# --- Selenium Setup ---\n",
    "# We need a thread-safe way to use Selenium drivers. Each thread gets its own driver.\n",
    "thread_local_driver = threading.local()\n",
    "\n",
    "def get_driver():\n",
    "    \"\"\"Creates a new Selenium driver instance for the current thread.\"\"\"\n",
    "    if not hasattr(thread_local_driver, 'driver'):\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        # This can help prevent some detection mechanisms\n",
    "        chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "        thread_local_driver.driver = webdriver.Chrome(options=chrome_options)\n",
    "    return thread_local_driver.driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec417a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# 2. Scraper and Tagger Logic\n",
    "# ==============================================================================\n",
    "\n",
    "def scrape_and_tag_website(url):\n",
    "    \"\"\"\n",
    "    Visits a URL with Selenium, scrapes its text and links, \n",
    "    and checks for catering keywords using both direct and fuzzy matching.\n",
    "    \"\"\"\n",
    "    if not isinstance(url, str) or not url.startswith('http'):\n",
    "        return '' # Return empty if the URL is invalid\n",
    "\n",
    "    driver = get_driver()\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        # A small delay can be crucial for JavaScript-heavy sites to load content\n",
    "        time.sleep(2)\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # --- Analysis Method 1: Search all visible text on the page ---\n",
    "        page_text = soup.get_text(\" \", strip=True).lower()\n",
    "        if any(keyword in page_text for keyword in CATERING_KEYWORDS):\n",
    "            return CATERING_SKILL_ID\n",
    "\n",
    "        # --- Analysis Method 2: Check link text and URLs with fuzzy matching ---\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            link_text = a.get_text().lower()\n",
    "            link_href = a['href'].lower()\n",
    "            \n",
    "            # Check for high similarity in link text or the link URL itself\n",
    "            if any(fuzz.partial_ratio(keyword, link_text) > 90 for keyword in CATERING_KEYWORDS):\n",
    "                return CATERING_SKILL_ID\n",
    "            if any(fuzz.partial_ratio(keyword, link_href) > 90 for keyword in CATERING_KEYWORDS):\n",
    "                return CATERING_SKILL_ID\n",
    "                \n",
    "    except Exception as e:\n",
    "        # If any error occurs during scraping (e.g., timeout, invalid URL), just skip it\n",
    "        print(f\"  -  Skipping {url} due to error: {e}\")\n",
    "        return ''\n",
    "        \n",
    "    return '' # Return empty if no keywords were found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f0ae9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 34 records from 'business_directory_cleaned.csv'.\n",
      "\n",
      "üöÄ Starting website analysis for catering keywords...\n",
      "-> Processing: Marbled LA\n",
      "-> Processing: Good Heart Catering\n",
      "-> Processing: Luxe Bites - LA's Best Charcuterie Boards and Event Catering\n",
      "-> Processing: Crateful Catering Los Angeles\n",
      "-> Processing: Bite Catering Couture\n",
      "-> Processing: Chubby Fingers Catering Co\n",
      "-> Processing: Aloha Catering Services Inc\n",
      "-> Processing: Heirloom LA\n",
      "-> Processing: Las Hermanas Catering\n",
      "-> Processing: Spotted Hen Catering\n",
      "-> Processing: Felice Italian Catering\n",
      "-> Processing: Robert's Catering Services\n",
      "-> Processing: TGIS Catering Services\n",
      "-> Processing: Bites and Bashes Catering\n",
      "-> Processing: Basil Pizza Bar Catering\n",
      "-> Processing: The Daily by HC\n",
      "-> Processing: Schaffer\n",
      "-> Processing: Simon's Caterers\n",
      "-> Processing: K Michelle's Kitchen Catering\n",
      "-> Processing: Ask 4 Tacos Catering\n",
      "-> Processing: Paulina's Catering\n",
      "-> Processing: OFF THE SHELF CATERING\n",
      "-> Processing: Haute Chefs Los Angeles\n",
      "-> Processing: Castle's Southern and Creole Catering\n",
      "-> Processing: Lalo's Catering Company\n",
      "-> Processing: Karla's Coffee Cart Catering\n",
      "-> Processing: Catering by Emuna\n",
      "-> Processing: Taste And Company\n",
      "-> Processing: Alfredo Catering Inc\n",
      "-> Processing: Four Seasons Catering\n",
      "-> Processing: Serves You Right! Catering\n",
      "-> Processing: Tr√®s LA Catering\n",
      "-> Processing: LA Spice Catering\n",
      "-> Processing: Team Butler Inc. (DBA) The Butler Pantry Catering Co.\n",
      "\n",
      "üè∑Ô∏è  Tagging complete. Found 'catering' for 31 of 34 businesses.\n",
      "üéâ Success! Results saved to 'final_tagged_catering_businesses.csv'\n",
      "\n",
      "üîß Closing Selenium drivers...\n",
      "‚úÖ Cleanup complete.\n",
      "\n",
      "Sample of tagged data:\n",
      "                                        Company Name  \\\n",
      "0                                         Marbled LA   \n",
      "1  Luxe Bites - LA's Best Charcuterie Boards and ...   \n",
      "2                                Good Heart Catering   \n",
      "3                              Bite Catering Couture   \n",
      "4                         Chubby Fingers Catering Co   \n",
      "5                      Crateful Catering Los Angeles   \n",
      "6                                        Heirloom LA   \n",
      "7                              Las Hermanas Catering   \n",
      "8                               Spotted Hen Catering   \n",
      "9                            Felice Italian Catering   \n",
      "\n",
      "                          Company Website Catering_Skill_ID  \n",
      "0                  http://www.marbled.la/                    \n",
      "1                  https://luxebites.com/                11  \n",
      "2          https://goodheartcatering.com/                11  \n",
      "3           https://www.bitecatering.net/                11  \n",
      "4              https://chubscatering.com/                11  \n",
      "5       https://www.cratefulcatering.com/                11  \n",
      "6                 https://heirloomla.com/                    \n",
      "7    https://losangelestacoscatering.com/                11  \n",
      "8      http://www.spottedhencatering.com/                11  \n",
      "9  https://www.feliceitaliancatering.com/                11  \n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 3. Main Execution Block\n",
    "# ==============================================================================\n",
    "import threading\n",
    "\n",
    "def process_row(row):\n",
    "    \"\"\"Worker function for each row in the DataFrame.\"\"\"\n",
    "    company_name = row['Company Name']\n",
    "    website_url = row['Company Website']\n",
    "    \n",
    "    print(f\"-> Processing: {company_name}\")\n",
    "    \n",
    "    # Run the scrape and tag function\n",
    "    catering_id = scrape_and_tag_website(website_url)\n",
    "    \n",
    "    # Return the original row data plus the new tag\n",
    "    # Using a dictionary ensures data alignment\n",
    "    return {\n",
    "        **row.to_dict(), # Unpack all original columns\n",
    "        'Catering_Skill_ID': catering_id # Add the new column\n",
    "    }\n",
    "\n",
    "# --- Load the pre-scraped data ---\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_FILENAME)\n",
    "    print(f\"‚úÖ Loaded {len(df)} records from '{INPUT_FILENAME}'.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå ERROR: The file '{INPUT_FILENAME}' was not found.\")\n",
    "    df = None\n",
    "\n",
    "if df is not None:\n",
    "    # --- Process each website using a thread pool ---\n",
    "    print(\"\\nüöÄ Starting website analysis for catering keywords...\")\n",
    "    all_results = []\n",
    "    # Convert DataFrame rows to a list of dictionaries for the executor\n",
    "    rows_to_process = [row for index, row in df.iterrows()]\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        # Submit each row to be processed by the worker function\n",
    "        future_to_row = {executor.submit(process_row, row): row['Company Name'] for row in rows_to_process}\n",
    "        \n",
    "        for future in as_completed(future_to_row):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                all_results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error processing business '{future_to_row[future]}': {e}\")\n",
    "    \n",
    "    # --- Create the final DataFrame and save ---\n",
    "    final_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    final_df.to_csv(OUTPUT_FILENAME, index=False)\n",
    "    \n",
    "    catering_found_count = (final_df['Catering_Skill_ID'] != '').sum()\n",
    "    print(f\"\\nüè∑Ô∏è  Tagging complete. Found 'catering' for {catering_found_count} of {len(final_df)} businesses.\")\n",
    "    print(f\"üéâ Success! Results saved to '{OUTPUT_FILENAME}'\")\n",
    "    \n",
    "    # --- Cleanup Selenium Drivers ---\n",
    "    # This part is a bit more complex with threading, but ensures cleanup\n",
    "    print(\"\\nüîß Closing Selenium drivers...\")\n",
    "    # The driver object in the main thread is not used, so we just pass\n",
    "    # Each thread's driver will be closed when the thread terminates.\n",
    "    # For a more robust solution, a driver pool manager would be used, but this is sufficient here.\n",
    "    # We can explicitly close the driver created in the main thread if it exists\n",
    "    if 'driver' in locals():\n",
    "        driver.quit()\n",
    "    print(\"‚úÖ Cleanup complete.\")\n",
    "    \n",
    "    # Display sample results\n",
    "    print(\"\\nSample of tagged data:\")\n",
    "    print(final_df[['Company Name', 'Company Website', 'Catering_Skill_ID']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07371a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (2.2.3)\n",
      "Collecting openai\n",
      "  Downloading openai-1.91.0-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.26-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain_openai\n",
      "  Downloading langchain_openai-0.3.25-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/codespace/.local/lib/python3.12/site-packages (4.13.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from openai) (4.9.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/codespace/.local/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openai) (4.13.2)\n",
      "Collecting langchain-core<1.0.0,>=0.3.66 (from langchain)\n",
      "  Downloading langchain_core-0.3.66-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.4.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading sqlalchemy-2.0.41-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/codespace/.local/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
      "  Downloading tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/codespace/.local/lib/python3.12/site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /usr/local/python/3.12.1/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.66->langchain)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.66->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/codespace/.local/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading zstandard-0.23.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.2.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<1,>=0.7->langchain_openai)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/codespace/.local/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
      "Downloading openai-1.91.0-py3-none-any.whl (735 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m735.8/735.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain-0.3.26-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_openai-0.3.25-py3-none-any.whl (69 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Downloading langchain_core-0.3.66-py3-none-any.whl (438 kB)\n",
      "Downloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Downloading langsmith-0.4.2-py3-none-any.whl (367 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sqlalchemy-2.0.41-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading greenlet-3.2.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (605 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m605.5/605.5 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading orjson-3.10.18-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (133 kB)\n",
      "Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading zstandard-0.23.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: zstandard, typing-inspection, tenacity, regex, pydantic-core, orjson, jsonpatch, jiter, greenlet, distro, annotated-types, tiktoken, SQLAlchemy, requests-toolbelt, pydantic, openai, langsmith, langchain-core, langchain-text-splitters, langchain_openai, langchain\n",
      "Successfully installed SQLAlchemy-2.0.41 annotated-types-0.7.0 distro-1.9.0 greenlet-3.2.3 jiter-0.10.0 jsonpatch-1.33 langchain-0.3.26 langchain-core-0.3.66 langchain-text-splitters-0.3.8 langchain_openai-0.3.25 langsmith-0.4.2 openai-1.91.0 orjson-3.10.18 pydantic-2.11.7 pydantic-core-2.33.2 regex-2024.11.6 requests-toolbelt-1.0.0 tenacity-9.1.2 tiktoken-0.9.0 typing-inspection-0.4.1 zstandard-0.23.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas openai langchain langchain_openai beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a77ef37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e83602b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b489d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c774851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Cell 1: Setup and Configuration (V3 - No Sampling)\n",
    "# ==============================================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76508bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading business data from 'business_directory_cleaned.csv'...\n",
      "‚úÖ Loaded and prepared to analyze all 34 websites.\n",
      "\n",
      "First 5 URLs to be analyzed:\n",
      "- http://www.marbled.la/\n",
      "- https://goodheartcatering.com/\n",
      "- https://luxebites.com/\n",
      "- https://www.cratefulcatering.com/\n",
      "- https://www.bitecatering.net/\n"
     ]
    }
   ],
   "source": [
    "# --- API Key Configuration ---\n",
    "# I'm removing it here. Please ensure it's set correctly when you run.\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" # Replace this again\n",
    "\n",
    "# --- File Configuration ---\n",
    "INPUT_FILENAME = 'business_directory_cleaned.csv'\n",
    "OUTPUT_KEYWORDS_FILENAME = 'catering_keyword_dictionary.json'\n",
    "\n",
    "# --- Load Your Data (No Sampling) ---\n",
    "print(f\"üöÄ Loading business data from '{INPUT_FILENAME}'...\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_FILENAME)\n",
    "    if 'Company Website' not in df.columns:\n",
    "        raise ValueError(\"The required column 'Company Website' was not found in the CSV.\")\n",
    "    \n",
    "    # Drop rows where the website URL is missing\n",
    "    df.dropna(subset=['Company Website'], inplace=True)\n",
    "    \n",
    "    # Directly create the list of all URLs from the DataFrame\n",
    "    website_urls = df['Company Website'].tolist()\n",
    "    \n",
    "    print(f\"‚úÖ Loaded and prepared to analyze all {len(website_urls)} websites.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå ERROR: The file '{INPUT_FILENAME}' was not found.\")\n",
    "    website_urls = []\n",
    "except ValueError as e:\n",
    "    print(f\"‚ùå ERROR: {e}\")\n",
    "    website_urls = []\n",
    "\n",
    "# Display a sample of the URLs to be analyzed\n",
    "if website_urls:\n",
    "    print(\"\\nFirst 5 URLs to be analyzed:\")\n",
    "    for url in website_urls[:5]:\n",
    "        print(f\"- {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8240e009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading business data from 'business_directory_cleaned.csv'...\n",
      "‚úÖ Loaded 34 total records.\n",
      "üî¨ Selected a sample of 15 websites for AI analysis.\n",
      "\n",
      "First 5 URLs to be analyzed:\n",
      "- http://www.hcmenu.com/\n",
      "- https://www.ask4tacos.com/\n",
      "- https://www.tasteandcompany.com/\n",
      "- http://www.emunainc.com/\n",
      "- https://losangelestacoscatering.com/\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 2. LOAD YOUR DATA\n",
    "# ==============================================================================\n",
    "\n",
    "print(f\"üöÄ Loading business data from '{INPUT_FILENAME}'...\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_FILENAME)\n",
    "    # Ensure the 'Company Website' column exists and drop rows where it's missing\n",
    "    if 'Company Website' not in df.columns:\n",
    "        raise ValueError(\"The required column 'Company Website' was not found in the CSV.\")\n",
    "    \n",
    "    df.dropna(subset=['Company Website'], inplace=True)\n",
    "    \n",
    "    # Take a random sample of the websites for analysis\n",
    "    if len(df) > SAMPLE_SIZE:\n",
    "        df_sample = df.sample(n=SAMPLE_SIZE, random_state=42) # random_state for reproducibility\n",
    "    else:\n",
    "        df_sample = df\n",
    "        \n",
    "    website_urls = df_sample['Company Website'].tolist()\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(df)} total records.\")\n",
    "    print(f\"üî¨ Selected a sample of {len(website_urls)} websites for AI analysis.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå ERROR: The file '{INPUT_FILENAME}' was not found. Please make sure it's in the same directory.\")\n",
    "    website_urls = []\n",
    "except ValueError as e:\n",
    "    print(f\"‚ùå ERROR: {e}\")\n",
    "    website_urls = []\n",
    "\n",
    "# Display the first 5 URLs we will analyze\n",
    "if website_urls:\n",
    "    print(\"\\nFirst 5 URLs to be analyzed:\")\n",
    "    for url in website_urls[:5]:\n",
    "        print(f\"- {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cde0ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¨ Performing a single test run on the first URL...\n",
      "  -> Analyzing: http://www.hcmenu.com/\n",
      "  -  Found keywords: ['full-service catering', 'special event venues', 'wedding planning', 'wedding catering', 'corporate catering', 'private catering', 'entertainment catering']\n",
      "\n",
      "‚úÖ Test complete. Keywords found for http://www.hcmenu.com/: ['full-service catering', 'special event venues', 'wedding planning', 'wedding catering', 'corporate catering', 'private catering', 'entertainment catering']\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. DEFINE THE AI KEYWORD EXTRACTOR (Robust Version)\n",
    "# ==============================================================================\n",
    "\n",
    "# Initialize the Language Model we'll use.\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# The prompt template is the key to our success. It's already good.\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert market research analyst for the food and beverage industry. \n",
    "The following is the text content scraped from a single company's website.\n",
    "\n",
    "Your task is to analyze this text and identify specific keywords and short phrases (2-3 words) that strongly indicate the company offers CATERING services.\n",
    "\n",
    "Please provide your output as a single, clean JSON object with one key, \"keywords\". \n",
    "If you find no relevant keywords, return an empty list.\n",
    "\n",
    "- Focus on service-related terms (e.g., \"corporate catering\", \"wedding events\").\n",
    "- Exclude generic business terms like 'contact us', 'about us', 'our menu', 'gallery', 'home', and copyright notices.\n",
    "- Base your answer *only* on the text provided.\n",
    "\n",
    "Here is the website content:\n",
    "---\n",
    "{context}\n",
    "---\n",
    "\"\"\")\n",
    "\n",
    "# The chain remains simple: prompt -> llm -> output parser\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "def analyze_website_for_keywords(url: str) -> list:\n",
    "    \"\"\"\n",
    "    Uses LangChain to scrape a website, then sends the content to an LLM\n",
    "    to extract keywords. Returns a list of keywords.\n",
    "    \"\"\"\n",
    "    print(f\"  -> Analyzing: {url}\")\n",
    "    try:\n",
    "        # Step 1: Use the loader in its simplest, most robust form.\n",
    "        # It will now scrape the entire body of the page.\n",
    "        loader = WebBaseLoader(\n",
    "            web_paths=(url,),\n",
    "            requests_kwargs={\"headers\": {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}}\n",
    "        )\n",
    "        docs = loader.load()\n",
    "        \n",
    "        if not docs:\n",
    "            print(f\"  -  WARNING: Could not load any content from {url}\")\n",
    "            return []\n",
    "        \n",
    "        # We might get multiple documents if the page is structured oddly.\n",
    "        # We'll join their content together to be safe.\n",
    "        website_content = \"\\n\".join([doc.page_content for doc in docs])\n",
    "        \n",
    "        # Check if we actually got any text.\n",
    "        if not website_content.strip():\n",
    "            print(f\"  -  WARNING: Loaded page from {url} but found no text content.\")\n",
    "            return []\n",
    "\n",
    "        # Step 2: Invoke the chain with the scraped content.\n",
    "        response_str = chain.invoke({\"context\": website_content})\n",
    "        \n",
    "        # Step 3: Parse the JSON response from the LLM.\n",
    "        # This is wrapped in a try-except block in case the LLM doesn't return valid JSON.\n",
    "        try:\n",
    "            # The LLM sometimes wraps the JSON in markdown backticks, so we clean it.\n",
    "            if response_str.startswith(\"```json\"):\n",
    "                response_str = response_str.strip(\"```json\").strip()\n",
    "            \n",
    "            response_json = json.loads(response_str)\n",
    "            keywords = response_json.get(\"keywords\", [])\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"  -  WARNING: LLM did not return valid JSON. Response was: {response_str}\")\n",
    "            return [] # Return empty list if JSON is bad\n",
    "        \n",
    "        if isinstance(keywords, list):\n",
    "            print(f\"  -  Found keywords: {keywords}\")\n",
    "            return keywords\n",
    "        else:\n",
    "            return []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  -  ERROR analyzing {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Let's test it on a single URL from our list to make sure it works ---\n",
    "if 'website_urls' in locals() and website_urls:\n",
    "    print(\"\\nüî¨ Performing a single test run on the first URL...\")\n",
    "    test_url = website_urls[0]\n",
    "    test_keywords = analyze_website_for_keywords(test_url)\n",
    "    print(f\"\\n‚úÖ Test complete. Keywords found for {test_url}: {test_keywords}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No URLs to test. Please run Cell 1 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4ab6063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting AI analysis for all 15 websites. This may take a few minutes...\n",
      "  -> Analyzing: http://www.hcmenu.com/\n",
      "  -> Analyzing: https://www.ask4tacos.com/\n",
      "  -> Analyzing: https://www.tasteandcompany.com/\n",
      "  -> Analyzing: http://www.emunainc.com/\n",
      "  -> Analyzing: https://losangelestacoscatering.com/\n",
      "  -  Found keywords: ['Private Dinner Catering', 'Business Lunch Catering', 'Catering Your Next Event', 'private and corporate events', 'Corporate Events']\n",
      "  -> Analyzing: http://www.laloscatering.com/\n",
      "  -  Found keywords: []\n",
      "  -> Analyzing: http://www.offtheshelfcatering.com/\n",
      "  -  Found keywords: ['corporate catering', 'wedding events', 'special occasion', 'event consulting', 'gourmet to go', 'event planning', 'order catering', 'plan your event']\n",
      "  -> Analyzing: http://www.tgiscatering.com/\n",
      "  -  Found keywords: ['full-service catering', 'special event venues', 'wedding planning', 'wedding catering', 'corporate catering', 'private catering', 'entertainment catering']\n",
      "  -> Analyzing: https://www.laspicecatering.com/\n",
      "  -  Found keywords: ['professional catering company', 'premium catering services', 'corporate function', 'food catering services', 'custom event packages', 'events catered']\n",
      "  -> Analyzing: http://www.spottedhencatering.com/\n",
      "  -  Found keywords: ['set catering', 'production catering', 'full catering option', 'film and TV catering', 'commercial catering']\n",
      "  -> Analyzing: http://www.marbled.la/\n",
      "  -  WARNING: Loaded page from http://www.marbled.la/ but found no text content.\n",
      "  -> Analyzing: https://www.bitecatering.net/\n",
      "  -  Found keywords: ['corporate catering', 'wedding events', 'drop-off catering', 'event planning', 'bar service']\n",
      "  -> Analyzing: http://www.schafferla.com/\n",
      "  -  Found keywords: ['international catering', 'party trays to go', 'breakfast packages', 'lunch & dinner packages', 'banquets', 'catering to LA']\n",
      "  -> Analyzing: https://simoncaterers.com/\n",
      "  -  Found keywords: ['wedding menus', 'drop off party catering', 'production catering', 'corporate catering', 'drop-off parties/events']\n",
      "  -> Analyzing: https://chubscatering.com/\n",
      "  -  Found keywords: ['catering service', 'event planning', 'corporate events', 'wedding events', 'luxury events', 'executive-level events']\n",
      "  -  Found keywords: ['corporate catering', 'wedding catering', 'social catering', 'catered events', 'event producers', 'catering services', 'catering experience', 'catering solutions', 'catering partner', 'catering market', 'catering productions', 'catering solutions']\n",
      "  -  Found keywords: ['Catering and Special Events', 'event catering', 'special events']\n",
      "  -  Found keywords: ['corporate catering', 'wedding events', 'event services', 'hospitality events', 'catering services', 'event & staffing flow', 'creative design', 'expert bartending', 'beachside lunch', 'rehearsal dinner']\n",
      "  -  Found keywords: ['catering company', 'intimate gatherings', 'large, high-end events', 'customized menus', 'corporate events', 'holiday parties', 'weddings', 'rehearsal dinners', 'baby showers', 'nonprofit events', 'church events', 'full-service']\n",
      "\n",
      "‚úÖ AI analysis complete for all websites.\n",
      "\n",
      "üéâ Success! Keyword dictionary created and saved to 'catering_keyword_dictionary.json'.\n",
      "\n",
      "--- Final Catering Keywords ---\n",
      "- catering services\n",
      "- catering solutions\n",
      "- corporate catering\n",
      "- corporate events\n",
      "- event planning\n",
      "- production catering\n",
      "- wedding catering\n",
      "- wedding events\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 4. RUN ANALYSIS ON ALL SITES AND AGGREGATE KEYWORDS\n",
    "# ==============================================================================\n",
    "\n",
    "all_extracted_keywords = []\n",
    "\n",
    "if 'website_urls' in locals() and website_urls:\n",
    "    print(f\"\\nüöÄ Starting AI analysis for all {len(website_urls)} websites. This may take a few minutes...\")\n",
    "    \n",
    "    # We use a ThreadPoolExecutor to run the analysis in parallel, which is much faster.\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        # Submit each URL to the executor to be analyzed by our function\n",
    "        future_to_url = {executor.submit(analyze_website_for_keywords, url): url for url in website_urls}\n",
    "        \n",
    "        for future in as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                # Get the result (the list of keywords) from the completed future\n",
    "                keywords = future.result()\n",
    "                if keywords:\n",
    "                    all_extracted_keywords.extend(keywords)\n",
    "            except Exception as exc:\n",
    "                print(f\"‚ùå An error occurred for URL {url}: {exc}\")\n",
    "\n",
    "    print(\"\\n‚úÖ AI analysis complete for all websites.\")\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # 5. PROCESS AND SAVE THE FINAL KEYWORD DICTIONARY\n",
    "    # ==============================================================================\n",
    "    \n",
    "    if all_extracted_keywords:\n",
    "        # Use collections.Counter to count the occurrences of each keyword\n",
    "        keyword_counts = Counter(all_extracted_keywords)\n",
    "        \n",
    "        # We can now decide on a threshold. For example, only keep keywords that appeared on at least 2 websites.\n",
    "        # This helps filter out very specific or rare terms.\n",
    "        MINIMUM_OCCURRENCES = 2\n",
    "        \n",
    "        # Create the final list of high-quality keywords\n",
    "        final_keywords = [\n",
    "            keyword for keyword, count in keyword_counts.items() \n",
    "            if count >= MINIMUM_OCCURRENCES\n",
    "        ]\n",
    "        \n",
    "        # Sort the keywords for consistency\n",
    "        final_keywords.sort()\n",
    "        \n",
    "        # This is the final dictionary structure we'll use in the next phase\n",
    "        keyword_dictionary = {\n",
    "            \"Catering\": {\n",
    "                \"skill_id\": 11,\n",
    "                \"keywords\": final_keywords\n",
    "            }\n",
    "            # In the future, you can add more categories here, like \"Pizza\", \"Bar\", etc.\n",
    "        }\n",
    "        \n",
    "        # Save the dictionary to a JSON file for later use\n",
    "        with open(OUTPUT_KEYWORDS_FILENAME, 'w') as f:\n",
    "            json.dump(keyword_dictionary, f, indent=4)\n",
    "            \n",
    "        print(f\"\\nüéâ Success! Keyword dictionary created and saved to '{OUTPUT_KEYWORDS_FILENAME}'.\")\n",
    "        print(\"\\n--- Final Catering Keywords ---\")\n",
    "        for keyword in final_keywords:\n",
    "            print(f\"- {keyword}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No keywords were extracted from any of the websites.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No URLs were loaded in Cell 1. Cannot proceed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06857eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d94056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bff9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0006dd0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Cell 1: Setup and Loading Data\n",
    "# ==============================================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import threading\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95521a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading all business data from 'business_directory_cleaned.csv'...\n",
      "‚úÖ Loaded and prepared to analyze all 34 websites.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "INPUT_FILENAME = 'business_directory_cleaned.csv'\n",
    "OUTPUT_KEYWORDS_FILENAME = 'catering_keyword_dictionary_full.json'\n",
    "\n",
    "# --- Load Data (No Sampling) ---\n",
    "print(f\"üöÄ Loading all business data from '{INPUT_FILENAME}'...\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_FILENAME)\n",
    "    if 'Company Website' not in df.columns:\n",
    "        raise ValueError(\"The required column 'Company Website' was not found in the CSV.\")\n",
    "    \n",
    "    # Drop rows where the website URL is missing or invalid\n",
    "    df.dropna(subset=['Company Website'], inplace=True)\n",
    "    df = df[df['Company Website'].str.startswith('http', na=False)]\n",
    "    \n",
    "    # Create the final list of all URLs to be processed\n",
    "    website_urls = df['Company Website'].tolist()\n",
    "    \n",
    "    print(f\"‚úÖ Loaded and prepared to analyze all {len(website_urls)} websites.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå ERROR: The file '{INPUT_FILENAME}' was not found.\")\n",
    "    website_urls = []\n",
    "except ValueError as e:\n",
    "    print(f\"‚ùå ERROR: {e}\")\n",
    "    website_urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26062697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Cell 2: AI Keyword Extraction Logic\n",
    "# ==============================================================================\n",
    "\n",
    "# Initialize the Language Model\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Define the prompt template for the AI\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert market research analyst for the food and beverage industry. \n",
    "The following is the text content scraped from a single company's website.\n",
    "\n",
    "Your task is to analyze this text and identify specific keywords and short phrases (2-3 words) that strongly indicate the company offers CATERING services.\n",
    "\n",
    "Please provide your output as a single, clean JSON object with one key, \"keywords\". \n",
    "If you find no relevant keywords, return an empty list.\n",
    "\n",
    "- Focus on service-related terms (e.g., \"corporate catering\", \"wedding events\").\n",
    "- Exclude generic business terms like 'contact us', 'about us', 'our menu', 'gallery', 'home', and copyright notices.\n",
    "- Base your answer *only* on the text provided.\n",
    "\n",
    "Here is the website content:\n",
    "---\n",
    "{context}\n",
    "---\n",
    "\"\"\")\n",
    "\n",
    "# Define the analysis chain\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "def analyze_website_for_keywords(url: str) -> list:\n",
    "    \"\"\"\n",
    "    Uses LangChain to scrape a website and sends the content to an LLM\n",
    "    to extract keywords. Returns a list of keywords.\n",
    "    \"\"\"\n",
    "    print(f\"  -> Analyzing: {url}\")\n",
    "    try:\n",
    "        # Use the loader in its simplest, most robust form.\n",
    "        loader = WebBaseLoader(\n",
    "            web_paths=(url,),\n",
    "            requests_kwargs={\"headers\": {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}}\n",
    "        )\n",
    "        docs = loader.load()\n",
    "        \n",
    "        if not docs:\n",
    "            print(f\"  -  WARNING: Could not load any content from {url}\")\n",
    "            return []\n",
    "        \n",
    "        website_content = \"\\n\".join([doc.page_content for doc in docs])\n",
    "        \n",
    "        if not website_content.strip():\n",
    "            print(f\"  -  WARNING: Loaded page from {url} but found no text content.\")\n",
    "            return []\n",
    "\n",
    "        # Invoke the chain with the scraped content.\n",
    "        response_str = chain.invoke({\"context\": website_content})\n",
    "        \n",
    "        # Parse the JSON response from the LLM.\n",
    "        try:\n",
    "            if response_str.startswith(\"```json\"):\n",
    "                response_str = response_str.strip(\"```json\").strip()\n",
    "            \n",
    "            response_json = json.loads(response_str)\n",
    "            keywords = response_json.get(\"keywords\", [])\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"  -  WARNING: LLM did not return valid JSON. Response: {response_str}\")\n",
    "            return []\n",
    "        \n",
    "        if isinstance(keywords, list):\n",
    "            print(f\"  -  Found keywords: {keywords}\")\n",
    "            return keywords\n",
    "        else:\n",
    "            return []\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  -  ERROR analyzing {url}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "927ce161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting AI analysis for all 34 websites. This may take some time...\n",
      "  -> Analyzing: http://www.marbled.la/\n",
      "  -> Analyzing: https://goodheartcatering.com/\n",
      "  -> Analyzing: https://luxebites.com/\n",
      "  -> Analyzing: https://www.cratefulcatering.com/\n",
      "  -> Analyzing: https://www.bitecatering.net/\n",
      "  -  WARNING: Loaded page from http://www.marbled.la/ but found no text content.\n",
      "  -> Analyzing: https://chubscatering.com/\n",
      "  -  Found keywords: ['catering company', 'intimate gatherings', 'large, high-end events', 'customized menus', 'corporate events', 'holiday parties', 'weddings', 'rehearsal dinners', 'baby showers', 'nonprofit events', 'church events', 'full-service']\n",
      "  -> Analyzing: http://alohacateringservicesinc.com/\n",
      "  -  Found keywords: ['corporate catering', 'wedding catering', 'social catering', 'catered events', 'event producers', 'catering services', 'catering experience', 'catering solutions', 'catering partner', 'catering market', 'catering productions', 'catering solutions']\n",
      "  -> Analyzing: https://heirloomla.com/\n",
      "  -  Found keywords: ['Event Catering', 'Corporate & Production Catering', 'Bar Catering', 'Smoothie Catering', 'wedding catering', 'small party catering', 'event catering services', 'wedding and event caterer', 'corporate events']\n",
      "  -> Analyzing: https://losangelestacoscatering.com/\n",
      "  -  Found keywords: ['corporate events', 'wedding menus', 'crew meals', 'beverage service', 'wedding catering', 'corporate catering', 'production catering', 'bar mitzvah catering', 'kosher catering', 'beverage catering']\n",
      "  -> Analyzing: http://www.spottedhencatering.com/\n",
      "  -  WARNING: Loaded page from http://alohacateringservicesinc.com/ but found no text content.\n",
      "  -> Analyzing: https://www.feliceitaliancatering.com/\n",
      "  -  Found keywords: ['corporate events', 'office meetings', 'drop-off catering', 'corporate breakfast meeting', 'office lunch', 'large company event', 'customizable menus', 'buffet-style catering', 'individually packed meals', 'business and office catering', 'non profit catering service', 'university catering service', 'medical sales catering service', 'destination management catering service']\n",
      "  -> Analyzing: https://www.robertscateringservice.com/\n",
      "  -  Found keywords: ['brunch buffet', 'fundraiser', 'cooking class', 'vegan garden dinner', 'book signing parties', 'beach party buffet', 'ladies luncheon', 'family style service', 'wedding in Ojai', 'dinner party', 'family lunch buffet']\n",
      "  -> Analyzing: http://www.tgiscatering.com/\n",
      "  -  Found keywords: []\n",
      "  -> Analyzing: http://bitesandbashes.com/\n",
      "  -  Found keywords: ['catering services', 'corporate function', 'food catering services', 'custom event packages', 'events catered', 'customizable packages']\n",
      "  -> Analyzing: https://basilpizzabar.com/\n",
      "  -  Found keywords: ['wedding menus', 'drop off party catering', 'production catering', 'corporate catering', 'drop-off parties/events']\n",
      "  -> Analyzing: http://www.hcmenu.com/\n",
      "  -  Found keywords: ['catering service', 'event planning', 'corporate events', 'wedding events', 'luxury events', 'executive-level events']\n",
      "  -> Analyzing: http://www.schafferla.com/\n",
      "  -  Found keywords: ['catering and events', 'full-service offerings', 'corporate lunch setups', 'interactive stations', 'plated dinners', 'event coordinators', 'bar services', 'corporate events', 'private events', 'non-profit galas', 'dinner parties', 'weddings', 'holiday parties']\n",
      "  -> Analyzing: https://simoncaterers.com/\n",
      "  -  Found keywords: ['full-service catering', 'special event venues', 'wedding planning', 'wedding catering', 'corporate catering', 'private catering', 'entertainment catering']\n",
      "  -> Analyzing: http://www.kmichellekitchencatering.com/\n",
      "  -  Found keywords: ['pizza catering', 'corporate events', 'private events', 'wedding events', 'wood-fired pizza catering', 'catering setups', 'buffet style', 'family style', 'individual guest ordering', 'cater our wedding', 'catering menu', 'catering your special event']\n",
      "  -> Analyzing: https://www.ask4tacos.com/\n",
      "  -  Found keywords: ['Mobile Pizza Catering', 'Artisan Pizza Catering', 'Los Angeles Pizza Catering', 'Indoor pizza catering', 'Wedding Pizza Catering', 'custom catering menus', 'Live Action Station', 'Live pizza station catering', 'Home Event Catering']\n",
      "  -> Analyzing: http://www.paulinasfoodcatering.com/\n",
      "  -  Found keywords: ['Catering and Special Events', 'event catering', 'special events']\n",
      "  -> Analyzing: http://www.offtheshelfcatering.com/\n",
      "  -  WARNING: Loaded page from http://www.paulinasfoodcatering.com/ but found no text content.\n",
      "  -> Analyzing: https://hautechefsla.com/\n",
      "  -  Found keywords: ['corporate catering', 'wedding events', 'event services', 'hospitality events', 'catering services', 'event & staffing flow', 'creative design', 'expert bartending', 'beachside lunch', 'rehearsal dinner']\n",
      "  -> Analyzing: http://www.castlescatering.com/\n",
      "  -  Found keywords: ['Private Dinner Catering', 'Business Lunch Catering', 'Catering Your Next Event', 'private and corporate events', 'Corporate Events']\n",
      "  -> Analyzing: http://www.laloscatering.com/\n",
      "  -  Found keywords: ['set catering', 'production catering', 'full catering option', 'film catering', 'television catering', 'commercial catering']\n",
      "  -> Analyzing: http://www.karlascoffeecart.com/\n",
      "  -  Found keywords: ['Corporate Catering', 'Event Catering', 'Wedding Catering', 'catering services', 'cater your next event', 'catering for any occasion', 'catered special events', 'corporate office parties', 'catering for all']\n",
      "  -> Analyzing: http://www.emunainc.com/\n",
      "  -  Found keywords: ['event dining', 'production catering', 'bespoke culinary services', 'stylish production catering', 'unparalleled event expertise']\n",
      "  -> Analyzing: https://www.tasteandcompany.com/\n",
      "  -  Found keywords: []\n",
      "  -> Analyzing: http://alfredocatering.net/\n",
      "  -  Found keywords: []\n",
      "  -> Analyzing: http://catering4seasons.com/\n",
      "  -  Found keywords: ['international catering', 'party trays to go', 'breakfast packages', 'lunch & dinner packages', 'catering to LA']\n",
      "  -> Analyzing: http://www.syrcatering.com/\n",
      "  -  Found keywords: ['full service catering', 'large scale full service catering', 'festivals catering', 'production catering', 'private events', 'corporate events', 'weddings']\n",
      "  -> Analyzing: http://www.tresla.com/\n",
      "  -  Found keywords: ['corporate catering', 'wedding events', 'special occasion', 'event consulting', 'gourmet to go', 'event planning', 'order catering', 'plan your event']\n",
      "  -> Analyzing: https://www.laspicecatering.com/\n",
      "  -  Found keywords: ['fusion catering', 'private events', 'corporate events', 'weddings', 'galas']\n",
      "  -> Analyzing: https://www.thebutlerpantrycateringco.com/\n",
      "  -  Found keywords: ['corporate catering', 'wedding events', 'drop-off catering', 'event planning', 'bar service']\n",
      "  -  Found keywords: ['event venues', 'Tres LA Catering', 'wedding day', 'catering company']\n",
      "  -  Found keywords: ['full service catering', 'event planning', 'corporate events', 'intimate dinner party', 'larger gathering', 'company holiday party', 'catering services', 'business function']\n",
      "  -  Found keywords: ['corporate banquets', 'wedding events', 'birthday parties', 'anniversarys', 'school breakfast/lunch programs', 'senior meals', 'PR firms', 'event catering', 'custom catering', 'catering booking form', 'film & catering menus']\n",
      "  -  Found keywords: ['food catering for events', 'preferred catering company', 'catering services', 'catering menus', 'food catering services', 'catering business', 'kosher food', 'international-based menus', 'catering for all kinds of events', 'small casual gathering', 'huge wedding', 'banqueter√≠a para eventos', 'servicios de banqueter√≠a']\n",
      "\n",
      "‚úÖ AI analysis complete for all websites.\n",
      "\n",
      "üéâ Success! Keyword dictionary created and saved to 'catering_keyword_dictionary_full.json'.\n",
      "\n",
      "--- Final Catering Keywords ---\n",
      "- Event Catering\n",
      "- catering company\n",
      "- catering services\n",
      "- catering solutions\n",
      "- corporate catering\n",
      "- corporate events\n",
      "- drop-off catering\n",
      "- event catering\n",
      "- event planning\n",
      "- food catering services\n",
      "- full service catering\n",
      "- holiday parties\n",
      "- private events\n",
      "- production catering\n",
      "- wedding catering\n",
      "- wedding events\n",
      "- wedding menus\n",
      "- weddings\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Cell 3: Run Analysis at Scale and Create Final Dictionary\n",
    "# ==============================================================================\n",
    "\n",
    "all_extracted_keywords = []\n",
    "\n",
    "if 'website_urls' in locals() and website_urls and os.environ.get(\"OPENAI_API_KEY\") != \"YOUR_OPENAI_API_KEY_HERE\":\n",
    "    print(f\"\\nüöÄ Starting AI analysis for all {len(website_urls)} websites. This may take some time...\")\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        future_to_url = {executor.submit(analyze_website_for_keywords, url): url for url in website_urls}\n",
    "        \n",
    "        for future in as_completed(future_to_url):\n",
    "            try:\n",
    "                keywords = future.result()\n",
    "                if keywords:\n",
    "                    all_extracted_keywords.extend(keywords)\n",
    "            except Exception as exc:\n",
    "                print(f\"‚ùå An error occurred for URL {future_to_url[future]}: {exc}\")\n",
    "\n",
    "    print(\"\\n‚úÖ AI analysis complete for all websites.\")\n",
    "    \n",
    "    # --- Process and save the final keyword dictionary ---\n",
    "    if all_extracted_keywords:\n",
    "        # Count the occurrences of each keyword\n",
    "        keyword_counts = Counter(all_extracted_keywords)\n",
    "        \n",
    "        # Only keep keywords that appeared on at least 2 different websites\n",
    "        MINIMUM_OCCURRENCES = 2\n",
    "        \n",
    "        final_keywords = sorted([\n",
    "            keyword for keyword, count in keyword_counts.items() \n",
    "            if count >= MINIMUM_OCCURRENCES\n",
    "        ])\n",
    "        \n",
    "        # Create the final dictionary structure\n",
    "        keyword_dictionary = {\n",
    "            \"Catering\": {\n",
    "                \"skill_id\": 11,\n",
    "                \"keywords\": final_keywords\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save the dictionary to a JSON file\n",
    "        with open(OUTPUT_KEYWORDS_FILENAME, 'w') as f:\n",
    "            json.dump(keyword_dictionary, f, indent=4)\n",
    "            \n",
    "        print(f\"\\nüéâ Success! Keyword dictionary created and saved to '{OUTPUT_KEYWORDS_FILENAME}'.\")\n",
    "        print(\"\\n--- Final Catering Keywords ---\")\n",
    "        for keyword in final_keywords:\n",
    "            print(f\"- {keyword}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No keywords were extracted from any of the websites.\")\n",
    "\n",
    "elif not os.environ.get(\"OPENAI_API_KEY\") or os.environ.get(\"OPENAI_API_KEY\") == \"YOUR_OPENAI_API_KEY_HERE\":\n",
    "    print(\"\\n‚ùå ERROR: Please set your OpenAI API key in Cell 1 before running.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No URLs were loaded. Cannot proceed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9677c999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aea42f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f23cdcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading keyword dictionary from 'catering_keyword_dictionary.json'...\n",
      "‚úÖ Successfully loaded 8 keywords for Skill ID 11.\n",
      "\n",
      "üöÄ Loading business data from 'business_directory_cleaned.csv'...\n",
      "‚úÖ Loaded 34 businesses with valid websites to be processed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "# ==============================================================================\n",
    "# Cell 1: Configuration and Loading Inputs\n",
    "# ==============================================================================\n",
    "\n",
    "# --- File Configuration ---\n",
    "# The original CSV with all your businesses\n",
    "INPUT_BUSINESS_FILENAME = 'business_directory_cleaned.csv' \n",
    "\n",
    "# The keyword dictionary file you just created with the AI\n",
    "INPUT_KEYWORDS_FILENAME = 'catering_keyword_dictionary.json'\n",
    "\n",
    "# The final, enriched output file\n",
    "OUTPUT_FILENAME = 'final_tagged_businesses.csv'\n",
    "\n",
    "# --- Load Keyword Dictionary ---\n",
    "print(f\"üöÄ Loading keyword dictionary from '{INPUT_KEYWORDS_FILENAME}'...\")\n",
    "try:\n",
    "    with open(INPUT_KEYWORDS_FILENAME, 'r') as f:\n",
    "        keyword_dict = json.load(f)\n",
    "    \n",
    "    # Extract the keywords and skill ID for the 'Catering' category\n",
    "    catering_info = keyword_dict.get('Catering', {})\n",
    "    CATERING_KEYWORDS = catering_info.get('keywords', [])\n",
    "    CATERING_SKILL_ID = catering_info.get('skill_id')\n",
    "\n",
    "    if not CATERING_KEYWORDS or not CATERING_SKILL_ID:\n",
    "        raise ValueError(\"Keywords or skill_id for 'Catering' not found in JSON file.\")\n",
    "        \n",
    "    print(f\"‚úÖ Successfully loaded {len(CATERING_KEYWORDS)} keywords for Skill ID {CATERING_SKILL_ID}.\")\n",
    "\n",
    "except (FileNotFoundError, ValueError) as e:\n",
    "    print(f\"‚ùå ERROR: Could not load or parse the keyword dictionary. {e}\")\n",
    "    CATERING_KEYWORDS = [] # Ensure the script doesn't fail later\n",
    "\n",
    "# --- Load Business Data ---\n",
    "print(f\"\\nüöÄ Loading business data from '{INPUT_BUSINESS_FILENAME}'...\")\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_BUSINESS_FILENAME)\n",
    "    df.dropna(subset=['Company Website'], inplace=True)\n",
    "    df = df[df['Company Website'].str.startswith('http', na=False)]\n",
    "    print(f\"‚úÖ Loaded {len(df)} businesses with valid websites to be processed.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå ERROR: The file '{INPUT_BUSINESS_FILENAME}' was not found.\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65eeb84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Cell 2: Scraper and Tagger Function\n",
    "# ==============================================================================\n",
    "\n",
    "def scrape_and_find_matches(url: str, keywords_to_find: list):\n",
    "    \"\"\"\n",
    "    Scrapes a single URL and checks its text against a list of keywords.\n",
    "    Returns the list of keywords that were found.\n",
    "    \"\"\"\n",
    "    if not url:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # We can use a simple requests-based scraper here, as we don't need to handle complex JS\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code != 200:\n",
    "            return []\n",
    "            \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        page_text = soup.get_text(\" \", strip=True).lower()\n",
    "        \n",
    "        # Find all keywords from our list that are present in the page text\n",
    "        matched_keywords = [\n",
    "            keyword for keyword in keywords_to_find \n",
    "            if keyword.lower() in page_text\n",
    "        ]\n",
    "        \n",
    "        return matched_keywords\n",
    "        \n",
    "    except requests.RequestException:\n",
    "        # If the website is down or fails to load, return an empty list\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43d40226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting tagging process for all 34 businesses...\n",
      "  -> Processing: Marbled LA\n",
      "  -> Processing: Good Heart Catering\n",
      "  -> Processing: Luxe Bites - LA's Best Charcuterie Boards and Event Catering\n",
      "  -> Processing: Crateful Catering Los Angeles\n",
      "  -> Processing: Bite Catering Couture\n",
      "  -> Processing: Chubby Fingers Catering Co\n",
      "  -> Processing: Aloha Catering Services Inc\n",
      "  -> Processing: Heirloom LA\n",
      "  -> Processing: Las Hermanas Catering\n",
      "  -> Processing: Spotted Hen Catering\n",
      "  -> Processing: Felice Italian Catering\n",
      "  -> Processing: Robert's Catering Services\n",
      "  -> Processing: TGIS Catering Services\n",
      "  -> Processing: Bites and Bashes Catering\n",
      "  -> Processing: Basil Pizza Bar Catering\n",
      "  -> Processing: The Daily by HC\n",
      "  -> Processing: Schaffer\n",
      "  -> Processing: Simon's Caterers\n",
      "  -> Processing: K Michelle's Kitchen Catering\n",
      "  -> Processing: Ask 4 Tacos Catering\n",
      "  -> Processing: Paulina's Catering\n",
      "  -> Processing: OFF THE SHELF CATERING\n",
      "  -> Processing: Haute Chefs Los Angeles\n",
      "  -> Processing: Castle's Southern and Creole Catering\n",
      "  -> Processing: Lalo's Catering Company\n",
      "  -> Processing: Karla's Coffee Cart Catering\n",
      "  -> Processing: Catering by Emuna\n",
      "  -> Processing: Taste And Company\n",
      "  -> Processing: Alfredo Catering Inc\n",
      "  -> Processing: Four Seasons Catering\n",
      "  -> Processing: Serves You Right! Catering\n",
      "  -> Processing: Tr√®s LA Catering\n",
      "  -> Processing: LA Spice Catering\n",
      "  -> Processing: Team Butler Inc. (DBA) The Butler Pantry Catering Co.\n",
      "\n",
      "üéâ Success! Enriched data saved to 'final_tagged_businesses.csv'.\n",
      "\n",
      "--- Sample of Final Output ---\n",
      "                     Company Name  \\\n",
      "0                      Marbled LA   \n",
      "4           Bite Catering Couture   \n",
      "5      Chubby Fingers Catering Co   \n",
      "1             Good Heart Catering   \n",
      "11     Robert's Catering Services   \n",
      "10        Felice Italian Catering   \n",
      "9            Spotted Hen Catering   \n",
      "3   Crateful Catering Los Angeles   \n",
      "14       Basil Pizza Bar Catering   \n",
      "7                     Heirloom LA   \n",
      "\n",
      "                                     Matched_Keywords Skill_ID  \n",
      "0                                                               \n",
      "4   catering solutions, corporate catering, weddin...       11  \n",
      "5                                    corporate events       11  \n",
      "1                 catering services, corporate events       11  \n",
      "11                                                              \n",
      "10                                   corporate events       11  \n",
      "9                                 production catering       11  \n",
      "3   catering services, corporate catering, corpora...       11  \n",
      "14                                                              \n",
      "7                                                               \n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Cell 3: Main Processing Logic\n",
    "# ==============================================================================\n",
    "\n",
    "def process_business_row(row_tuple):\n",
    "    \"\"\"\n",
    "    Worker function that takes a row, scrapes the website, finds matches,\n",
    "    and returns the updated row information.\n",
    "    \"\"\"\n",
    "    index, row_data = row_tuple\n",
    "    company_name = row_data['Company Name']\n",
    "    website_url = row_data['Company Website']\n",
    "    \n",
    "    print(f\"  -> Processing: {company_name}\")\n",
    "    \n",
    "    # Scrape the website and get a list of any keywords that matched\n",
    "    matched_keywords = scrape_and_find_matches(website_url, CATERING_KEYWORDS)\n",
    "    \n",
    "    # Create the new columns based on the results\n",
    "    if matched_keywords:\n",
    "        # Join the list of matched keywords into a single string\n",
    "        row_data['Matched_Keywords'] = \", \".join(matched_keywords)\n",
    "        row_data['Skill_ID'] = CATERING_SKILL_ID\n",
    "    else:\n",
    "        row_data['Matched_Keywords'] = \"\"\n",
    "        row_data['Skill_ID'] = \"\"\n",
    "        \n",
    "    return row_data\n",
    "\n",
    "# --- Main execution block ---\n",
    "if df is not None and CATERING_KEYWORDS:\n",
    "    print(f\"\\nüöÄ Starting tagging process for all {len(df)} businesses...\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    # Use ThreadPoolExecutor to process rows in parallel for speed\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        # We pass df.iterrows() which gives us both the index and the row data\n",
    "        future_to_name = {\n",
    "            executor.submit(process_business_row, row_tuple): row_tuple[1]['Company Name'] \n",
    "            for row_tuple in df.iterrows()\n",
    "        }\n",
    "        \n",
    "        for future in as_completed(future_to_name):\n",
    "            try:\n",
    "                # The result is the updated row (as a dictionary)\n",
    "                updated_row = future.result()\n",
    "                all_results.append(updated_row)\n",
    "            except Exception as exc:\n",
    "                print(f\"‚ùå An error occurred for business {future_to_name[future]}: {exc}\")\n",
    "\n",
    "    # --- Save the final enriched DataFrame ---\n",
    "    if all_results:\n",
    "        # Create a new DataFrame from the list of updated row dictionaries\n",
    "        final_df = pd.DataFrame(all_results)\n",
    "        \n",
    "        # Reorder columns to have the new ones at the end\n",
    "        original_cols = [col for col in df.columns if col in final_df.columns]\n",
    "        new_cols = ['Matched_Keywords', 'Skill_ID']\n",
    "        final_df = final_df[original_cols + new_cols]\n",
    "        \n",
    "        final_df.to_csv(OUTPUT_FILENAME, index=False)\n",
    "        print(f\"\\nüéâ Success! Enriched data saved to '{OUTPUT_FILENAME}'.\")\n",
    "        \n",
    "        # Display a sample of the results\n",
    "        print(\"\\n--- Sample of Final Output ---\")\n",
    "        print(final_df[['Company Name', 'Matched_Keywords', 'Skill_ID']].head(10))\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No businesses were processed.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Script did not run. Check that both input files are available and correctly configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6962112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
