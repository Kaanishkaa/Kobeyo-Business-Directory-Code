{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69668ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import threading\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import deque\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Constants\n",
    "GOOGLE_API_KEY = \"AIzaSyDAixXNtdHf_xoBNbOYWQauftZbk6XkTe8\"\n",
    "TEXT_SEARCH_URL = \"https://maps.googleapis.com/maps/api/place/textsearch/json\"\n",
    "PLACE_DETAILS_URL = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
    "SEARCH_TAGS = [\"mechanic\"]\n",
    "SEARCH_LOCATIONS = [\"Los Angeles\"]\n",
    "MAX_PAGES_PER_SITE = 15\n",
    "CAREERS_MAX_PAGES = 5\n",
    "THREAD_POOL_WORKERS = 5\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fc560d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third-party job sites to exclude\n",
    "THIRD_PARTY_JOB_SITES = [\n",
    "    \"indeed.com\", \"ziprecruiter.com\", \"linkedin.com/jobs\", \"monster.com\",\n",
    "    \"glassdoor.com\", \"careerbuilder.com\", \"simplyhired.com\", \"dice.com\",\n",
    "    \"flexjobs.com\", \"upwork.com\", \"freelancer.com\", \"fiverr.com\",\n",
    "    \"snagajob.com\", \"workday.com\", \"bamboohr.com\", \"greenhouse.io\"\n",
    "]\n",
    "\n",
    "# URL Shortening Configuration\n",
    "MAX_URL_LENGTH = 80\n",
    "ELLIPSIS = \"...\"\n",
    "\n",
    "# Setup headless Chrome for Selenium\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "driver = webdriver.Chrome(options=chrome_options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6a1b378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= URL PROCESSING FUNCTIONS =================\n",
    "\n",
    "def shorten_url(url, max_length=MAX_URL_LENGTH):\n",
    "    \"\"\"Shorten a URL if it exceeds the maximum length\"\"\"\n",
    "    if len(url) <= max_length:\n",
    "        return url\n",
    "\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        domain = parsed.netloc\n",
    "        scheme = parsed.scheme\n",
    "        base_url = f\"{scheme}://{domain}\"\n",
    "\n",
    "        if len(base_url) >= max_length - len(ELLIPSIS):\n",
    "            return base_url[:max_length - len(ELLIPSIS)] + ELLIPSIS\n",
    "\n",
    "        remaining_space = max_length - len(base_url) - len(ELLIPSIS)\n",
    "        if remaining_space > 0 and parsed.path:\n",
    "            path_part = parsed.path[:remaining_space]\n",
    "            return f\"{base_url}{path_part}{ELLIPSIS}\"\n",
    "        else:\n",
    "            return base_url\n",
    "\n",
    "    except Exception:\n",
    "        return url[:max_length - len(ELLIPSIS)] + ELLIPSIS\n",
    "\n",
    "def check_url_status(url, timeout=5):\n",
    "    \"\"\"Check if a URL is working/accessible (returns True if working)\"\"\"\n",
    "    if not url or url == \"None found\":\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        headers = {\"User-Agent\": random.choice(USER_AGENTS)}\n",
    "        response = requests.head(url, headers=headers, timeout=timeout, allow_redirects=True)\n",
    "        return response.status_code < 400\n",
    "    except:\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)\n",
    "            return response.status_code < 400\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "def process_urls(url_string):\n",
    "    \"\"\"Process and shorten URLs, filter out broken ones\"\"\"\n",
    "    if not url_string or url_string == \"None found\":\n",
    "        return url_string\n",
    "\n",
    "    urls = [url.strip() for url in url_string.split(\";\")]\n",
    "    working_urls = []\n",
    "\n",
    "    for url in urls:\n",
    "        if url and check_url_status(url):\n",
    "            working_urls.append(shorten_url(url))\n",
    "\n",
    "    return \"; \".join(working_urls) if working_urls else \"None found\"\n",
    "\n",
    "def is_third_party_job_site(url):\n",
    "    \"\"\"Check if URL is from a third-party job site\"\"\"\n",
    "    return any(site in url.lower() for site in THIRD_PARTY_JOB_SITES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41ba158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= GOOGLE MAPS API FUNCTIONS =================\n",
    "\n",
    "def search_places(query, location, max_results=50):\n",
    "    all_place_ids = []\n",
    "    all_names = []\n",
    "    all_addresses = []\n",
    "\n",
    "    params = {\n",
    "        \"query\": f\"{query} in {location}\",\n",
    "        \"key\": GOOGLE_API_KEY,\n",
    "    }\n",
    "\n",
    "    url = \"https://maps.googleapis.com/maps/api/place/textsearch/json\"\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(all_place_ids) < max_results:\n",
    "        if next_page_token:\n",
    "            params[\"pagetoken\"] = next_page_token\n",
    "            time.sleep(2)  # Required delay for next_page_token to become active\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        for result in data.get(\"results\", []):\n",
    "            all_place_ids.append(result[\"place_id\"])\n",
    "            all_names.append(result[\"name\"])\n",
    "            all_addresses.append(result.get(\"formatted_address\", \"\"))\n",
    "\n",
    "            if len(all_place_ids) >= max_results:\n",
    "                break\n",
    "\n",
    "        next_page_token = data.get(\"next_page_token\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return all_place_ids, all_names, all_addresses\n",
    "\n",
    "\n",
    "def get_place_details(place_id):\n",
    "    \"\"\"Get detailed information about a place\"\"\"\n",
    "    params = {\n",
    "        \"place_id\": place_id,\n",
    "        \"fields\": \"name,formatted_address,website\",\n",
    "        \"key\": GOOGLE_API_KEY,\n",
    "    }\n",
    "    response = requests.get(PLACE_DETAILS_URL, params=params)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    return response.json().get(\"result\", {})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bc6b355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_sales_emails(email_list):\n",
    "    best_keywords = [\n",
    "        \"sales@\", \"orders@\"\n",
    "    ]\n",
    "\n",
    "    better_keywords = [\n",
    "        \"info@\", \"contact@\", \"contactus@\", \"hello@\", \"admin@\", \"mail@\", \"support@\",\n",
    "        \"store@\", \"clinic@\", \"hola@\",  # generic or store-specific emails\n",
    "    ]\n",
    "\n",
    "    exclude_keywords = [\n",
    "        \"invoices@\", \"billing@\", \"guestservices@\", \"estimates@\"\n",
    "    ]\n",
    "\n",
    "    categorized = {\"BEST\": [], \"BETTER\": [], \"EXCLUDE\": []}\n",
    "\n",
    "    for email in email_list:\n",
    "        e = email.lower()\n",
    "\n",
    "        if any(x in e for x in exclude_keywords):\n",
    "            categorized[\"EXCLUDE\"].append(e)\n",
    "        elif any(x in e for x in best_keywords):\n",
    "            categorized[\"BEST\"].append(e)\n",
    "        elif any(x in e for x in better_keywords):\n",
    "            categorized[\"BETTER\"].append(e)\n",
    "        elif len(e.split(\"@\")[0]) <= 4 or re.match(r\".*@(gmail|yahoo|hotmail|outlook)\\.\", e):\n",
    "            categorized[\"EXCLUDE\"].append(e)\n",
    "        else:\n",
    "            categorized[\"BETTER\"].append(e)  # fallback if unknown\n",
    "\n",
    "    return categorized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d78380bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emails(text, soup=None):\n",
    "    pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
    "    emails = set(re.findall(pattern, text))\n",
    "\n",
    "    if soup:\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            href = a['href'].lower()\n",
    "            if href.startswith(\"mailto:\"):\n",
    "                email = href[7:].split(\"?\")[0].strip()\n",
    "                if re.match(pattern, email):\n",
    "                    emails.add(email)\n",
    "            visible = a.get_text(strip=True)\n",
    "            if re.match(pattern, visible):\n",
    "                emails.add(visible)\n",
    "\n",
    "    return list(emails)\n",
    "\n",
    "def categorize_emails(email_list):\n",
    "    best_keywords = [\n",
    "        \"careers@\", \"joinus@\", \"hr@\", \"ta@\", \"talentaquisition@\", \"humanresources@\", \"apply@\", \"jobs@\", \n",
    "        \"hiring@\", \"recruiting@\", \"recruitment@\", \"talent@\", \"talentteam@\", \"people@\", \"peopleops@\", \n",
    "        \"applications@\", \"submit@\", \"cv@\", \"resume@\", \"workwithus@\", \"jobshr@\", \"hrteam@\", \"recruiters@\", \n",
    "        \"talentmgmt@\", \"hiringteam@\", \"teamhr@\", \"opportunities@\", \"team@\", \"staffing@\", \"onboarding@\"\n",
    "    ]\n",
    "\n",
    "    better_keywords = [\n",
    "        \"info@\", \"contact@\", \"contactus@\", \"hello@\", \"admin@\", \"mail@\", \"hola@\", \"store@\", \"clinic@\", \"office@\"\n",
    "    ]\n",
    "\n",
    "    exclude_keywords = [\n",
    "        \"support@\", \"invoices@\", \"billing@\", \"guestservices@\", \"estimates@\", \n",
    "        \"sales@\", \"orders@\", \"customerservice@\"\n",
    "    ]\n",
    "\n",
    "    categorized = {\"BEST\": [], \"BETTER\": [], \"EXCLUDE\": []}\n",
    "\n",
    "    for email in email_list:\n",
    "        e = email.lower()\n",
    "\n",
    "        if any(x in e for x in exclude_keywords):\n",
    "            categorized[\"EXCLUDE\"].append(e)\n",
    "        elif any(x in e for x in best_keywords):\n",
    "            categorized[\"BEST\"].append(e)\n",
    "        elif any(x in e for x in better_keywords):\n",
    "            categorized[\"BETTER\"].append(e)\n",
    "        elif len(e.split(\"@\")[0]) <= 4:  # <== only now apply short-length heuristic (very conservative)\n",
    "            categorized[\"EXCLUDE\"].append(e)\n",
    "        else:\n",
    "            categorized[\"BETTER\"].append(e)  # fallback if unknown, but likely org email\n",
    "\n",
    "    return categorized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77ccb2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= PAGE DETECTION FUNCTIONS =================\n",
    "\n",
    "def detect_careers_page(url, text, soup):\n",
    "    \"\"\"Detect if this is a careers/jobs page and if it's internal\"\"\"\n",
    "    if is_third_party_job_site(url):\n",
    "        return False, \"\"\n",
    "\n",
    "    careers_indicators = [\n",
    "        \"career\", \"careers\", \"job\", \"jobs\", \"employment\", \"hiring\", \"positions\",\n",
    "        \"join our team\", \"work with us\", \"apply now\", \"open positions\"\n",
    "    ]\n",
    "\n",
    "    url_lower = url.lower()\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Check URL path\n",
    "    url_has_careers = any(indicator in url_lower for indicator in careers_indicators)\n",
    "\n",
    "    # Check page content\n",
    "    content_has_careers = any(indicator in text_lower for indicator in careers_indicators)\n",
    "\n",
    "    # Look for application forms\n",
    "    has_application_form = False\n",
    "    if soup:\n",
    "        forms = soup.find_all(\"form\")\n",
    "        for form in forms:\n",
    "            form_text = form.get_text().lower()\n",
    "            if any(word in form_text for word in [\"apply\", \"application\", \"resume\", \"cv\", \"position\"]):\n",
    "                has_application_form = True\n",
    "                break\n",
    "\n",
    "    if url_has_careers or (content_has_careers and has_application_form):\n",
    "        return True, url\n",
    "\n",
    "    return False, \"\"\n",
    "\n",
    "def detect_products_services_page(url, text, soup):\n",
    "    \"\"\"Detect pages showing products, services, or industries served\"\"\"\n",
    "    if not text:\n",
    "        return False, \"\"\n",
    "\n",
    "    # Keywords that indicate products/services pages\n",
    "    product_service_indicators = [\n",
    "        \"products\", \"services\", \"solutions\", \"offerings\", \"what we do\",\n",
    "        \"industries\", \"sectors\", \"specialties\", \"capabilities\", \"expertise\",\n",
    "        \"portfolio\", \"catalog\", \"menu\", \"pricing\", \"packages\"\n",
    "    ]\n",
    "\n",
    "    url_lower = url.lower()\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Check URL path\n",
    "    url_indicates_products = any(indicator in url_lower for indicator in product_service_indicators)\n",
    "\n",
    "    # Check if content has substantial product/service information\n",
    "    content_score = sum(1 for indicator in product_service_indicators if indicator in text_lower)\n",
    "\n",
    "    # Look for structured content (lists, grids, etc.)\n",
    "    has_structured_content = False\n",
    "    if soup:\n",
    "        lists = soup.find_all(['ul', 'ol', 'div'])\n",
    "        for element in lists:\n",
    "            element_text = element.get_text().lower()\n",
    "            if any(indicator in element_text for indicator in product_service_indicators):\n",
    "                has_structured_content = True\n",
    "                break\n",
    "\n",
    "    if url_indicates_products or (content_score >= 2 and has_structured_content):\n",
    "        return True, url\n",
    "\n",
    "    return False, \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25afc845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= EXTRACT SOCIAL MEDIA LINKS =================\n",
    "\n",
    "def extract_socials(soup):\n",
    "    \"\"\"Extract Instagram, Facebook, and X (Twitter) links \"\"\"\n",
    "    social_links = {\"Instagram\": \"\", \"Facebook\": \"\", \"X\": \"\"}\n",
    "\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        if \"instagram.com\" in href and not social_links[\"Instagram\"]:\n",
    "            social_links[\"Instagram\"] = href\n",
    "        elif \"facebook.com\" in href and not social_links[\"Facebook\"]:\n",
    "            social_links[\"Facebook\"] = href\n",
    "        elif \"twitter.com\" in href or \"x.com\" in href:\n",
    "            social_links[\"X\"] = href\n",
    "\n",
    "    return social_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f38a64d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= WEB CRAWLING =================\n",
    "\n",
    "def crawl_site_comprehensive(base_url, max_pages=15):\n",
    "    \"\"\"Comprehensive website crawling with categorized emails, social media, and Selenium fallback\"\"\"\n",
    "    visited = set()\n",
    "    queue = deque([base_url])\n",
    "    emails = set()\n",
    "    social_links = {\"Instagram\": \"\", \"Facebook\": \"\", \"X\": \"\"}\n",
    "    careers_pages = set()\n",
    "    products_services_pages = set()\n",
    "    debug_info = \"\"\n",
    "\n",
    "    domain = urlparse(base_url).netloc.replace(\"www.\", \"\")\n",
    "    pages_crawled = 0\n",
    "\n",
    "    try:\n",
    "        while queue and pages_crawled < max_pages:\n",
    "            url = queue.popleft()\n",
    "            if url in visited:\n",
    "                continue\n",
    "            visited.add(url)\n",
    "\n",
    "            headers = {\"User-Agent\": random.choice(USER_AGENTS)}\n",
    "            resp = requests.get(url, headers=headers, timeout=10)\n",
    "            resp.raise_for_status()\n",
    "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "            text = soup.get_text(\" \", strip=True)\n",
    "\n",
    "            emails.update(extract_emails(text, soup))\n",
    "\n",
    "            if not all(social_links.values()):\n",
    "                new_socials = extract_socials(soup)\n",
    "                for key in social_links:\n",
    "                    if not social_links[key] and new_socials[key]:\n",
    "                        social_links[key] = new_socials[key]\n",
    "\n",
    "            is_careers, careers_url = detect_careers_page(url, text, soup)\n",
    "            if is_careers:\n",
    "                careers_pages.add(careers_url)\n",
    "\n",
    "            is_products, products_url = detect_products_services_page(url, text, soup)\n",
    "            if is_products:\n",
    "                products_services_pages.add(products_url)\n",
    "\n",
    "            for a in soup.find_all(\"a\", href=True):\n",
    "                new_url = urljoin(url, a['href'])\n",
    "                parsed_url = urlparse(new_url)\n",
    "                if domain in parsed_url.netloc and new_url not in visited:\n",
    "                    queue.append(new_url)\n",
    "\n",
    "            pages_crawled += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        debug_info += f\"Requests failed: {type(e).__name__}: {str(e).split(':')[0]}. \"\n",
    "\n",
    "    # Retry with Selenium\n",
    "    if not emails or not all(social_links.values()):\n",
    "        try:\n",
    "            driver.set_page_load_timeout(30)\n",
    "            driver.get(base_url)\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            text = soup.get_text(\" \", strip=True)\n",
    "\n",
    "            new_emails = extract_emails(text, soup)\n",
    "            emails.update(new_emails)\n",
    "\n",
    "            new_socials = extract_socials(soup)\n",
    "            for key in social_links:\n",
    "                if not social_links[key] and new_socials[key]:\n",
    "                    social_links[key] = new_socials[key]\n",
    "\n",
    "            if not new_emails and not debug_info:\n",
    "                debug_info = \"Selenium used but still no emails found.\"\n",
    "\n",
    "        except Exception as e:\n",
    "            error_type = type(e).__name__\n",
    "            short_message = str(e).split(\":\")[0][:100]\n",
    "            debug_info += f\"Selenium failed ({error_type}): {short_message.strip()}.\"\n",
    "\n",
    "    if emails:\n",
    "        debug_info = \"\"\n",
    "    elif not debug_info:\n",
    "        debug_info = \"No emails found from requests or Selenium.\"\n",
    "\n",
    "    categorized = categorize_emails(list(emails))\n",
    "    sales_categorized = categorize_sales_emails(list(emails))\n",
    "\n",
    "    return {\n",
    "    \"best_email\": categorized[\"BEST\"],\n",
    "    \"better_email\": categorized[\"BETTER\"],\n",
    "    \"excluded_email\": categorized[\"EXCLUDE\"],\n",
    "    \"sales_best\": sales_categorized[\"BEST\"],\n",
    "    \"sales_better\": sales_categorized[\"BETTER\"],\n",
    "    \"sales_exclude\": sales_categorized[\"EXCLUDE\"],\n",
    "    \"careers_pages\": list(careers_pages),\n",
    "    \"products_services_pages\": list(products_services_pages),\n",
    "    \"social_links\": social_links,\n",
    "    \"debug_info\": debug_info,\n",
    "    \"pages_crawled\": pages_crawled\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5dc5dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= BUSINESS PROCESSING =================\n",
    "\n",
    "def process_business_comprehensive(place_id, name, fallback_address):\n",
    "    \"\"\"Process a business with categorized emails and links\"\"\"\n",
    "    print(f\"Processing business: {name}\")\n",
    "\n",
    "    details = get_place_details(place_id)\n",
    "    if not details:\n",
    "        return None\n",
    "\n",
    "    website = details.get(\"website\", \"\")\n",
    "    address = details.get(\"formatted_address\", fallback_address)\n",
    "\n",
    "    result = {\n",
    "        \"Company Name\": name,\n",
    "        \"Company Address\": address,\n",
    "        \"Company Website\": website,\n",
    "        \"Best Email\": \"\",\n",
    "        \"Better Email\": \"\",\n",
    "        \"Excluded Email\": \"\",\n",
    "        \"Sales BEST\": \"\",\n",
    "        \"Sales BETTER\": \"\",\n",
    "        \"Sales EXCLUDE\": \"\",    \n",
    "        \"Company Careers Page\": \"\",\n",
    "        \"Company Products/Services Page\": \"\",\n",
    "        \"Instagram\": \"\",\n",
    "        \"Facebook\": \"\",\n",
    "        \"X\": \"\"\n",
    "    }\n",
    "\n",
    "    if website:\n",
    "        try:\n",
    "            crawl_results = crawl_site_comprehensive(website, MAX_PAGES_PER_SITE)\n",
    "\n",
    "            if crawl_results[\"best_email\"]:\n",
    "                result[\"Best Email\"] = \"; \".join(crawl_results[\"best_email\"])\n",
    "\n",
    "            if crawl_results[\"better_email\"]:\n",
    "                result[\"Better Email\"] = \"; \".join(crawl_results[\"better_email\"])\n",
    "\n",
    "            if crawl_results[\"excluded_email\"]:\n",
    "                result[\"Excluded Email\"] = \"; \".join(crawl_results[\"excluded_email\"])\n",
    "            \n",
    "            if crawl_results[\"sales_best\"]:\n",
    "                result[\"Sales BEST\"] = \"; \".join(crawl_results[\"sales_best\"])\n",
    "\n",
    "            if crawl_results[\"sales_better\"]:\n",
    "                result[\"Sales BETTER\"] = \"; \".join(crawl_results[\"sales_better\"])\n",
    "\n",
    "            if crawl_results[\"sales_exclude\"]:\n",
    "                result[\"Sales EXCLUDE\"] = \"; \".join(crawl_results[\"sales_exclude\"])\n",
    "\n",
    "            if crawl_results[\"careers_pages\"]:\n",
    "                result[\"Company Careers Page\"] = \"; \".join(crawl_results[\"careers_pages\"])\n",
    "\n",
    "            if crawl_results[\"products_services_pages\"]:\n",
    "                result[\"Company Products/Services Page\"] = \"; \".join(crawl_results[\"products_services_pages\"])\n",
    "\n",
    "\n",
    "            socials = crawl_results[\"social_links\"]\n",
    "            result[\"Instagram\"] = socials.get(\"Instagram\", \"\")\n",
    "            result[\"Facebook\"] = socials.get(\"Facebook\", \"\")\n",
    "            result[\"X\"] = socials.get(\"X\", \"\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to crawl {website}: {e}\")\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "715b3863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= MAIN FUNCTION =================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"🚀 Starting comprehensive business data scraper\")\n",
    "\n",
    "    lock = threading.Lock()\n",
    "    checkpoint_file = \"business_directory.csv\"\n",
    "\n",
    "    # Define CSV columns\n",
    "    csv_columns = [\n",
    "        \"Company Name\",\n",
    "        \"Company Address\",\n",
    "        \"Company Website\",\n",
    "        \"HR Email\",\n",
    "        \"Sales BEST\",\n",
    "        \"Sales BETTER\",\n",
    "        \"Sales EXCLUDE\",\n",
    "        \"Company Careers Page\",\n",
    "        \"Company Products/Services Page\",\n",
    "        \"Instagram\",\n",
    "        \"Facebook\",\n",
    "        \"X\"\n",
    "    ]\n",
    "\n",
    "    # Load existing data\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        existing_df = pd.read_csv(checkpoint_file)\n",
    "        print(f\"📄 Loaded {len(existing_df)} existing records\")\n",
    "    else:\n",
    "        existing_df = pd.DataFrame(columns=csv_columns)\n",
    "\n",
    "    existing_data = {row['Company Name']: row for _, row in existing_df.iterrows()} if not existing_df.empty else {}\n",
    "\n",
    "    # crawl_times = []\n",
    "    metrics = {\n",
    "        \"total_businesses\": 0,\n",
    "        \"emails_found\": 0,\n",
    "        \"best_emails\": 0,\n",
    "        \"better_emails\": 0,\n",
    "        \"excluded_emails\": 0,\n",
    "        \"priority_email_used\": 0,\n",
    "        \"social_found\": 0,\n",
    "        \"all_socials_found\": 0,\n",
    "        \"nothing_scraped\": 0\n",
    "    }\n",
    "\n",
    "    for location in SEARCH_LOCATIONS:\n",
    "        for tag in SEARCH_TAGS:\n",
    "            print(f\"\\n🔍 Searching for {tag} in {location}\")\n",
    "            place_ids, names, addresses  = search_places(tag, location)\n",
    "            print(f\"📍 Found {len(place_ids)} businesses\")\n",
    "\n",
    "            with ThreadPoolExecutor(max_workers=THREAD_POOL_WORKERS) as executor:\n",
    "                futures = {\n",
    "                    executor.submit(process_business_comprehensive, pid, name, addr): name\n",
    "                    for pid, name, addr in zip(place_ids, names, addresses)\n",
    "                    if name not in existing_data\n",
    "                }\n",
    "\n",
    "                for future in as_completed(futures):\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        with lock:\n",
    "                            # Apply email priority logic\n",
    "                            priority_email = \"\"\n",
    "                            if result[\"Best Email\"] and result[\"Best Email\"] != \"None found\":\n",
    "                                priority_email = result[\"Best Email\"].split(\";\")[0]\n",
    "                            elif result[\"Better Email\"] and result[\"Better Email\"] != \"None found\":\n",
    "                                priority_email = result[\"Better Email\"].split(\";\")[0]\n",
    "                            elif result[\"Excluded Email\"] and any(\"@\" in e and any(char.isalpha() for char in e.split(\"@\")[0]) for e in result[\"Excluded Email\"].split(\";\")):\n",
    "                                for e in result[\"Excluded Email\"].split(\";\"):\n",
    "                                    if \"@\" in e and any(char.isalpha() for char in e.split(\"@\")[0]):\n",
    "                                        priority_email = e\n",
    "                                        break\n",
    "\n",
    "                            result[\"HR Email\"] = priority_email\n",
    "\n",
    "                            # Clean URLs\n",
    "                            for field in [\"Company Careers Page\", \"Company Products/Services Page\"]:\n",
    "                                if result[field]:\n",
    "                                    result[field] = process_urls(result[field])\n",
    "                                # else:\n",
    "                                #     result[field] = \"None found\"\n",
    "\n",
    "                            # Fill blanks\n",
    "                            # for field in [\"Best Email\", \"Better Email\", \"Excluded Email\", \"Instagram\", \"Facebook\", \"X\", \"Priority Email\"]:\n",
    "                            #     if not result[field]:\n",
    "                            #         result[field] = \"None found\"\n",
    "\n",
    "                            # Update metrics\n",
    "                            metrics[\"total_businesses\"] += 1\n",
    "                            if any(result[f] != \"None found\" for f in [\"Best Email\", \"Better Email\", \"Excluded Email\"]):\n",
    "                                metrics[\"emails_found\"] += 1\n",
    "                            if result[\"Best Email\"] != \"None found\":\n",
    "                                metrics[\"best_emails\"] += 1\n",
    "                            if result[\"Better Email\"] != \"None found\":\n",
    "                                metrics[\"better_emails\"] += 1\n",
    "                            if result[\"Excluded Email\"] != \"None found\":\n",
    "                                metrics[\"excluded_emails\"] += 1\n",
    "                            if result[\"HR Email\"] != \"None found\":\n",
    "                                metrics[\"priority_email_used\"] += 1\n",
    "                            if any(result[f] != \"None found\" for f in [\"Instagram\", \"Facebook\", \"X\"]):\n",
    "                                metrics[\"social_found\"] += 1\n",
    "                            if all(result[f] != \"None found\" for f in [\"Instagram\", \"Facebook\", \"X\"]):\n",
    "                                metrics[\"all_socials_found\"] += 1\n",
    "                            if all(result[f] == \"None found\" for f in [\"Best Email\", \"Better Email\", \"Excluded Email\", \"Instagram\", \"Facebook\", \"X\"]):\n",
    "                                metrics[\"nothing_scraped\"] += 1\n",
    "\n",
    "                            for col in [\"Best Email\", \"Better Email\", \"Excluded Email\"]:\n",
    "                                if col in result:\n",
    "                                    del result[col]\n",
    "                            # Save result\n",
    "                            existing_df = pd.concat([existing_df, pd.DataFrame([result])], ignore_index=True)\n",
    "                            existing_df.to_csv(checkpoint_file, index=False)\n",
    "                            print(f\"✅ Saved: {result['Company Name']}\")\n",
    "\n",
    "    # Report\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"📊 FINAL METRICS REPORT\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    total = metrics[\"total_businesses\"]\n",
    "    def pct(v): return f\"{(v/total*100):.1f}%\" if total else \"0.0%\"\n",
    "\n",
    "    print(f\"📈 Total businesses processed: {total}\")\n",
    "    print(f\"🌐 Businesses with any emails: {metrics['emails_found']} ({pct(metrics['emails_found'])})\")\n",
    "    print(f\"👔 Businesses with BEST emails: {metrics['best_emails']} ({pct(metrics['best_emails'])})\")\n",
    "    print(f\"📧 Businesses with BETTER emails: {metrics['better_emails']} ({pct(metrics['better_emails'])})\")\n",
    "    print(f\"🚫 Businesses with EXCLUDED emails: {metrics['excluded_emails']} ({pct(metrics['excluded_emails'])})\")\n",
    "    print(f\"🎯 Businesses where Priority Email used: {metrics['priority_email_used']} ({pct(metrics['priority_email_used'])})\")\n",
    "    print(f\"📱 Businesses with any social media: {metrics['social_found']} ({pct(metrics['social_found'])})\")\n",
    "    print(f\"✔️ Businesses with all 3 socials: {metrics['all_socials_found']} ({pct(metrics['all_socials_found'])})\")\n",
    "    print(f\"❌ Businesses with no contact info: {metrics['nothing_scraped']} ({pct(metrics['nothing_scraped'])})\")\n",
    "    print(\"🎉 Scraping completed successfully!\")\n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2979bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting comprehensive business data scraper\n",
      "📄 Loaded 59 existing records\n",
      "\n",
      "🔍 Searching for mechanic in Los Angeles\n",
      "📍 Found 50 businesses\n",
      "Processing business: Figueroa's Auto Repair\n",
      "Processing business: 24/7 LA Auto Club mobile mechanic\n",
      "Processing business: JC's Auto Repair Shop Los Angeles\n",
      "Processing business: MP Auto Repair\n",
      "Processing business: Pinky's\n",
      "Processing business: Affordable Mobile Mechanics\n",
      "✅ Saved: Figueroa's Auto Repair\n",
      "Processing business: Bussard's All Pro Automotive Center\n",
      "✅ Saved: MP Auto Repair\n",
      "Processing business: WeHo Auto Repair\n",
      "✅ Saved: Affordable Mobile Mechanics\n",
      "Processing business: Westside Mobile Mechanics | Car Battery Replacement, Brakes, Jumpstart and More\n",
      "✅ Saved: WeHo Auto Repair\n",
      "Processing business: Avo's Automotive\n",
      "✅ Saved: Westside Mobile Mechanics | Car Battery Replacement, Brakes, Jumpstart and More\n",
      "Processing business: Auto Repair Specialists\n",
      "✅ Saved: 24/7 LA Auto Club mobile mechanic\n",
      "Processing business: Mid City Complete Auto\n",
      "Processing business: Sunset Auto Repair\n",
      "Processing business: Mobile Mechanic Los Angeles\n",
      "✅ Saved: Pinky's\n",
      "✅ Saved: Auto Repair Specialists\n",
      "Processing business: Hi Tech Automotive\n",
      "✅ Saved: JC's Auto Repair Shop Los Angeles\n",
      "Processing business: Complete Automotive Systems\n",
      "✅ Saved: Avo's Automotive\n",
      "✅ Saved: Mobile Mechanic Los Angeles\n",
      "Processing business: LA Auto Repair\n",
      "Processing business: 10 Minute Oil & General Mechanic\n",
      "Processing business: United Automotive Repair\n",
      "Processing business: Rivera's Mechanic Shop\n",
      "Processing business: M&D Auto Mechanic Service\n",
      "Processing business: My Mobile Mechanic of Los Angeles\n",
      "✅ Saved: Bussard's All Pro Automotive Center\n",
      "✅ Saved: Complete Automotive Systems\n",
      "✅ Saved: 10 Minute Oil & General Mechanic\n",
      "✅ Saved: LA Auto Repair\n",
      "✅ Saved: Rivera's Mechanic Shop\n",
      "✅ Saved: M&D Auto Mechanic Service\n",
      "Processing business: Miranda's Shop\n",
      "Processing business: Drive Moor Auto Repair [los angeles]\n",
      "Processing business: Long Automotive\n",
      "Processing business: Witmer Auto Service\n",
      "Processing business: LA Mobile Mechanic\n",
      "✅ Saved: United Automotive Repair\n",
      "✅ Saved: Miranda's Shop\n",
      "✅ Saved: Drive Moor Auto Repair [los angeles]\n",
      "✅ Saved: Long Automotive\n",
      "✅ Saved: Witmer Auto Service\n",
      "Processing business: 5 Speed Motors Auto Repair\n",
      "✅ Saved: LA Mobile Mechanic\n",
      "Processing business: E & C Auto Repair\n",
      "Processing business: Jasson’s Mobile Mecanic\n",
      "Processing business: MECHANIC ON THE SPOT 24/7\n",
      "Processing business: LA & J Auto Repair\n",
      "Processing business: ANTLO Mobile Mechanic\n",
      "Processing business: 2Shy Mobile Mechanic, (Almost) 24/7 Availability Affordable Auto Repair Services\n",
      "✅ Saved: Hi Tech Automotive\n",
      "✅ Saved: E & C Auto Repair\n",
      "✅ Saved: Jasson’s Mobile Mecanic\n",
      "✅ Saved: MECHANIC ON THE SPOT 24/7\n",
      "✅ Saved: LA & J Auto Repair\n",
      "✅ Saved: ANTLO Mobile Mechanic\n",
      "Processing business: LA&J Auto RepairProcessing business: Hilario Auto Repair\n",
      "\n",
      "✅ Saved: 5 Speed Motors Auto Repair\n",
      "Processing business: Los Angeles Air Conditioning Assistance\n",
      "Processing business: The Mechanic Franklin Auto Center\n",
      "Processing business: ELITE AUTO GARAGE\n",
      "Processing business: La Auto Center\n",
      "Processing business: J&E Complete Auto Service\n",
      "✅ Saved: Mid City Complete Auto\n",
      "✅ Saved: LA&J Auto Repair\n",
      "✅ Saved: Hilario Auto Repair\n",
      "✅ Saved: The Mechanic Franklin Auto Center\n",
      "✅ Saved: Los Angeles Air Conditioning Assistance\n",
      "✅ Saved: My Mobile Mechanic of Los Angeles\n",
      "Processing business: Fox Heating & Air Conditioning Services\n",
      "Processing business: Auto Services Body Shop Mechanic Shop inc\n",
      "✅ Saved: 2Shy Mobile Mechanic, (Almost) 24/7 Availability Affordable Auto Repair Services\n",
      "✅ Saved: Fox Heating & Air Conditioning Services\n",
      "Processing business: Hobart Auto Center\n",
      "✅ Saved: Auto Services Body Shop Mechanic Shop inc\n",
      "Processing business: Superformance Foreign Auto Repair\n",
      "✅ Saved: ELITE AUTO GARAGE\n",
      "Processing business: University Tire & Auto Service\n",
      "✅ Saved: Hobart Auto Center\n",
      "Processing business: A1 Auto Repair & Smog Check- Pass or Free\n",
      "✅ Saved: La Auto Center\n",
      "Processing business: Don's Auto Repair DBA Prestige Auto AI Inc\n",
      "Processing business: Hollywood Star Smog Check & Auto Repair\n",
      "✅ Saved: Sunset Auto Repair\n",
      "✅ Saved: J&E Complete Auto Service\n",
      "Processing business: Carotech Automotive & Complete Car Care Center\n",
      "✅ Saved: University Tire & Auto Service\n",
      "✅ Saved: Superformance Foreign Auto Repair\n",
      "✅ Saved: Don's Auto Repair DBA Prestige Auto AI Inc\n",
      "✅ Saved: A1 Auto Repair & Smog Check- Pass or Free\n",
      "✅ Saved: Hollywood Star Smog Check & Auto Repair\n",
      "✅ Saved: Carotech Automotive & Complete Car Care Center\n",
      "\n",
      "==================================================\n",
      "📊 FINAL METRICS REPORT\n",
      "==================================================\n",
      "📈 Total businesses processed: 50\n",
      "🌐 Businesses with any emails: 50 (100.0%)\n",
      "👔 Businesses with BEST emails: 50 (100.0%)\n",
      "📧 Businesses with BETTER emails: 50 (100.0%)\n",
      "🚫 Businesses with EXCLUDED emails: 50 (100.0%)\n",
      "🎯 Businesses where Priority Email used: 50 (100.0%)\n",
      "📱 Businesses with any social media: 50 (100.0%)\n",
      "✔️ Businesses with all 3 socials: 50 (100.0%)\n",
      "❌ Businesses with no contact info: 0 (0.0%)\n",
      "🎉 Scraping completed successfully!\n",
      "==================================================\n",
      "🔧 Selenium driver closed successfully\n"
     ]
    }
   ],
   "source": [
    "# ================= CODE DRIVER METHOD =================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    finally:\n",
    "        try:\n",
    "            driver.quit()\n",
    "            print(\"🔧 Selenium driver closed successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error while closing Selenium driver: {type(e).__name__} - {str(e)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
