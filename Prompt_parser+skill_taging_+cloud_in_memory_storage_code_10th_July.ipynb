{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAP9HA-vH6iS",
        "outputId": "7bf6a8b6-5dae-4bfe-b143-46f48834015c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.34.2-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting urllib3~=2.5.0 (from urllib3[socks]~=2.5.0->selenium)\n",
            "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting trio~=0.30.0 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.6.15 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.7.9)\n",
            "Requirement already satisfied: typing_extensions~=4.14.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.14.1)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.30.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.34.2-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.8/129.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, urllib3, outcome, trio, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.4.0\n",
            "    Uninstalling urllib3-2.4.0:\n",
            "      Successfully uninstalled urllib3-2.4.0\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.34.2 trio-0.30.0 trio-websocket-0.12.2 urllib3-2.5.0 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "pip install selenium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_0aQ0GTH8ea",
        "outputId": "7baad17e-e5d2-4e6f-d586-d2a23fb8e728"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googlemaps\n",
            "  Downloading googlemaps-4.10.0.tar.gz (33 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests<3.0,>=2.20.0 in /usr/local/lib/python3.11/dist-packages (from googlemaps) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.20.0->googlemaps) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.20.0->googlemaps) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.20.0->googlemaps) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.20.0->googlemaps) (2025.7.9)\n",
            "Building wheels for collected packages: googlemaps\n",
            "  Building wheel for googlemaps (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googlemaps: filename=googlemaps-4.10.0-py3-none-any.whl size=40714 sha256=1e5371c712a7dcb6875e29b93fb811166b1a87598cbdc1a88aadb2aa8fa92e94\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/09/77/3cc2f5659cbc62341b30f806aca2b25e6a26c351daa5b1f49a\n",
            "Successfully built googlemaps\n",
            "Installing collected packages: googlemaps\n",
            "Successfully installed googlemaps-4.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install googlemaps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAEr4YYFH8hz"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import requests\n",
        "import openai\n",
        "import time\n",
        "import csv\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Dict, Any\n",
        "from datetime import datetime\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cr4i0p4oNQ0g"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Business:\n",
        "    name: str\n",
        "    address: str\n",
        "    location: Dict[str, float]\n",
        "    website: Optional[str] = None\n",
        "    phone: Optional[str] = None\n",
        "    description: str = \"\"\n",
        "    place_id: str = \"\"\n",
        "    rating: Optional[float] = None\n",
        "    business_type: List[str] = None\n",
        "    potential_jobs: List[str] = None\n",
        "    required_skills: List[str] = None\n",
        "    job_categories: List[str] = None\n",
        "    hours: Optional[Dict] = None\n",
        "    reviews_count: int = 0\n",
        "    hiring_likelihood: int = 5\n",
        "\n",
        "@dataclass\n",
        "class ParsedBusinessQuery:\n",
        "    business_type_keywords: List[str]\n",
        "    location_keywords: List[str]\n",
        "    industry_keywords: List[str]\n",
        "    size_indicators: List[str]\n",
        "    original_query: str\n",
        "    confidence_score: float"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "-FbBAI_4NQ4o",
        "outputId": "0a9341da-c9c6-46d2-b879-fac28240803e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Business Scraping with Skill Matching\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-2333349415.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-5-2333349415.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;31m# Get API keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m     \u001b[0mgoogle_api_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your Google Places API Key: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m     \u001b[0mopenai_api_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your OpenAI API Key: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "class SmartBusinessScraper:\n",
        "    def __init__(self, google_api_key: str, openai_api_key: str):\n",
        "        self.google_api_key = google_api_key\n",
        "        self.openai_client = openai.OpenAI(api_key=openai_api_key)\n",
        "        self.places_base_url = \"https://maps.googleapis.com/maps/api/place\"\n",
        "\n",
        "        # Comprehensive skill dictionary organized by categories\n",
        "        self.skill_dictionary = {\n",
        "            \"food_service\": [\n",
        "                \"cooking\", \"food preparation\", \"dishwashing\", \"serving\", \"bartending\",\n",
        "                \"barista\", \"food safety\", \"customer service\", \"cash handling\", \"menu knowledge\",\n",
        "                \"kitchen operations\", \"food plating\", \"inventory management\", \"cleaning\",\n",
        "                \"order taking\", \"table service\", \"beverage preparation\", \"food handling\"\n",
        "            ],\n",
        "            \"retail\": [\n",
        "                \"sales\", \"customer service\", \"cash register\", \"inventory\", \"merchandising\",\n",
        "                \"product knowledge\", \"visual display\", \"stock management\", \"cashier\",\n",
        "                \"point of sale\", \"customer relations\", \"loss prevention\", \"fitting room\"\n",
        "            ],\n",
        "            \"hospitality\": [\n",
        "                \"front desk\", \"housekeeping\", \"guest services\", \"reservations\", \"concierge\",\n",
        "                \"room service\", \"event coordination\", \"customer relations\", \"hospitality management\",\n",
        "                \"cleaning\", \"laundry\", \"maintenance\", \"security\"\n",
        "            ],\n",
        "            \"healthcare\": [\n",
        "                \"patient care\", \"medical assistance\", \"receptionist\", \"scheduling\", \"filing\",\n",
        "                \"insurance processing\", \"medical records\", \"customer service\", \"cleaning\",\n",
        "                \"administrative support\", \"data entry\"\n",
        "            ],\n",
        "            \"office_admin\": [\n",
        "                \"data entry\", \"filing\", \"receptionist\", \"phone answering\", \"scheduling\",\n",
        "                \"customer service\", \"administrative support\", \"document management\",\n",
        "                \"computer skills\", \"organization\", \"communication\"\n",
        "            ],\n",
        "            \"manual_labor\": [\n",
        "                \"construction\", \"maintenance\", \"cleaning\", \"landscaping\", \"delivery\",\n",
        "                \"warehouse\", \"loading\", \"unloading\", \"assembly\", \"repair\", \"installation\",\n",
        "                \"heavy lifting\", \"equipment operation\"\n",
        "            ],\n",
        "            \"creative\": [\n",
        "                \"graphic design\", \"photography\", \"writing\", \"social media\", \"marketing\",\n",
        "                \"content creation\", \"video editing\", \"web design\", \"art\", \"creative writing\"\n",
        "            ],\n",
        "            \"transportation\": [\n",
        "                \"driving\", \"delivery\", \"logistics\", \"vehicle maintenance\", \"customer service\",\n",
        "                \"navigation\", \"time management\", \"safety protocols\"\n",
        "            ],\n",
        "            \"technology\": [\n",
        "                \"software development\", \"programming\", \"IT support\", \"data analysis\", \"cybersecurity\",\n",
        "                \"web development\", \"database management\", \"system administration\", \"technical support\"\n",
        "            ],\n",
        "            \"education\": [\n",
        "                \"teaching\", \"tutoring\", \"curriculum development\", \"student support\", \"administration\",\n",
        "                \"classroom management\", \"educational technology\", \"assessment\", \"counseling\"\n",
        "            ],\n",
        "            \"finance\": [\n",
        "                \"accounting\", \"bookkeeping\", \"financial analysis\", \"tax preparation\", \"auditing\",\n",
        "                \"banking\", \"investment\", \"insurance\", \"payroll\", \"budgeting\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        # Business type patterns for query parsing\n",
        "        self.business_type_patterns = [\n",
        "            r'\\b(restaurant|cafe|coffee|food|dining|bar|pub|eatery|bistro|diner)\\b',\n",
        "            r'\\b(shop|store|retail|boutique|market|mall|outlet|plaza)\\b',\n",
        "            r'\\b(hotel|motel|resort|inn|lodge|hospitality|accommodation)\\b',\n",
        "            r'\\b(hospital|clinic|medical|healthcare|dental|pharmacy|wellness)\\b',\n",
        "            r'\\b(office|corporate|business|professional|consulting|agency)\\b',\n",
        "            r'\\b(construction|contractor|builder|renovation|maintenance)\\b',\n",
        "            r'\\b(creative|design|marketing|advertising|media|studio)\\b',\n",
        "            r'\\b(transportation|logistics|delivery|shipping|trucking)\\b',\n",
        "            r'\\b(tech|technology|software|IT|startup|digital)\\b',\n",
        "            r'\\b(school|education|university|college|training|academy)\\b',\n",
        "            r'\\b(bank|financial|accounting|insurance|investment)\\b'\n",
        "        ]\n",
        "\n",
        "        self.location_patterns = [\n",
        "            r'\\bin\\s+([A-Za-z\\s,]+)(?:\\s|$)',\n",
        "            r'\\b(near|around|at|located)\\s+([A-Za-z\\s,]+)(?:\\s|$)',\n",
        "            r'\\b([A-Za-z\\s]+),?\\s+(CA|California|Los Angeles|LA|NY|New York|TX|Texas)\\b'\n",
        "        ]\n",
        "\n",
        "    def parse_business_query(self, query: str) -> ParsedBusinessQuery:\n",
        "        \"\"\"Parse natural language business search query\"\"\"\n",
        "        query_lower = query.lower().strip()\n",
        "\n",
        "        # Extract business type keywords\n",
        "        business_type_keywords = self._extract_business_type_keywords(query_lower)\n",
        "\n",
        "        # Extract location keywords\n",
        "        location_keywords = self._extract_location_keywords(query_lower)\n",
        "\n",
        "        # Extract industry keywords\n",
        "        industry_keywords = self._extract_industry_keywords(query_lower)\n",
        "\n",
        "        # Extract size indicators\n",
        "        size_indicators = self._extract_size_indicators(query_lower)\n",
        "\n",
        "        # Calculate confidence score\n",
        "        confidence_score = self._calculate_confidence_score(\n",
        "            business_type_keywords, location_keywords, industry_keywords\n",
        "        )\n",
        "\n",
        "        return ParsedBusinessQuery(\n",
        "            business_type_keywords=business_type_keywords,\n",
        "            location_keywords=location_keywords,\n",
        "            industry_keywords=industry_keywords,\n",
        "            size_indicators=size_indicators,\n",
        "            original_query=query,\n",
        "            confidence_score=confidence_score\n",
        "        )\n",
        "\n",
        "    def _extract_business_type_keywords(self, query: str) -> List[str]:\n",
        "        \"\"\"Extract business type keywords from query\"\"\"\n",
        "        keywords = []\n",
        "\n",
        "        for pattern in self.business_type_patterns:\n",
        "            matches = re.findall(pattern, query, re.IGNORECASE)\n",
        "            keywords.extend(matches)\n",
        "\n",
        "        # Extract other meaningful business words\n",
        "        words = re.findall(r'\\b[a-zA-Z]{3,}\\b', query)\n",
        "        business_words = [\n",
        "            word for word in words\n",
        "            if word not in ['looking', 'for', 'find', 'search', 'near', 'in', 'around', 'businesses']\n",
        "        ]\n",
        "\n",
        "        keywords.extend(business_words[:3])  # Limit to prevent noise\n",
        "        return list(set(keywords))\n",
        "\n",
        "    def _extract_location_keywords(self, query: str) -> List[str]:\n",
        "        \"\"\"Extract location information from query\"\"\"\n",
        "        locations = []\n",
        "\n",
        "        for pattern in self.location_patterns:\n",
        "            matches = re.findall(pattern, query, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                if isinstance(match, tuple):\n",
        "                    locations.extend([loc.strip() for loc in match if loc.strip()])\n",
        "                else:\n",
        "                    locations.append(match.strip())\n",
        "\n",
        "        # Default to Los Angeles if no location specified\n",
        "        if not locations:\n",
        "            locations = [\"Los Angeles, CA\"]\n",
        "\n",
        "        return locations\n",
        "\n",
        "    def _extract_industry_keywords(self, query: str) -> List[str]:\n",
        "        \"\"\"Extract industry-related keywords\"\"\"\n",
        "        industry_patterns = [\n",
        "            r'\\b(tech|technology|healthcare|finance|education|retail|manufacturing|startup|enterprise)\\b',\n",
        "            r'\\b(ai|artificial intelligence|machine learning|blockchain|cloud|mobile|web)\\b',\n",
        "            r'\\b(hospital|clinic|school|university|bank|restaurant|store)\\b'\n",
        "        ]\n",
        "\n",
        "        keywords = []\n",
        "        for pattern in industry_patterns:\n",
        "            matches = re.findall(pattern, query, re.IGNORECASE)\n",
        "            keywords.extend(matches)\n",
        "\n",
        "        return keywords\n",
        "\n",
        "    def _extract_size_indicators(self, query: str) -> List[str]:\n",
        "        \"\"\"Extract company size indicators\"\"\"\n",
        "        size_pattern = r'\\b(startup|small|large|enterprise|corporation|big|fortune|chain|local)\\b'\n",
        "        matches = re.findall(size_pattern, query, re.IGNORECASE)\n",
        "        return matches\n",
        "\n",
        "    def _calculate_confidence_score(self, business_keywords: List[str],\n",
        "                                  location_keywords: List[str],\n",
        "                                  industry_keywords: List[str]) -> float:\n",
        "        \"\"\"Calculate confidence score for parsed query\"\"\"\n",
        "        score = 0.0\n",
        "\n",
        "        if business_keywords:\n",
        "            score += 0.4 * min(len(business_keywords) / 3, 1.0)\n",
        "\n",
        "        if location_keywords:\n",
        "            score += 0.3\n",
        "\n",
        "        if industry_keywords:\n",
        "            score += 0.2\n",
        "\n",
        "        score += 0.1  # Base score\n",
        "\n",
        "        return round(min(score, 1.0), 2)\n",
        "\n",
        "    def search_businesses(self, parsed_query: ParsedBusinessQuery, max_results: int = 20) -> List[Dict]:\n",
        "        \"\"\"Search for businesses using Google Places API based on parsed query\"\"\"\n",
        "        all_results = []\n",
        "\n",
        "        # Build search queries\n",
        "        search_queries = self._build_search_queries(parsed_query)\n",
        "\n",
        "        for search_query in search_queries:\n",
        "            try:\n",
        "                places = self._search_places_api(search_query, parsed_query.location_keywords[0])\n",
        "                all_results.extend(places)\n",
        "            except Exception as e:\n",
        "                print(f\"Error searching for '{search_query}': {e}\")\n",
        "                continue\n",
        "\n",
        "        # Remove duplicates based on place_id\n",
        "        unique_places = {}\n",
        "        for place in all_results:\n",
        "            place_id = place.get('place_id')\n",
        "            if place_id and place_id not in unique_places:\n",
        "                unique_places[place_id] = place\n",
        "\n",
        "        return list(unique_places.values())[:max_results]\n",
        "\n",
        "    def _build_search_queries(self, parsed_query: ParsedBusinessQuery) -> List[str]:\n",
        "        \"\"\"Build search queries based on parsed information\"\"\"\n",
        "        queries = []\n",
        "\n",
        "        # Primary searches based on business type\n",
        "        for business_type in parsed_query.business_type_keywords[:3]:\n",
        "            queries.append(business_type)\n",
        "\n",
        "            # Combine with industry keywords\n",
        "            for industry in parsed_query.industry_keywords:\n",
        "                queries.append(f\"{industry} {business_type}\")\n",
        "\n",
        "        # Fallback general searches\n",
        "        if not queries:\n",
        "            queries = [\"businesses\", \"companies\", \"stores\", \"restaurants\", \"offices\"]\n",
        "\n",
        "        return queries[:5]  # Limit API calls\n",
        "\n",
        "    def _search_places_api(self, query: str, location: str) -> List[Dict]:\n",
        "        \"\"\"Search Google Places API\"\"\"\n",
        "        url = f\"{self.places_base_url}/textsearch/json\"\n",
        "\n",
        "        params = {\n",
        "            'query': f\"{query} in {location}\",\n",
        "            'key': self.google_api_key,\n",
        "            'type': 'establishment'\n",
        "        }\n",
        "\n",
        "        response = requests.get(url, params=params)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            raise Exception(f\"Google Places API error: {response.status_code}\")\n",
        "\n",
        "        data = response.json()\n",
        "\n",
        "        if data.get(\"status\") != \"OK\":\n",
        "            if data.get(\"status\") == \"ZERO_RESULTS\":\n",
        "                return []\n",
        "            raise Exception(f\"Google Places API error: {data.get('error_message', 'Unknown error')}\")\n",
        "\n",
        "        return data.get(\"results\", [])\n",
        "\n",
        "    def get_business_details(self, place_id: str) -> Dict:\n",
        "        \"\"\"Get detailed information about a specific business\"\"\"\n",
        "        details_url = f\"{self.places_base_url}/details/json\"\n",
        "\n",
        "        params = {\n",
        "            \"place_id\": place_id,\n",
        "            \"fields\": \"name,formatted_address,geometry,website,formatted_phone_number,business_status,opening_hours,rating,reviews,types,editorial_summary\",\n",
        "            \"key\": self.google_api_key\n",
        "        }\n",
        "\n",
        "        response = requests.get(details_url, params=params)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            if data.get(\"status\") == \"OK\":\n",
        "                return data.get(\"result\", {})\n",
        "\n",
        "        return {}\n",
        "\n",
        "    def analyze_business_with_ai(self, business_info: Dict) -> Dict:\n",
        "        \"\"\"Use OpenAI to analyze business and match skills\"\"\"\n",
        "        business_name = business_info.get(\"name\", \"\")\n",
        "        business_types = business_info.get(\"types\", [])\n",
        "        reviews = business_info.get(\"reviews\", [])\n",
        "        editorial_summary = business_info.get(\"editorial_summary\", {}).get(\"overview\", \"\")\n",
        "\n",
        "        # Create context from reviews and business types\n",
        "        review_text = \" \".join([review.get(\"text\", \"\")[:200] for review in reviews[:3]])\n",
        "        business_context = f\"Business types: {', '.join(business_types)}. Summary: {editorial_summary}. Recent reviews: {review_text}\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Analyze this business and identify potential job opportunities and required skills:\n",
        "\n",
        "        Business Name: {business_name}\n",
        "        Business Context: {business_context}\n",
        "\n",
        "        Available Skill Categories and Skills:\n",
        "        {json.dumps(self.skill_dictionary, indent=2)}\n",
        "\n",
        "        Please provide a JSON response with:\n",
        "        1. \"description\": A brief description of the business and what they likely do\n",
        "        2. \"potential_jobs\": List of specific job roles this business might hire for\n",
        "        3. \"required_skills\": List of skills from the skill dictionary that would be relevant\n",
        "        4. \"job_categories\": List of job categories from the skill dictionary that apply\n",
        "        5. \"hiring_likelihood\": Score from 1-10 indicating how likely they are to hire entry-level workers\n",
        "\n",
        "        Focus on entry-level positions that don't require extensive experience.\n",
        "        Only use skills that are provided in the skill dictionary.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.openai_client.chat.completions.create(\n",
        "                model=\"gpt-4\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are an expert at analyzing businesses and identifying job opportunities. Always respond with valid JSON.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                temperature=0.3\n",
        "            )\n",
        "\n",
        "            content = response.choices[0].message.content\n",
        "            # Try to extract JSON from the response\n",
        "            json_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
        "            if json_match:\n",
        "                return json.loads(json_match.group())\n",
        "            else:\n",
        "                return json.loads(content)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"AI analysis error: {e}\")\n",
        "            return {\n",
        "                \"description\": \"Analysis unavailable\",\n",
        "                \"potential_jobs\": [],\n",
        "                \"required_skills\": [],\n",
        "                \"job_categories\": [],\n",
        "                \"hiring_likelihood\": 5\n",
        "            }\n",
        "\n",
        "    def process_business(self, place_data: Dict) -> Business:\n",
        "        \"\"\"Process a single business and create Business object\"\"\"\n",
        "        # Get detailed information\n",
        "        place_id = place_data.get(\"place_id\")\n",
        "        details = self.get_business_details(place_id) if place_id else {}\n",
        "\n",
        "        # Merge basic and detailed data\n",
        "        business_info = {**place_data, **details}\n",
        "\n",
        "        # Get AI analysis\n",
        "        ai_analysis = self.analyze_business_with_ai(business_info)\n",
        "\n",
        "        # Extract location\n",
        "        geometry = business_info.get(\"geometry\", {})\n",
        "        location = geometry.get(\"location\", {})\n",
        "\n",
        "        return Business(\n",
        "            name=business_info.get(\"name\", \"\"),\n",
        "            address=business_info.get(\"formatted_address\", \"\"),\n",
        "            location={\"lat\": location.get(\"lat\"), \"lng\": location.get(\"lng\")},\n",
        "            website=business_info.get(\"website\"),\n",
        "            phone=business_info.get(\"formatted_phone_number\"),\n",
        "            description=ai_analysis.get(\"description\", \"\"),\n",
        "            place_id=place_id,\n",
        "            rating=business_info.get(\"rating\"),\n",
        "            business_type=business_info.get(\"types\", []),\n",
        "            potential_jobs=ai_analysis.get(\"potential_jobs\", []),\n",
        "            required_skills=ai_analysis.get(\"required_skills\", []),\n",
        "            job_categories=ai_analysis.get(\"job_categories\", []),\n",
        "            hours=business_info.get(\"opening_hours\", {}).get(\"weekday_text\"),\n",
        "            reviews_count=business_info.get(\"user_ratings_total\", 0),\n",
        "            hiring_likelihood=ai_analysis.get(\"hiring_likelihood\", 5)\n",
        "        )\n",
        "\n",
        "    def scrape_businesses(self, query: str, max_results: int = 20) -> List[Business]:\n",
        "        \"\"\"Main method to scrape businesses based on natural language query\"\"\"\n",
        "        print(f\"🔍 Parsing query: '{query}'\")\n",
        "\n",
        "        # Parse the query\n",
        "        parsed_query = self.parse_business_query(query)\n",
        "\n",
        "        print(f\"📊 Query Analysis:\")\n",
        "        print(f\"  • Business Types: {', '.join(parsed_query.business_type_keywords)}\")\n",
        "        print(f\"  • Location: {', '.join(parsed_query.location_keywords)}\")\n",
        "        print(f\"  • Industry: {', '.join(parsed_query.industry_keywords) or 'General'}\")\n",
        "        print(f\"  • Confidence: {parsed_query.confidence_score}/1.0\")\n",
        "        print()\n",
        "\n",
        "        # Search for businesses\n",
        "        places = self.search_businesses(parsed_query, max_results)\n",
        "\n",
        "        print(f\"🏢 Found {len(places)} businesses to analyze\")\n",
        "\n",
        "        businesses = []\n",
        "        for i, place in enumerate(places):\n",
        "            business_name = place.get('name', 'Unknown')\n",
        "            print(f\"Processing business {i+1}/{len(places)}: {business_name}\")\n",
        "\n",
        "            try:\n",
        "                business = self.process_business(place)\n",
        "                businesses.append(business)\n",
        "\n",
        "                # Rate limiting\n",
        "                time.sleep(0.5)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {business_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Sort by hiring likelihood and rating\n",
        "        businesses.sort(key=lambda b: (b.hiring_likelihood, b.rating or 0), reverse=True)\n",
        "\n",
        "        return businesses\n",
        "\n",
        "    def export_results(self, businesses: List[Business], filename: str = \"business_opportunities.csv\"):\n",
        "        \"\"\"Export results to CSV file\"\"\"\n",
        "        with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
        "            writer = csv.writer(f)\n",
        "\n",
        "            # Write header\n",
        "            writer.writerow([\n",
        "                'name', 'address', 'latitude', 'longitude', 'website', 'phone',\n",
        "                'description', 'rating', 'reviews_count', 'business_type',\n",
        "                'potential_jobs', 'required_skills', 'job_categories',\n",
        "                'hiring_likelihood', 'hours'\n",
        "            ])\n",
        "\n",
        "            # Write data\n",
        "            for business in businesses:\n",
        "                writer.writerow([\n",
        "                    business.name,\n",
        "                    business.address,\n",
        "                    business.location.get('lat', ''),\n",
        "                    business.location.get('lng', ''),\n",
        "                    business.website or '',\n",
        "                    business.phone or '',\n",
        "                    business.description,\n",
        "                    business.rating or '',\n",
        "                    business.reviews_count,\n",
        "                    '; '.join(business.business_type),\n",
        "                    '; '.join(business.potential_jobs),\n",
        "                    '; '.join(business.required_skills),\n",
        "                    '; '.join(business.job_categories),\n",
        "                    business.hiring_likelihood,\n",
        "                    '; '.join(business.hours) if business.hours else ''\n",
        "                ])\n",
        "\n",
        "        print(f\"✅ Results exported to {filename}\")\n",
        "\n",
        "    def display_results(self, businesses: List[Business]):\n",
        "        \"\"\"Display formatted results\"\"\"\n",
        "        if not businesses:\n",
        "            print(\"❌ No businesses found matching your criteria.\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\n🎯 Found {len(businesses)} Business Opportunities\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        for i, business in enumerate(businesses, 1):\n",
        "            print(f\"\\n{i}. 🏢 {business.name}\")\n",
        "            print(f\"    Address: {business.address}\")\n",
        "            print(f\"    Phone: {business.phone or 'Not available'}\")\n",
        "            print(f\"    Website: {business.website or 'Not available'}\")\n",
        "            print(f\"    Rating: {business.rating or 'N/A'}/5.0 ({business.reviews_count} reviews)\")\n",
        "            print(f\"    Description: {business.description}\")\n",
        "            print(f\"    Potential Jobs: {', '.join(business.potential_jobs) or 'General positions'}\")\n",
        "            print(f\"    Required Skills: {', '.join(business.required_skills) or 'Basic skills'}\")\n",
        "            print(f\"    Job Categories: {', '.join(business.job_categories) or 'General'}\")\n",
        "\n",
        "\n",
        "            print(\"-\" * 80)\n",
        "def main():\n",
        "    \"\"\"Main function for interactive use\"\"\"\n",
        "    print(\" Business Scraping with Skill Matching\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Get API keys\n",
        "    google_api_key = input(\"Enter your Google Places API Key: \").strip()\n",
        "    openai_api_key = input(\"Enter your OpenAI API Key: \").strip()\n",
        "\n",
        "    if not google_api_key or not openai_api_key:\n",
        "        print(\"❌ Error: Both API keys are required\")\n",
        "        return\n",
        "\n",
        "    scraper = SmartBusinessScraper(google_api_key, openai_api_key)\n",
        "\n",
        "    print(\"\\n🔍 Enter business search queries in natural language\")\n",
        "    print(\"Examples:\")\n",
        "    print(\"  • 'restaurants in downtown LA'\")\n",
        "    print(\"  • 'retail stores in Santa Monica'\")\n",
        "\n",
        "    print(\"\\nType 'quit' to exit\\n\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"🔍 Enter business search: \").strip()\n",
        "\n",
        "        if query.lower() in ['quit', 'exit', 'q']:\n",
        "            print(\"👋 Thanks for using the Smart Business Scraper!\")\n",
        "            break\n",
        "\n",
        "        if not query:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Scrape businesses\n",
        "            businesses = scraper.scrape_businesses(query, max_results=15)\n",
        "\n",
        "            # Display results\n",
        "            scraper.display_results(businesses)\n",
        "\n",
        "            # Ask if user wants to export\n",
        "            export = input(\"\\n💾 Export results to CSV? (y/n): \").strip().lower()\n",
        "            if export == 'y':\n",
        "                filename = f\"businesses_{query.replace(' ', '_').replace(',', '').lower()}.csv\"\n",
        "                scraper.export_results(businesses, filename)\n",
        "\n",
        "            print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error: {e}\")\n",
        "            print(\"Please try a different search query.\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Img30jTdx-EQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "468273bb-6af2-4ce9-dd1a-cbcb074f0aa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Smart Business Scraper\n",
            "========================================\n",
            "\n",
            "Enter your search query (e.g., 'cafes in Los Angeles', 'restaurants near me'): cafes in Los Angele\n",
            "Mounted at /content/drive\n",
            "❌ Error loading skill tags: [Errno 2] No such file or directory: '/content/drive/MyDrive/Kobeyo Business Directory/skill tags.csv'\n",
            "\n",
            "🔍 Processing: cafes in Los Angele\n",
            "🔍 Searching for: 'cafes in Los Angele'\n",
            "🏢 Found 10 businesses to analyze\n",
            "Processing business 1/10: Urth Caffe\n",
            "Processing business 2/10: Jurassic Magic\n",
            "Processing business 3/10: Blue Elephant Café\n",
            "Processing business 4/10: De La Tierra Café\n",
            "Processing business 5/10: Dalian's Café\n",
            "Processing business 6/10: Alcove\n",
            "Processing business 7/10: Alchemist Coffee Project\n",
            "Processing business 8/10: CAFE NIDO: Coffee & Books\n",
            "Processing business 9/10: Bottega Louie\n",
            "Processing business 10/10: Cafe Dulce\n",
            "\n",
            "🎯 Found 10 Businesses\n",
            "================================================================================\n",
            "\n",
            "1. 🏢 Urth Caffe\n",
            "   Address: 459 S Hewitt St, Los Angeles, CA 90013, USA\n",
            "   Website: https://www.urthcaffe.com/\n",
            "   Phone: (213) 797-4534\n",
            "   Skill IDs: BS001, BS002\n",
            "   Skill Names: Customer Service, Food and Beverage Management\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "2. 🏢 Jurassic Magic\n",
            "   Address: 1865 S Mansfield Ave, Los Angeles, CA 90019, USA\n",
            "   Website: https://www.jurassicmagic.xyz/\n",
            "   Phone: (424) 835-0598\n",
            "   Skill IDs: marketing, customer_service\n",
            "   Skill Names: Marketing, Customer Service\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "3. 🏢 Blue Elephant Café\n",
            "   Address: 2024 W Washington Blvd, Los Angeles, CA 90018, USA\n",
            "   Website: https://www.blueelephantcafe.com/\n",
            "   Phone: (323) 641-7836\n",
            "   Skill IDs: bs1, bs2, bs3\n",
            "   Skill Names: Customer Service, Food Service Management, Event Planning\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "4. 🏢 De La Tierra Café\n",
            "   Address: 1144 N Vermont Ave, Los Angeles, CA 90029, USA\n",
            "   Website: https://www.instagram.com/delatierracafeorganic?utm_source=qr\n",
            "   Phone: (323) 522-6501\n",
            "   Skill IDs: BS1, BS2\n",
            "   Skill Names: Customer Service, Social Media Marketing\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "5. 🏢 Dalian's Café\n",
            "   Address: 530 S Grand Ave, Los Angeles, CA 90071, USA\n",
            "   Website: http://dalianscafe.com/\n",
            "   Phone: (213) 559-0009\n",
            "   Skill IDs: id1, id2\n",
            "   Skill Names: name1, name2\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "6. 🏢 Alcove\n",
            "   Address: 1929 Hillhurst Ave, Los Angeles, CA 90027, USA\n",
            "   Website: http://www.alcovecafe.com/\n",
            "   Phone: (323) 644-0100\n",
            "   Skill IDs: customer_service, food_safety\n",
            "   Skill Names: Customer Service, Food Safety\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "7. 🏢 Alchemist Coffee Project\n",
            "   Address: 698 S Vermont Ave Ste 103, Los Angeles, CA 90005, USA\n",
            "   Website: http://alchemistcp.com/\n",
            "   Phone: (213) 388-8767\n",
            "   Skill IDs: BS1, BS2\n",
            "   Skill Names: Customer Service, Inventory Management\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "8. 🏢 CAFE NIDO: Coffee & Books\n",
            "   Address: 2815 W Sunset Blvd, Los Angeles, CA 90026, USA\n",
            "   Website: http://www.cafenido.com/\n",
            "   Phone: Not available\n",
            "   Skill IDs: b001, b002, b003\n",
            "   Skill Names: Customer Service, Food and Beverage Management, Retail Operations\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "9. 🏢 Bottega Louie\n",
            "   Address: 700 S Grand Ave, Los Angeles, CA 90017, USA\n",
            "   Website: http://www.bottegalouie.com/\n",
            "   Phone: (213) 802-1470\n",
            "   Skill IDs: bs1, bs2, bs3\n",
            "   Skill Names: Customer Service, Food Service, Retail Management\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "10. 🏢 Cafe Dulce\n",
            "   Address: 134 Japanese Village Plaza Mall, Los Angeles, CA 90012, USA\n",
            "   Website: https://cafedulcelittletokyo.square.site/\n",
            "   Phone: (213) 346-9910\n",
            "   Skill IDs: bs1, bs2, bs3\n",
            "   Skill Names: Customer Service, Food Safety, Menu Planning\n",
            "--------------------------------------------------------------------------------\n",
            "✅ Results saved to /content/drive/MyDrive/Kobeyo Business Directory/business_directory.csv\n",
            "✅ Completed processing\n",
            "\n",
            "🎉 Business scraping completed!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import json\n",
        "import requests\n",
        "import openai\n",
        "import time\n",
        "import csv\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Dict, Any\n",
        "from datetime import datetime\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from google.colab import drive\n",
        "import urllib.parse\n",
        "\n",
        "# API Keys - Replace with your actual keys\n",
        "GOOGLE_API_KEY = \"\"     # add key\n",
        "OPENAI_API_KEY = \"\"\n",
        "\n",
        "@dataclass\n",
        "class Business:\n",
        "    place_id: str\n",
        "    business_name: str\n",
        "    address: str\n",
        "    phone_number: Optional[str] = None\n",
        "    latitude: Optional[float] = None\n",
        "    longitude: Optional[float] = None\n",
        "    website: Optional[str] = None\n",
        "    hr_email: Optional[str] = None\n",
        "    sales_email: Optional[str] = None\n",
        "    careers_page: Optional[str] = None\n",
        "    sales_page: Optional[str] = None\n",
        "    social_media_links: Dict[str, str] = None\n",
        "    business_type: List[str] = None\n",
        "    skill_ids: List[str] = None\n",
        "    skill_names: List[str] = None\n",
        "\n",
        "class SmartBusinessScraper:\n",
        "    def __init__(self, google_api_key: str, openai_api_key: str):\n",
        "        self.google_api_key = google_api_key\n",
        "        self.openai_client = openai.OpenAI(api_key=openai_api_key)\n",
        "        self.places_base_url = \"https://maps.googleapis.com/maps/api/place\"\n",
        "        self.skill_tags_df = None\n",
        "        self.website_context = {}  # Store scraped website context\n",
        "\n",
        "        # Mount Google Drive\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "        # Load skill tags CSV\n",
        "        self.load_skill_tags()\n",
        "\n",
        "        # Setup selenium driver\n",
        "        self.setup_driver()\n",
        "\n",
        "    def setup_driver(self):\n",
        "        \"\"\"Setup Chrome driver for web scraping\"\"\"\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument(\"--headless\")\n",
        "        chrome_options.add_argument(\"--no-sandbox\")\n",
        "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "        chrome_options.add_argument(\"--disable-gpu\")\n",
        "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
        "        chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\")\n",
        "        self.driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "    def load_skill_tags(self):\n",
        "        \"\"\"Load skill tags from CSV file\"\"\"\n",
        "        try:\n",
        "            file_path = '/content/drive/MyDrive/Kobeyo Business Directory/skill tags.csv'\n",
        "            self.skill_tags_df = pd.read_csv(file_path)\n",
        "\n",
        "            # Print column names to debug\n",
        "            print(f\"CSV columns: {list(self.skill_tags_df.columns)}\")\n",
        "\n",
        "            # Map the actual column names to standardized names\n",
        "            column_mapping = {\n",
        "                'Skills Tags': 'skills_tags',\n",
        "                'Prompt Rule': 'prompt_rule',\n",
        "                'Skills IDs': 'skill_ids',\n",
        "                'Skills Names': 'skill_names'\n",
        "            }\n",
        "\n",
        "            # Rename columns to match our code expectations\n",
        "            self.skill_tags_df = self.skill_tags_df.rename(columns=column_mapping)\n",
        "\n",
        "            print(f\"✅ Loaded skill tags with {len(self.skill_tags_df)} entries\")\n",
        "            print(f\"Columns after mapping: {list(self.skill_tags_df.columns)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading skill tags: {e}\")\n",
        "            self.skill_tags_df = pd.DataFrame()\n",
        "\n",
        "    def scrape_website_content(self, url: str) -> str:\n",
        "        \"\"\"Scrape website content to extract skills-related information\"\"\"\n",
        "        if not url:\n",
        "            return \"\"\n",
        "\n",
        "        try:\n",
        "            # Normalize URL\n",
        "            if not url.startswith(('http://', 'https://')):\n",
        "                url = 'https://' + url\n",
        "\n",
        "            self.driver.get(url)\n",
        "            time.sleep(3)\n",
        "\n",
        "            # Get page content\n",
        "            page_source = self.driver.page_source\n",
        "            soup = BeautifulSoup(page_source, 'html.parser')\n",
        "\n",
        "            # Extract relevant text content\n",
        "            text_content = soup.get_text()\n",
        "\n",
        "            # Focus on skills-related content (first 2000 characters)\n",
        "            skills_context = text_content[:2000].lower()\n",
        "\n",
        "            # Store in memory\n",
        "            self.website_context[url] = skills_context\n",
        "\n",
        "            return skills_context\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping website {url}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def find_careers_and_sales_pages(self, website_url: str) -> Dict[str, str]:\n",
        "        \"\"\"Find careers and sales pages on the website\"\"\"\n",
        "        careers_page = None\n",
        "        sales_page = None\n",
        "\n",
        "        if not website_url:\n",
        "            return {\"careers_page\": careers_page, \"sales_page\": sales_page}\n",
        "\n",
        "        try:\n",
        "            if not website_url.startswith(('http://', 'https://')):\n",
        "                website_url = 'https://' + website_url\n",
        "\n",
        "            self.driver.get(website_url)\n",
        "            time.sleep(2)\n",
        "\n",
        "            # Look for careers/jobs links\n",
        "            careers_keywords = ['careers', 'jobs', 'employment', 'work-with-us', 'join-our-team']\n",
        "            for keyword in careers_keywords:\n",
        "                try:\n",
        "                    element = self.driver.find_element(By.PARTIAL_LINK_TEXT, keyword)\n",
        "                    careers_page = element.get_attribute('href')\n",
        "                    break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            # Look for sales/contact links\n",
        "            sales_keywords = ['sales', 'contact', 'get-quote', 'services', 'about']\n",
        "            for keyword in sales_keywords:\n",
        "                try:\n",
        "                    element = self.driver.find_element(By.PARTIAL_LINK_TEXT, keyword)\n",
        "                    sales_page = element.get_attribute('href')\n",
        "                    break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error finding pages on {website_url}: {e}\")\n",
        "\n",
        "        return {\"careers_page\": careers_page, \"sales_page\": sales_page}\n",
        "\n",
        "    def extract_emails_and_social_media(self, website_url: str) -> Dict[str, Any]:\n",
        "        \"\"\"Extract emails and social media links from website\"\"\"\n",
        "        hr_email = None\n",
        "        sales_email = None\n",
        "        social_media = {\"facebook\": None, \"instagram\": None, \"linkedin\": None}\n",
        "\n",
        "        if not website_url:\n",
        "            return {\"hr_email\": hr_email, \"sales_email\": sales_email, \"social_media\": social_media}\n",
        "\n",
        "        try:\n",
        "            if not website_url.startswith(('http://', 'https://')):\n",
        "                website_url = 'https://' + website_url\n",
        "\n",
        "            self.driver.get(website_url)\n",
        "            time.sleep(2)\n",
        "\n",
        "            page_source = self.driver.page_source\n",
        "\n",
        "            # Extract emails\n",
        "            email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "            emails = re.findall(email_pattern, page_source)\n",
        "\n",
        "            for email in emails:\n",
        "                email_lower = email.lower()\n",
        "                if any(keyword in email_lower for keyword in ['hr', 'jobs', 'careers', 'hiring']):\n",
        "                    hr_email = email\n",
        "                elif any(keyword in email_lower for keyword in ['sales', 'contact', 'info', 'hello']):\n",
        "                    sales_email = email\n",
        "\n",
        "            # Extract social media links\n",
        "            social_patterns = {\n",
        "                'facebook': r'https?://(?:www\\.)?facebook\\.com/[^/\\s]+',\n",
        "                'instagram': r'https?://(?:www\\.)?instagram\\.com/[^/\\s]+',\n",
        "                'linkedin': r'https?://(?:www\\.)?linkedin\\.com/[^/\\s]+'\n",
        "            }\n",
        "\n",
        "            for platform, pattern in social_patterns.items():\n",
        "                matches = re.findall(pattern, page_source)\n",
        "                if matches:\n",
        "                    social_media[platform] = matches[0]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting emails/social media from {website_url}: {e}\")\n",
        "\n",
        "        return {\"hr_email\": hr_email, \"sales_email\": sales_email, \"social_media\": social_media}\n",
        "\n",
        "    def search_businesses(self, query: str, location: str = \"Los Angeles, CA\", max_results: int = 10) -> List[Dict]:\n",
        "        \"\"\"Search for businesses using Google Places API\"\"\"\n",
        "        url = f\"{self.places_base_url}/textsearch/json\"\n",
        "        params = {\n",
        "            'query': f\"{query} in {location}\",\n",
        "            'key': self.google_api_key,\n",
        "            'type': 'establishment'\n",
        "        }\n",
        "\n",
        "        response = requests.get(url, params=params)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            raise Exception(f\"Google Places API error: {response.status_code}\")\n",
        "\n",
        "        data = response.json()\n",
        "\n",
        "        if data.get(\"status\") != \"OK\":\n",
        "            if data.get(\"status\") == \"ZERO_RESULTS\":\n",
        "                return []\n",
        "            raise Exception(f\"Google Places API error: {data.get('error_message', 'Unknown error')}\")\n",
        "\n",
        "        return data.get(\"results\", [])[:max_results]\n",
        "\n",
        "    def get_business_details(self, place_id: str) -> Dict:\n",
        "        \"\"\"Get detailed information about a specific business\"\"\"\n",
        "        details_url = f\"{self.places_base_url}/details/json\"\n",
        "        params = {\n",
        "            \"place_id\": place_id,\n",
        "            \"fields\": \"name,formatted_address,geometry,website,formatted_phone_number,business_status,opening_hours,rating,reviews,types\",\n",
        "            \"key\": self.google_api_key\n",
        "        }\n",
        "\n",
        "        response = requests.get(details_url, params=params)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            if data.get(\"status\") == \"OK\":\n",
        "                return data.get(\"result\", {})\n",
        "\n",
        "        return {}\n",
        "\n",
        "    def get_relevant_skills(self, business_types: List[str]) -> str:\n",
        "        \"\"\"Get only relevant skills from CSV based on business types\"\"\"\n",
        "        if self.skill_tags_df.empty:\n",
        "            return \"No skill tags available\"\n",
        "\n",
        "        # Filter skills based on business types\n",
        "        relevant_skills = []\n",
        "\n",
        "        # Check if the required columns exist (after mapping)\n",
        "        required_columns = ['skill_ids', 'skill_names', 'prompt_rule']\n",
        "        missing_columns = [col for col in required_columns if col not in self.skill_tags_df.columns]\n",
        "\n",
        "        if missing_columns:\n",
        "            print(f\"⚠️ Missing columns in CSV: {missing_columns}\")\n",
        "            print(f\"Available columns: {list(self.skill_tags_df.columns)}\")\n",
        "            return \"CSV format issue - missing required columns\"\n",
        "\n",
        "        try:\n",
        "            for business_type in business_types:\n",
        "                # Look for matching skills in the CSV\n",
        "                matches = self.skill_tags_df[\n",
        "                    self.skill_tags_df['prompt_rule'].str.contains(business_type, case=False, na=False)\n",
        "                ]\n",
        "\n",
        "                if not matches.empty:\n",
        "                    relevant_skills.extend(matches.to_dict('records'))\n",
        "\n",
        "            # If no specific matches, get general food/cafe related skills\n",
        "            if not relevant_skills:\n",
        "                food_keywords = ['restaurant', 'cafe', 'food', 'beverage', 'kitchen', 'service']\n",
        "                for keyword in food_keywords:\n",
        "                    matches = self.skill_tags_df[\n",
        "                        self.skill_tags_df['prompt_rule'].str.contains(keyword, case=False, na=False)\n",
        "                    ]\n",
        "                    if not matches.empty:\n",
        "                        relevant_skills.extend(matches.to_dict('records')[:3])  # Limit to 3 per keyword\n",
        "                        break\n",
        "\n",
        "            # Limit to top 5 relevant skills to reduce tokens\n",
        "            relevant_skills = relevant_skills[:5]\n",
        "\n",
        "            if not relevant_skills:\n",
        "                return \"No relevant skills found\"\n",
        "\n",
        "            # Format as concise string\n",
        "            skills_text = \"\"\n",
        "            for skill in relevant_skills:\n",
        "                skills_text += f\"ID: {skill.get('skill_ids', 'N/A')}, Name: {skill.get('skill_names', 'N/A')}, Rule: {skill.get('prompt_rule', 'N/A')[:100]}...\\n\"\n",
        "\n",
        "            return skills_text\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing skills: {e}\")\n",
        "            return \"Error processing skills from CSV\"\n",
        "\n",
        "    def tag_skills_with_ai(self, business_info: Dict, website_context: str) -> Dict[str, Any]:\n",
        "        \"\"\"Use OpenAI to tag skills based on business information and skill tags CSV\"\"\"\n",
        "        business_name = business_info.get(\"name\", \"\")\n",
        "        business_types = business_info.get(\"types\", [])\n",
        "\n",
        "        # Get only relevant skills to reduce token usage\n",
        "        relevant_skills = self.get_relevant_skills(business_types)\n",
        "\n",
        "        # Create much shorter prompt\n",
        "        prompt = f\"\"\"\n",
        "Business: {business_name}\n",
        "Types: {', '.join(business_types[:3])}\n",
        "Context: {website_context[:200]}\n",
        "\n",
        "Relevant Skills:\n",
        "{relevant_skills}\n",
        "\n",
        "Based on this business info, return relevant skill IDs and names as JSON:\n",
        "{{\n",
        "  \"skill_ids\": [\"id1\", \"id2\"],\n",
        "  \"skill_names\": [\"name1\", \"name2\"]\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.openai_client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",  # Switch to cheaper model\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"Tag business skills. Respond with valid JSON only.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                temperature=0.3,\n",
        "                max_tokens=200  # Limit response tokens\n",
        "            )\n",
        "\n",
        "            content = response.choices[0].message.content\n",
        "            json_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
        "\n",
        "            if json_match:\n",
        "                return json.loads(json_match.group())\n",
        "            else:\n",
        "                return json.loads(content)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"AI skill tagging error: {e}\")\n",
        "            return {\"skill_ids\": [], \"skill_names\": []}\n",
        "\n",
        "    def process_business(self, place_data: Dict) -> Business:\n",
        "        \"\"\"Process a single business and create Business object\"\"\"\n",
        "        place_id = place_data.get(\"place_id\")\n",
        "        details = self.get_business_details(place_id) if place_id else {}\n",
        "\n",
        "        # Merge basic and detailed data\n",
        "        business_info = {**place_data, **details}\n",
        "\n",
        "        # Get website\n",
        "        website = business_info.get(\"website\")\n",
        "\n",
        "        # Scrape website content for skills context\n",
        "        website_context = self.scrape_website_content(website) if website else \"\"\n",
        "\n",
        "        # Find careers and sales pages\n",
        "        pages_info = self.find_careers_and_sales_pages(website)\n",
        "\n",
        "        # Extract emails and social media\n",
        "        contact_info = self.extract_emails_and_social_media(website)\n",
        "\n",
        "        # Get AI skill tagging\n",
        "        skill_info = self.tag_skills_with_ai(business_info, website_context)\n",
        "\n",
        "        # Extract location\n",
        "        geometry = business_info.get(\"geometry\", {})\n",
        "        location = geometry.get(\"location\", {})\n",
        "\n",
        "        return Business(\n",
        "            place_id=place_id,\n",
        "            business_name=business_info.get(\"name\", \"\"),\n",
        "            address=business_info.get(\"formatted_address\", \"\"),\n",
        "            phone_number=business_info.get(\"formatted_phone_number\"),\n",
        "            latitude=location.get(\"lat\"),\n",
        "            longitude=location.get(\"lng\"),\n",
        "            website=website,\n",
        "            hr_email=contact_info.get(\"hr_email\"),\n",
        "            sales_email=contact_info.get(\"sales_email\"),\n",
        "            careers_page=pages_info.get(\"careers_page\"),\n",
        "            sales_page=pages_info.get(\"sales_page\"),\n",
        "            social_media_links=contact_info.get(\"social_media\"),\n",
        "            business_type=business_info.get(\"types\", []),\n",
        "            skill_ids=skill_info.get(\"skill_ids\", []),\n",
        "            skill_names=skill_info.get(\"skill_names\", [])\n",
        "        )\n",
        "\n",
        "    def scrape_businesses(self, search_query: str, max_results: int = 10) -> List[Business]:\n",
        "        \"\"\"Main method to scrape businesses\"\"\"\n",
        "        print(f\"🔍 Searching for: '{search_query}'\")\n",
        "\n",
        "        # Extract location from query if present, otherwise use default\n",
        "        location = \"Los Angeles, CA\"  # Default location\n",
        "        if \" in \" in search_query.lower():\n",
        "            parts = search_query.split(\" in \")\n",
        "            if len(parts) == 2:\n",
        "                search_query = parts[0].strip()\n",
        "                location = parts[1].strip()\n",
        "\n",
        "        # Search for businesses\n",
        "        places = self.search_businesses(search_query, location, max_results)\n",
        "        print(f\"🏢 Found {len(places)} businesses to analyze\")\n",
        "\n",
        "        businesses = []\n",
        "        for i, place in enumerate(places):\n",
        "            business_name = place.get('name', 'Unknown')\n",
        "            print(f\"Processing business {i+1}/{len(places)}: {business_name}\")\n",
        "\n",
        "            try:\n",
        "                business = self.process_business(place)\n",
        "                businesses.append(business)\n",
        "\n",
        "                # Rate limiting\n",
        "                time.sleep(1)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {business_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        return businesses\n",
        "\n",
        "    def save_to_google_drive(self, businesses: List[Business], filename: str = \"business_directory.csv\"):\n",
        "        \"\"\"Save results to Google Drive\"\"\"\n",
        "        drive_path = f\"/content/drive/MyDrive/Kobeyo Business Directory/{filename}\"\n",
        "\n",
        "        # Check if file exists to append or create new\n",
        "        file_exists = os.path.exists(drive_path)\n",
        "\n",
        "        with open(drive_path, 'a' if file_exists else 'w', newline='', encoding='utf-8') as f:\n",
        "            writer = csv.writer(f)\n",
        "\n",
        "            # Write header only if file is new\n",
        "            if not file_exists:\n",
        "                writer.writerow([\n",
        "                    'place_id', 'business_name', 'address', 'phone_number', 'latitude', 'longitude',\n",
        "                    'website', 'hr_email', 'sales_email', 'careers_page', 'sales_page',\n",
        "                    'social_media_facebook', 'social_media_instagram', 'social_media_linkedin',\n",
        "                    'business_type', 'skill_ids', 'skill_names'\n",
        "                ])\n",
        "\n",
        "            # Write data\n",
        "            for business in businesses:\n",
        "                social_media = business.social_media_links or {}\n",
        "                writer.writerow([\n",
        "                    business.place_id,\n",
        "                    business.business_name,\n",
        "                    business.address,\n",
        "                    business.phone_number or '',\n",
        "                    business.latitude or '',\n",
        "                    business.longitude or '',\n",
        "                    business.website or '',\n",
        "                    business.hr_email or '',\n",
        "                    business.sales_email or '',\n",
        "                    business.careers_page or '',\n",
        "                    business.sales_page or '',\n",
        "                    social_media.get('facebook', ''),\n",
        "                    social_media.get('instagram', ''),\n",
        "                    social_media.get('linkedin', ''),\n",
        "                    '; '.join(business.business_type) if business.business_type else '',\n",
        "                    '; '.join(business.skill_ids) if business.skill_ids else '',\n",
        "                    '; '.join(business.skill_names) if business.skill_names else ''\n",
        "                ])\n",
        "\n",
        "        print(f\"✅ Results saved to {drive_path}\")\n",
        "\n",
        "    def display_results(self, businesses: List[Business]):\n",
        "        \"\"\"Display formatted results - simplified version\"\"\"\n",
        "        if not businesses:\n",
        "            print(\"❌ No businesses found matching your criteria.\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\n🎯 Found {len(businesses)} Businesses\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        for i, business in enumerate(businesses, 1):\n",
        "            print(f\"\\n{i}. 🏢 {business.business_name}\")\n",
        "            print(f\"   Address: {business.address}\")\n",
        "            print(f\"   Website: {business.website or 'Not available'}\")\n",
        "            print(f\"   Phone: {business.phone_number or 'Not available'}\")\n",
        "            print(f\"   Skill IDs: {', '.join(business.skill_ids) if business.skill_ids else 'None'}\")\n",
        "            print(f\"   Skill Names: {', '.join(business.skill_names) if business.skill_names else 'None'}\")\n",
        "            print(\"-\" * 80)\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"Clean up selenium driver\"\"\"\n",
        "        if hasattr(self, 'driver'):\n",
        "            self.driver.quit()\n",
        "\n",
        "def get_user_input():\n",
        "    \"\"\"Get user input for search query\"\"\"\n",
        "    print(\"🔧 Smart Business Scraper\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    search_query = input(\"\\nEnter your search query (e.g., 'cafes in Los Angeles', 'restaurants near me'): \").strip()\n",
        "\n",
        "    if not search_query:\n",
        "        search_query = \"cafes in Los Angeles\"\n",
        "        print(f\"Using default: {search_query}\")\n",
        "\n",
        "    return search_query\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the scraper\"\"\"\n",
        "    # Get user input\n",
        "    search_query = get_user_input()\n",
        "\n",
        "    # Initialize scraper\n",
        "    scraper = SmartBusinessScraper(GOOGLE_API_KEY, OPENAI_API_KEY)\n",
        "\n",
        "    try:\n",
        "        print(f\"\\n🔍 Processing: {search_query}\")\n",
        "\n",
        "        # Limit to 10 businesses to conserve OpenAI tokens\n",
        "        businesses = scraper.scrape_businesses(search_query, max_results=10)\n",
        "\n",
        "        # Display results\n",
        "        scraper.display_results(businesses)\n",
        "\n",
        "        # Save to Google Drive\n",
        "        scraper.save_to_google_drive(businesses)\n",
        "\n",
        "        print(f\"✅ Completed processing\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing '{search_query}': {e}\")\n",
        "\n",
        "    print(\"\\n🎉 Business scraping completed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}