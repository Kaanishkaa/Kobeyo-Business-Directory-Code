{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install selenium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RU12zC28xzQ",
        "outputId": "8f3a82de-08eb-408b-90c8-ebc659f143c9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.33.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: urllib3~=2.4.0 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (2.4.0)\n",
            "Collecting trio~=0.30.0 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.4.26 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.4.26)\n",
            "Collecting typing_extensions~=4.13.2 (from selenium)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.30.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.33.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, typing_extensions, outcome, trio, trio-websocket, selenium\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.14.0\n",
            "    Uninstalling typing_extensions-4.14.0:\n",
            "      Successfully uninstalled typing_extensions-4.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed outcome-1.3.0.post0 selenium-4.33.0 trio-0.30.0 trio-websocket-0.12.2 typing_extensions-4.13.2 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "import json\n",
        "import pandas as pd\n",
        "import requests\n",
        "import threading\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from collections import deque\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "# Constants\n",
        "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY', \"AIzaSyCcSSpO0cTgBz0J9IX6QSAMMJ0mgJkcCto\")\n",
        "TEXT_SEARCH_URL = \"https://maps.googleapis.com/maps/api/place/textsearch/json\"\n",
        "PLACE_DETAILS_URL = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
        "SEARCH_TAGS = [\"restaurant\"]\n",
        "SEARCH_LOCATIONS = [\"Los Angeles\"]\n",
        "MAX_PAGES_PER_SITE = 15\n",
        "CAREERS_MAX_PAGES = 5\n",
        "THREAD_POOL_WORKERS = 5\n",
        "USER_AGENTS = [\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "]\n",
        "\n",
        "# Third-party job sites to exclude\n",
        "THIRD_PARTY_JOB_SITES = [\n",
        "    \"indeed.com\", \"ziprecruiter.com\", \"linkedin.com/jobs\", \"monster.com\",\n",
        "    \"glassdoor.com\", \"careerbuilder.com\", \"simplyhired.com\", \"dice.com\",\n",
        "    \"flexjobs.com\", \"upwork.com\", \"freelancer.com\", \"fiverr.com\",\n",
        "    \"snagajob.com\", \"workday.com\", \"bamboohr.com\", \"greenhouse.io\"\n",
        "]\n",
        "\n",
        "# URL Shortening Configuration\n",
        "MAX_URL_LENGTH = 80\n",
        "ELLIPSIS = \"...\"\n",
        "\n",
        "# Setup headless Chrome for Selenium\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-gpu\")\n",
        "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# ================= URL PROCESSING FUNCTIONS =================\n",
        "\n",
        "def shorten_url(url, max_length=MAX_URL_LENGTH):\n",
        "    \"\"\"Shorten a URL if it exceeds the maximum length\"\"\"\n",
        "    if len(url) <= max_length:\n",
        "        return url\n",
        "\n",
        "    try:\n",
        "        parsed = urlparse(url)\n",
        "        domain = parsed.netloc\n",
        "        scheme = parsed.scheme\n",
        "        base_url = f\"{scheme}://{domain}\"\n",
        "\n",
        "        if len(base_url) >= max_length - len(ELLIPSIS):\n",
        "            return base_url[:max_length - len(ELLIPSIS)] + ELLIPSIS\n",
        "\n",
        "        remaining_space = max_length - len(base_url) - len(ELLIPSIS)\n",
        "        if remaining_space > 0 and parsed.path:\n",
        "            path_part = parsed.path[:remaining_space]\n",
        "            return f\"{base_url}{path_part}{ELLIPSIS}\"\n",
        "        else:\n",
        "            return base_url\n",
        "\n",
        "    except Exception:\n",
        "        return url[:max_length - len(ELLIPSIS)] + ELLIPSIS\n",
        "\n",
        "def check_url_status(url, timeout=5):\n",
        "    \"\"\"Check if a URL is working/accessible (returns True if working)\"\"\"\n",
        "    if not url or url == \"None found\":\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        headers = {\"User-Agent\": random.choice(USER_AGENTS)}\n",
        "        response = requests.head(url, headers=headers, timeout=timeout, allow_redirects=True)\n",
        "        return response.status_code < 400\n",
        "    except:\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)\n",
        "            return response.status_code < 400\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "def process_urls(url_string):\n",
        "    \"\"\"Process and shorten URLs, filter out broken ones\"\"\"\n",
        "    if not url_string or url_string == \"None found\":\n",
        "        return url_string\n",
        "\n",
        "    urls = [url.strip() for url in url_string.split(\";\")]\n",
        "    working_urls = []\n",
        "\n",
        "    for url in urls:\n",
        "        if url and check_url_status(url):\n",
        "            working_urls.append(shorten_url(url))\n",
        "\n",
        "    return \"; \".join(working_urls) if working_urls else \"None found\"\n",
        "\n",
        "def is_third_party_job_site(url):\n",
        "    \"\"\"Check if URL is from a third-party job site\"\"\"\n",
        "    return any(site in url.lower() for site in THIRD_PARTY_JOB_SITES)\n",
        "\n",
        "# ================= GOOGLE MAPS API FUNCTIONS =================\n",
        "\n",
        "def search_places(query, location):\n",
        "    \"\"\"Search for places using Google Maps API\"\"\"\n",
        "    params = {\"query\": f\"{query} in {location}\", \"key\": GOOGLE_API_KEY}\n",
        "    response = requests.get(TEXT_SEARCH_URL, params=params)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Error: {response.status_code}\")\n",
        "        return [], []\n",
        "    results = response.json().get(\"results\", [])\n",
        "    return [r.get(\"place_id\") for r in results], [r.get(\"name\") for r in results]\n",
        "\n",
        "def get_place_details(place_id):\n",
        "    \"\"\"Get detailed information about a place\"\"\"\n",
        "    params = {\n",
        "        \"place_id\": place_id,\n",
        "        \"fields\": \"name,formatted_address,website,formatted_phone_number\",\n",
        "        \"key\": GOOGLE_API_KEY,\n",
        "    }\n",
        "    response = requests.get(PLACE_DETAILS_URL, params=params)\n",
        "    if response.status_code != 200:\n",
        "        return None\n",
        "    return response.json().get(\"result\", {})\n",
        "\n",
        "# ================= EMAIL EXTRACTION FUNCTIONS =================\n",
        "\n",
        "def extract_emails(text, soup=None):\n",
        "    \"\"\"Extract all email addresses from text\"\"\"\n",
        "    pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
        "    emails = set(re.findall(pattern, text))\n",
        "\n",
        "    if soup:\n",
        "        for a in soup.find_all(\"a\", href=True):\n",
        "            href = a['href'].lower()\n",
        "            if href.startswith(\"mailto:\"):\n",
        "                email = href[7:].split(\"?\")[0].strip()\n",
        "                if re.match(pattern, email):\n",
        "                    emails.add(email)\n",
        "            visible = a.get_text(strip=True)\n",
        "            if re.match(pattern, visible):\n",
        "                emails.add(visible)\n",
        "    return list(emails)\n",
        "\n",
        "def extract_hr_emails(text):\n",
        "    \"\"\"Extract HR-related email addresses\"\"\"\n",
        "    pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
        "    emails = re.findall(pattern, text)\n",
        "\n",
        "    hr_emails = []\n",
        "    for email in emails:\n",
        "        email_lower = email.lower()\n",
        "        if any(email_lower.startswith(prefix) for prefix in [\"hr@\", \"hiring@\", \"recruiting@\", \"talent@\", \"jobs@\", \"careers@\"]):\n",
        "            hr_emails.append(email)\n",
        "\n",
        "    return list(set(hr_emails))\n",
        "\n",
        "def extract_general_emails(text):\n",
        "    \"\"\"Extract general contact emails\"\"\"\n",
        "    pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
        "    emails = re.findall(pattern, text)\n",
        "\n",
        "    general_emails = []\n",
        "    for email in emails:\n",
        "        email_lower = email.lower()\n",
        "        if any(email_lower.startswith(prefix) for prefix in [\"info@\", \"contact@\", \"hello@\", \"support@\", \"admin@\"]):\n",
        "            general_emails.append(email)\n",
        "        elif email_lower.endswith(\"@gmail.com\") or email_lower.endswith(\"@yahoo.com\"):\n",
        "            general_emails.append(email)\n",
        "\n",
        "    return list(set(general_emails))\n",
        "\n",
        "def extract_sales_emails(text):\n",
        "    \"\"\"Extract sales-related email addresses\"\"\"\n",
        "    pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
        "    emails = re.findall(pattern, text)\n",
        "\n",
        "    sales_emails = []\n",
        "    for email in emails:\n",
        "        email_lower = email.lower()\n",
        "        if any(email_lower.startswith(prefix) for prefix in [\"sales@\", \"business@\", \"partnerships@\", \"marketing@\"]):\n",
        "            sales_emails.append(email)\n",
        "\n",
        "    return list(set(sales_emails))\n",
        "\n",
        "# ================= PAGE DETECTION FUNCTIONS =================\n",
        "\n",
        "def detect_careers_page(url, text, soup):\n",
        "    \"\"\"Detect if this is a careers/jobs page and if it's internal\"\"\"\n",
        "    if is_third_party_job_site(url):\n",
        "        return False, \"\"\n",
        "\n",
        "    careers_indicators = [\n",
        "        \"career\", \"careers\", \"job\", \"jobs\", \"employment\", \"hiring\", \"positions\",\n",
        "        \"join our team\", \"work with us\", \"apply now\", \"open positions\"\n",
        "    ]\n",
        "\n",
        "    url_lower = url.lower()\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    # Check URL path\n",
        "    url_has_careers = any(indicator in url_lower for indicator in careers_indicators)\n",
        "\n",
        "    # Check page content\n",
        "    content_has_careers = any(indicator in text_lower for indicator in careers_indicators)\n",
        "\n",
        "    # Look for application forms\n",
        "    has_application_form = False\n",
        "    if soup:\n",
        "        forms = soup.find_all(\"form\")\n",
        "        for form in forms:\n",
        "            form_text = form.get_text().lower()\n",
        "            if any(word in form_text for word in [\"apply\", \"application\", \"resume\", \"cv\", \"position\"]):\n",
        "                has_application_form = True\n",
        "                break\n",
        "\n",
        "    if url_has_careers or (content_has_careers and has_application_form):\n",
        "        return True, url\n",
        "\n",
        "    return False, \"\"\n",
        "\n",
        "def detect_products_services_page(url, text, soup):\n",
        "    \"\"\"Detect pages showing products, services, or industries served\"\"\"\n",
        "    if not text:\n",
        "        return False, \"\"\n",
        "\n",
        "    # Keywords that indicate products/services pages\n",
        "    product_service_indicators = [\n",
        "        \"products\", \"services\", \"solutions\", \"offerings\", \"what we do\",\n",
        "        \"industries\", \"sectors\", \"specialties\", \"capabilities\", \"expertise\",\n",
        "        \"portfolio\", \"catalog\", \"menu\", \"pricing\", \"packages\"\n",
        "    ]\n",
        "\n",
        "    url_lower = url.lower()\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    # Check URL path\n",
        "    url_indicates_products = any(indicator in url_lower for indicator in product_service_indicators)\n",
        "\n",
        "    # Check if content has substantial product/service information\n",
        "    content_score = sum(1 for indicator in product_service_indicators if indicator in text_lower)\n",
        "\n",
        "    # Look for structured content (lists, grids, etc.)\n",
        "    has_structured_content = False\n",
        "    if soup:\n",
        "        # Look for lists or structured content\n",
        "        lists = soup.find_all(['ul', 'ol', 'div'])\n",
        "        for element in lists:\n",
        "            element_text = element.get_text().lower()\n",
        "            if any(indicator in element_text for indicator in product_service_indicators):\n",
        "                has_structured_content = True\n",
        "                break\n",
        "\n",
        "    # Determine if this is likely a products/services page\n",
        "    if url_indicates_products or (content_score >= 2 and has_structured_content):\n",
        "        return True, url\n",
        "\n",
        "    return False, \"\"\n",
        "\n",
        "# ================= WEB CRAWLING FUNCTIONS =================\n",
        "\n",
        "def crawl_site_comprehensive(base_url, max_pages=15):\n",
        "    \"\"\"Comprehensive website crawling\"\"\"\n",
        "    visited = set()\n",
        "    queue = deque([base_url])\n",
        "\n",
        "    # Email collections\n",
        "    hr_emails = set()\n",
        "    general_emails = set()\n",
        "    sales_emails = set()\n",
        "\n",
        "    # Page collections\n",
        "    careers_pages = set()\n",
        "    products_services_pages = set()\n",
        "\n",
        "    domain = urlparse(base_url).netloc.replace(\"www.\", \"\")\n",
        "    pages_crawled = 0\n",
        "\n",
        "    while queue and pages_crawled < max_pages:\n",
        "        url = queue.popleft()\n",
        "        if url in visited:\n",
        "            continue\n",
        "        visited.add(url)\n",
        "\n",
        "        try:\n",
        "            headers = {\"User-Agent\": random.choice(USER_AGENTS)}\n",
        "            resp = requests.get(url, headers=headers, timeout=10)\n",
        "            resp.raise_for_status()\n",
        "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "            text = soup.get_text(\" \", strip=True)\n",
        "\n",
        "            # Extract emails by type\n",
        "            hr_emails.update(extract_hr_emails(text))\n",
        "            general_emails.update(extract_general_emails(text))\n",
        "            sales_emails.update(extract_sales_emails(text))\n",
        "\n",
        "            # Detect page types\n",
        "            is_careers, careers_url = detect_careers_page(url, text, soup)\n",
        "            if is_careers:\n",
        "                careers_pages.add(careers_url)\n",
        "\n",
        "            is_products, products_url = detect_products_services_page(url, text, soup)\n",
        "            if is_products:\n",
        "                products_services_pages.add(products_url)\n",
        "\n",
        "            # Discover more links\n",
        "            for a in soup.find_all(\"a\", href=True):\n",
        "                new_url = urljoin(url, a['href'])\n",
        "                parsed_url = urlparse(new_url)\n",
        "\n",
        "                if domain in parsed_url.netloc and new_url not in visited:\n",
        "                    # Prioritize important pages\n",
        "                    link_text = a.get_text(strip=True).lower()\n",
        "                    href_lower = a['href'].lower()\n",
        "\n",
        "                    priority_keywords = ['career', 'job', 'product', 'service', 'about', 'contact']\n",
        "                    if any(keyword in link_text or keyword in href_lower for keyword in priority_keywords):\n",
        "                        queue.appendleft(new_url)\n",
        "                    else:\n",
        "                        queue.append(new_url)\n",
        "\n",
        "            time.sleep(random.uniform(0.5, 1.0))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error crawling {url}: {e}\")\n",
        "\n",
        "        pages_crawled += 1\n",
        "\n",
        "    return {\n",
        "        \"hr_emails\": list(hr_emails),\n",
        "        \"general_emails\": list(general_emails),\n",
        "        \"sales_emails\": list(sales_emails),\n",
        "        \"careers_pages\": list(careers_pages),\n",
        "        \"products_services_pages\": list(products_services_pages),\n",
        "        \"pages_crawled\": pages_crawled\n",
        "    }\n",
        "\n",
        "# ================= BUSINESS PROCESSING =================\n",
        "\n",
        "def process_business_comprehensive(place_id, name):\n",
        "    \"\"\"Process a business with comprehensive data extraction\"\"\"\n",
        "    print(f\"Processing business: {name}\")\n",
        "\n",
        "    # Get Google Places details\n",
        "    details = get_place_details(place_id)\n",
        "    if not details:\n",
        "        return None\n",
        "\n",
        "    website = details.get(\"website\", \"\")\n",
        "    address = details.get(\"formatted_address\", \"\")\n",
        "\n",
        "    # Initialize result\n",
        "    result = {\n",
        "        \"Company Name\": name,\n",
        "        \"Company Address\": address,\n",
        "        \"Company HR Email\": \"\",\n",
        "        \"General Email\": \"\",\n",
        "        \"Company Sales Email\": \"\",\n",
        "        \"Company Careers Page\": \"\",\n",
        "        \"Company Products/Services Page\": \"\"\n",
        "    }\n",
        "\n",
        "    # Crawl website if available\n",
        "    if website:\n",
        "        try:\n",
        "            crawl_results = crawl_site_comprehensive(website, MAX_PAGES_PER_SITE)\n",
        "\n",
        "            # Set emails\n",
        "            if crawl_results[\"hr_emails\"]:\n",
        "                result[\"Company HR Email\"] = \"; \".join(crawl_results[\"hr_emails\"])\n",
        "\n",
        "            if crawl_results[\"general_emails\"]:\n",
        "                result[\"General Email\"] = \"; \".join(crawl_results[\"general_emails\"])\n",
        "\n",
        "            if crawl_results[\"sales_emails\"]:\n",
        "                result[\"Company Sales Email\"] = \"; \".join(crawl_results[\"sales_emails\"])\n",
        "\n",
        "            # Set pages\n",
        "            if crawl_results[\"careers_pages\"]:\n",
        "                result[\"Company Careers Page\"] = \"; \".join(crawl_results[\"careers_pages\"])\n",
        "\n",
        "            if crawl_results[\"products_services_pages\"]:\n",
        "                result[\"Company Products/Services Page\"] = \"; \".join(crawl_results[\"products_services_pages\"])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to crawl {website}: {e}\")\n",
        "\n",
        "    return result\n",
        "\n",
        "# ================= MAIN FUNCTION =================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    print(\"🚀 Starting comprehensive business data scraper\")\n",
        "\n",
        "    lock = threading.Lock()\n",
        "    checkpoint_file = \"scrapped_data.csv\"\n",
        "\n",
        "    # Define CSV columns\n",
        "    csv_columns = [\n",
        "        \"Company Name\",\n",
        "        \"Company Address\",\n",
        "        \"Company HR Email\",\n",
        "        \"General Email\",\n",
        "        \"Company Sales Email\",\n",
        "        \"Company Careers Page\",\n",
        "        \"Company Products/Services Page\"\n",
        "    ]\n",
        "\n",
        "    # Load existing data\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        existing_df = pd.read_csv(checkpoint_file)\n",
        "        print(f\"📄 Loaded {len(existing_df)} existing records\")\n",
        "    else:\n",
        "        existing_df = pd.DataFrame(columns=csv_columns)\n",
        "\n",
        "    existing_data = {row['Company Name']: row for _, row in existing_df.iterrows()} if not existing_df.empty else {}\n",
        "\n",
        "    # Process each location and search tag\n",
        "    for location in SEARCH_LOCATIONS:\n",
        "        for tag in SEARCH_TAGS:\n",
        "            print(f\"\\n🔍 Searching for {tag} in {location}\")\n",
        "            place_ids, names = search_places(tag, location)\n",
        "            print(f\"📍 Found {len(place_ids)} businesses\")\n",
        "\n",
        "            # Process businesses with threading\n",
        "            with ThreadPoolExecutor(max_workers=THREAD_POOL_WORKERS) as executor:\n",
        "                futures = {\n",
        "                    executor.submit(process_business_comprehensive, pid, name): name\n",
        "                    for pid, name in zip(place_ids, names)\n",
        "                    if name not in existing_data\n",
        "                }\n",
        "\n",
        "                for future in as_completed(futures):\n",
        "                    result = future.result()\n",
        "                    if result:\n",
        "                        with lock:\n",
        "                            # Process URLs (remove broken ones, shorten working ones)\n",
        "                            csv_result = result.copy()\n",
        "\n",
        "                            # Process careers page URLs\n",
        "                            if result[\"Company Careers Page\"]:\n",
        "                                csv_result[\"Company Careers Page\"] = process_urls(result[\"Company Careers Page\"])\n",
        "                            else:\n",
        "                                csv_result[\"Company Careers Page\"] = \"None found\"\n",
        "\n",
        "                            # Process products/services page URLs\n",
        "                            if result[\"Company Products/Services Page\"]:\n",
        "                                csv_result[\"Company Products/Services Page\"] = process_urls(result[\"Company Products/Services Page\"])\n",
        "                            else:\n",
        "                                csv_result[\"Company Products/Services Page\"] = \"None found\"\n",
        "\n",
        "                            # Set default values for empty fields\n",
        "                            for field in [\"Company HR Email\", \"General Email\", \"Company Sales Email\"]:\n",
        "                                if not csv_result[field]:\n",
        "                                    csv_result[field] = \"None found\"\n",
        "\n",
        "                            # Add to DataFrame and save\n",
        "                            existing_df = pd.concat([existing_df, pd.DataFrame([csv_result])], ignore_index=True)\n",
        "                            existing_df.to_csv(checkpoint_file, index=False)\n",
        "                            print(f\"✅ Saved: {result['Company Name']}\")\n",
        "\n",
        "    # Generate final report\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"📊 FINAL METRICS REPORT\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    total = len(existing_df)\n",
        "    with_hr = len(existing_df[existing_df['Company HR Email'] != 'None found'])\n",
        "    with_general = len(existing_df[existing_df['General Email'] != 'None found'])\n",
        "    with_sales = len(existing_df[existing_df['Company Sales Email'] != 'None found'])\n",
        "    with_careers = len(existing_df[existing_df['Company Careers Page'] != 'None found'])\n",
        "    with_products = len(existing_df[existing_df['Company Products/Services Page'] != 'None found'])\n",
        "\n",
        "    print(f\"📈 Total businesses: {total}\")\n",
        "    print(f\"👔 With HR emails: {with_hr} ({with_hr/total*100:.1f}%)\")\n",
        "    print(f\"📧 With general emails: {with_general} ({with_general/total*100:.1f}%)\")\n",
        "    print(f\"💼 With sales emails: {with_sales} ({with_sales/total*100:.1f}%)\")\n",
        "    print(f\"🎯 With careers pages: {with_careers} ({with_careers/total*100:.1f}%)\")\n",
        "    print(f\"📦 With products/services pages: {with_products} ({with_products/total*100:.1f}%)\")\n",
        "\n",
        "    print(f\"\\n🎉 Scraping completed! Data saved to: {checkpoint_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        main()\n",
        "    finally:\n",
        "        try:\n",
        "            driver.quit()\n",
        "            print(\"🔧 Selenium driver closed\")\n",
        "        except:\n",
        "            pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5l2rcIGRlhK",
        "outputId": "32541018-c55d-4566-9764-20cee628d367"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting comprehensive business data scraper\n",
            "\n",
            "🔍 Searching for restaurant in Los Angeles\n",
            "📍 Found 20 businesses\n",
            "Processing business: Girl & the Goat Los Angeles\n",
            "Processing business: Perch\n",
            "Processing business: Bottega Louie\n",
            "Processing business: The Little Door\n",
            "Processing business: Water Grill\n",
            "Error crawling http://www.bottegalouie.com/pages/guestservices@bottegalouie.com: 404 Client Error: Not Found for url: https://www.bottegalouie.com/pages/guestservices@bottegalouie.com\n",
            "Error crawling https://www.bottegalouie.com/pages/guestservices@bottegalouie.com: 404 Client Error: Not Found for url: https://www.bottegalouie.com/pages/guestservices@bottegalouie.com\n",
            "Processing business: Bestia\n",
            "Processing business: 71Above\n",
            "✅ Saved: Bottega Louie\n",
            "Processing business: Chi Spacca\n",
            "✅ Saved: Water Grill\n",
            "✅ Saved: The Little Door\n",
            "Error crawling https://www.71above.com/: 403 Client Error: Forbidden for url: https://www.71above.com/\n",
            "Processing business: République Café Bakery & République Restaurant\n",
            "✅ Saved: 71Above\n",
            "Processing business: JOEY DTLA\n",
            "Processing business: Redbird\n",
            "✅ Saved: Perch\n",
            "✅ Saved: Girl & the Goat Los Angeles\n",
            "Processing business: Fleming’s Prime Steakhouse & Wine Bar\n",
            "✅ Saved: Bestia\n",
            "Processing business: Providence\n",
            "✅ Saved: République Café Bakery & République Restaurant\n",
            "Error crawling https://providencela.com/: 403 Client Error: Forbidden for url: https://providencela.com/\n",
            "Processing business: Majordomo\n",
            "✅ Saved: Providence\n",
            "Processing business: Guelaguetza Restaurant\n",
            "Processing business: Le Grand Restaurant\n",
            "Processing business: Cabra Los Angeles\n",
            "✅ Saved: Chi Spacca\n",
            "Error crawling https://www.legrand-restaurant.com/s/DC9FDA07-0C92-4719-BDC2-C6CA498D4E9A.jpeg: 404 Client Error: Not Found for url: https://www.legrand-restaurant.com/s/DC9FDA07-0C92-4719-BDC2-C6CA498D4E9A.jpeg\n",
            "✅ Saved: JOEY DTLA\n",
            "Processing business: BROKEN MOUTH | Lee's Homestyle\n",
            "Processing business: Home Restaurant\n",
            "Error crawling https://www.homerestaurantla.com/: 403 Client Error: Forbidden for url: https://www.homerestaurantla.com/\n",
            "Processing business: Moonlark's Dinette\n",
            "✅ Saved: Redbird\n",
            "✅ Saved: Fleming’s Prime Steakhouse & Wine Bar\n",
            "✅ Saved: Le Grand Restaurant\n",
            "✅ Saved: Home Restaurant\n",
            "✅ Saved: BROKEN MOUTH | Lee's Homestyle\n",
            "✅ Saved: Majordomo\n",
            "Error crawling https://www.moonlarksla.com/new-home#about: 404 Client Error: Not Found for url: https://www.moonlarksla.com/new-home#about\n",
            "✅ Saved: Guelaguetza Restaurant\n",
            "✅ Saved: Cabra Los Angeles\n",
            "✅ Saved: Moonlark's Dinette\n",
            "\n",
            "==================================================\n",
            "📊 FINAL METRICS REPORT\n",
            "==================================================\n",
            "📈 Total businesses: 20\n",
            "👔 With HR emails: 1 (5.0%)\n",
            "📧 With general emails: 8 (40.0%)\n",
            "💼 With sales emails: 3 (15.0%)\n",
            "🎯 With careers pages: 4 (20.0%)\n",
            "📦 With products/services pages: 16 (80.0%)\n",
            "\n",
            "🎉 Scraping completed! Data saved to: scrapped_data.csv\n",
            "🔧 Selenium driver closed\n"
          ]
        }
      ]
    }
  ]
}