{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install selenium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RU12zC28xzQ",
        "outputId": "dbdc1811-c7f8-458f-cd24-e06ae76f9762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.33.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: urllib3~=2.4.0 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (2.4.0)\n",
            "Collecting trio~=0.30.0 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.4.26 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.4.26)\n",
            "Collecting typing_extensions~=4.13.2 (from selenium)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.30.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.33.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, typing_extensions, outcome, trio, trio-websocket, selenium\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.14.0\n",
            "    Uninstalling typing_extensions-4.14.0:\n",
            "      Successfully uninstalled typing_extensions-4.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed outcome-1.3.0.post0 selenium-4.33.0 trio-0.30.0 trio-websocket-0.12.2 typing_extensions-4.13.2 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSTA6X9n8uEC",
        "outputId": "9efbcd62-8be3-4344-e6e0-8f7395b1762a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting comprehensive restaurant & careers scraper\n",
            "\n",
            "🔍 Searching for restaurant in Los Angeles\n",
            "📍 Found 20 businesses\n",
            "Processing business: Girl & the Goat Los Angeles\n",
            "Processing business: Perch\n",
            "Processing business: Bottega Louie\n",
            "Processing business: The Little Door\n",
            "Processing business: Water Grill\n",
            "DEBUG for https://www.watergrill.com/la: {'pages_fetched': 5, 'social_found': 4, 'emails_found': 7, 'job_content_found': 5, 'selenium_used': False}\n",
            "Processing business: Bestia\n",
            "✅ Saved: Water Grill\n",
            "DEBUG for http://girlandthegoat.com/: {'pages_fetched': 5, 'social_found': 4, 'emails_found': 4, 'job_content_found': 2, 'selenium_used': False}\n",
            "Processing business: Chi Spacca\n",
            "✅ Saved: Girl & the Goat Los Angeles\n",
            "DEBUG for http://www.chispacca.com/: {'pages_fetched': 5, 'social_found': 3, 'emails_found': 2, 'job_content_found': 1, 'selenium_used': False}\n",
            "Processing business: 71Above\n",
            "✅ Saved: Chi Spacca\n",
            "Error crawling https://www.71above.com/: 403 Client Error: Forbidden for url: https://www.71above.com/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: localhost. Connection pool size: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG for https://thelittledoor.com/: {'pages_fetched': 5, 'social_found': 2, 'emails_found': 1, 'job_content_found': 0, 'selenium_used': True}\n",
            "Processing business: République Café Bakery & République Restaurant\n",
            "✅ Saved: The Little Door\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: localhost. Connection pool size: 1\n",
            "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: localhost. Connection pool size: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG for http://www.bottegalouie.com/: {'pages_fetched': 5, 'social_found': 3, 'emails_found': 0, 'job_content_found': 5, 'selenium_used': True}\n",
            "Processing business: Fleming’s Prime Steakhouse & Wine Bar\n",
            "✅ Saved: Bottega Louie\n",
            "DEBUG for http://perchla.com/: {'pages_fetched': 5, 'social_found': 3, 'emails_found': 0, 'job_content_found': 0, 'selenium_used': True}\n",
            "Processing business: Redbird\n",
            "✅ Saved: Perch\n",
            "DEBUG for https://www.71above.com/: {'pages_fetched': 0, 'social_found': 3, 'emails_found': 0, 'job_content_found': 0, 'selenium_used': True}\n",
            "Processing business: JOEY DTLA\n",
            "✅ Saved: 71Above\n",
            "DEBUG for http://www.bestiala.com/: {'pages_fetched': 5, 'social_found': 3, 'emails_found': 1, 'job_content_found': 0, 'selenium_used': True}\n",
            "Processing business: Providence\n",
            "✅ Saved: Bestia\n",
            "Error crawling https://providencela.com/: 403 Client Error: Forbidden for url: https://providencela.com/\n",
            "DEBUG for https://joeyrestaurants.com/location/joey-dtla: {'pages_fetched': 5, 'social_found': 2, 'emails_found': 1, 'job_content_found': 5, 'selenium_used': False}\n",
            "Processing business: Majordomo\n",
            "✅ Saved: JOEY DTLA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: localhost. Connection pool size: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG for https://providencela.com/: {'pages_fetched': 0, 'social_found': 1, 'emails_found': 0, 'job_content_found': 0, 'selenium_used': True}\n",
            "Processing business: Maccheroni Republic\n",
            "✅ Saved: Providence\n",
            "DEBUG for http://www.redbird.la/: {'pages_fetched': 5, 'social_found': 2, 'emails_found': 1, 'job_content_found': 5, 'selenium_used': False}\n",
            "Processing business: Guelaguetza Restaurant\n",
            "✅ Saved: Redbird\n",
            "DEBUG for http://www.republiquela.com/: {'pages_fetched': 5, 'social_found': 2, 'emails_found': 0, 'job_content_found': 1, 'selenium_used': True}\n",
            "Processing business: Le Grand Restaurant\n",
            "✅ Saved: République Café Bakery & République Restaurant\n",
            "DEBUG for https://www.flemingssteakhouse.com/Locations/CA/Los-Angeles?y_source=1_MTU1MDMxOTctNzE1LWxvY2F0aW9uLndlYnNpdGU%3D: {'pages_fetched': 5, 'social_found': 2, 'emails_found': 0, 'job_content_found': 5, 'selenium_used': True}\n",
            "Processing business: BROKEN MOUTH | Lee's Homestyle\n",
            "✅ Saved: Fleming’s Prime Steakhouse & Wine Bar\n",
            "DEBUG for http://www.majordomo.la/: {'pages_fetched': 5, 'social_found': 1, 'emails_found': 2, 'job_content_found': 5, 'selenium_used': False}\n",
            "Processing business: Moonlark's Dinette\n",
            "✅ Saved: Majordomo\n",
            "DEBUG for http://www.ilovemole.com/: {'pages_fetched': 5, 'social_found': 3, 'emails_found': 2, 'job_content_found': 5, 'selenium_used': False}\n",
            "Processing business: Cabra Los Angeles\n",
            "✅ Saved: Guelaguetza Restaurant\n",
            "DEBUG for https://www.legrand-restaurant.com/: {'pages_fetched': 5, 'social_found': 2, 'emails_found': 1, 'job_content_found': 2, 'selenium_used': False}\n",
            "✅ Saved: Le Grand Restaurant\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Connection pool is full, discarding connection: localhost. Connection pool size: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG for http://eatbrokenmouth.com/: {'pages_fetched': 4, 'social_found': 2, 'emails_found': 1, 'job_content_found': 0, 'selenium_used': True}\n",
            "✅ Saved: BROKEN MOUTH | Lee's Homestyle\n",
            "DEBUG for https://cabralosangeles.com/: {'pages_fetched': 5, 'social_found': 0, 'emails_found': 3, 'job_content_found': 2, 'selenium_used': False}\n",
            "✅ Saved: Cabra Los Angeles\n",
            "DEBUG for https://www.moonlarksla.com/: {'pages_fetched': 5, 'social_found': 2, 'emails_found': 1, 'job_content_found': 0, 'selenium_used': True}\n",
            "✅ Saved: Moonlark's Dinette\n",
            "DEBUG for http://www.maccheronirepublic.com/: {'pages_fetched': 5, 'social_found': 2, 'emails_found': 1, 'job_content_found': 4, 'selenium_used': False}\n",
            "✅ Saved: Maccheroni Republic\n",
            "\n",
            "==================================================\n",
            "📊 FINAL METRICS REPORT\n",
            "==================================================\n",
            "📈 Total businesses processed: 20\n",
            "🌐 Businesses with websites: 20 (100.0%)\n",
            "📧 Businesses with emails: 14 (70.0%)\n",
            "👔 Businesses with HR emails: 7 (35.0%)\n",
            "📱 Businesses with social media: 19 (95.0%)\n",
            "💼 Businesses with job postings: 13 (65.0%)\n",
            "\n",
            "🎉 Scraping completed successfully!\n",
            "📄 Data saved to: comprehensive_restaurant_data.csv\n",
            "🔧 Selenium driver closed\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "import json\n",
        "import pandas as pd\n",
        "import requests\n",
        "import threading\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from collections import deque\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "# Constants\n",
        "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY', "Enter your key")  # Use env var\n",
        "TEXT_SEARCH_URL = \"https://maps.googleapis.com/maps/api/place/textsearch/json\"\n",
        "PLACE_DETAILS_URL = \"https://maps.googleapis.com/maps/api/place/details/json\"\n",
        "SEARCH_TAGS = [\"restaurant\"]\n",
        "SEARCH_LOCATIONS = [\"Los Angeles\"]\n",
        "MAX_PAGES_PER_SITE = 15\n",
        "CAREERS_MAX_PAGES = 5  # Separate limit for careers crawling\n",
        "THREAD_POOL_WORKERS = 5\n",
        "USER_AGENTS = [\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "]\n",
        "SOCIAL_PLATFORMS = [\"facebook.com\", \"instagram.com\", \"linkedin.com\", \"twitter.com\", \"tiktok.com\"]\n",
        "\n",
        "# Setup headless Chrome for Selenium\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-gpu\")\n",
        "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# ================= GOOGLE MAPS API FUNCTIONS =================\n",
        "\n",
        "def search_places(query, location):\n",
        "    \"\"\"Search for places using Google Maps API\"\"\"\n",
        "    params = {\"query\": f\"{query} in {location}\", \"key\": GOOGLE_API_KEY}\n",
        "    response = requests.get(TEXT_SEARCH_URL, params=params)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Error: {response.status_code}\")\n",
        "        return [], []\n",
        "    results = response.json().get(\"results\", [])\n",
        "    return [r.get(\"place_id\") for r in results], [r.get(\"name\") for r in results]\n",
        "\n",
        "def get_place_details(place_id):\n",
        "    \"\"\"Get detailed information about a place\"\"\"\n",
        "    params = {\n",
        "        \"place_id\": place_id,\n",
        "        \"fields\": \"name,formatted_address,website,formatted_phone_number\",\n",
        "        \"key\": GOOGLE_API_KEY,\n",
        "    }\n",
        "    response = requests.get(PLACE_DETAILS_URL, params=params)\n",
        "    if response.status_code != 200:\n",
        "        return None\n",
        "    return response.json().get(\"result\", {})\n",
        "\n",
        "# ================= EMAIL EXTRACTION FUNCTIONS =================\n",
        "\n",
        "def extract_emails(text, soup=None):\n",
        "    \"\"\"Extract all email addresses from text\"\"\"\n",
        "    pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
        "    emails = set(re.findall(pattern, text))\n",
        "\n",
        "    # Extract from mailto links if soup is provided\n",
        "    if soup:\n",
        "        for a in soup.find_all(\"a\", href=True):\n",
        "            href = a['href'].lower()\n",
        "            if href.startswith(\"mailto:\"):\n",
        "                email = href[7:].split(\"?\")[0].strip()\n",
        "                if re.match(pattern, email):\n",
        "                    emails.add(email)\n",
        "            visible = a.get_text(strip=True)\n",
        "            if re.match(pattern, visible):\n",
        "                emails.add(visible)\n",
        "    return list(emails)\n",
        "\n",
        "def extract_hr_emails(text):\n",
        "    \"\"\"Extract hiring/HR-related email addresses from text\"\"\"\n",
        "    pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
        "    emails = re.findall(pattern, text)\n",
        "\n",
        "    # Filter for business/hiring related emails\n",
        "    valid_emails = []\n",
        "    for email in emails:\n",
        "        email_lower = email.lower()\n",
        "        # Include hiring-related and general contact emails\n",
        "        if any(email_lower.startswith(prefix) for prefix in [\"hr@\", \"jobs@\", \"careers@\", \"hiring@\", \"recruiting@\", \"info@\", \"contact@\", \"hello@\"]):\n",
        "            valid_emails.append(email)\n",
        "        # Include Gmail accounts (often used by small businesses)\n",
        "        elif email_lower.endswith(\"@gmail.com\"):\n",
        "            valid_emails.append(email)\n",
        "\n",
        "    return list(set(valid_emails))\n",
        "\n",
        "# ================= SOCIAL MEDIA EXTRACTION =================\n",
        "\n",
        "def extract_social_links(soup, base_url):\n",
        "    \"\"\"Extract social media links from soup\"\"\"\n",
        "    social_links = set()\n",
        "    SOCIAL_DOMAINS = [\"facebook.com\", \"instagram.com\", \"linkedin.com\", \"twitter.com\", \"tiktok.com\"]\n",
        "\n",
        "    if soup:\n",
        "        for a in soup.find_all(\"a\", href=True):\n",
        "            href = a['href'].strip()\n",
        "            if any(domain in href for domain in SOCIAL_DOMAINS):\n",
        "                social_links.add(href)\n",
        "    return list(social_links)\n",
        "\n",
        "def get_social_from_google(details):\n",
        "    \"\"\"Extract social links from Google Place details (placeholder)\"\"\"\n",
        "    # This would extract social media from Google Place details if available\n",
        "    # For now, returning empty list as Google Places API doesn't directly provide social links\n",
        "    return []\n",
        "\n",
        "# ================= JOB CONTENT DETECTION =================\n",
        "\n",
        "def detect_job_content(text):\n",
        "    \"\"\"Detect job-related content in text\"\"\"\n",
        "    job_keywords = [\n",
        "        \"we're hiring\", \"now hiring\", \"join our team\", \"career\", \"careers\", \"job opening\",\n",
        "        \"position available\", \"apply now\", \"job application\", \"employment\", \"work with us\",\n",
        "        \"job opportunity\", \"open position\", \"join us\"\n",
        "    ]\n",
        "\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    for keyword in job_keywords:\n",
        "        if keyword in text_lower:\n",
        "            # Find the context around the keyword\n",
        "            start = max(0, text_lower.find(keyword) - 100)\n",
        "            end = min(len(text_lower), text_lower.find(keyword) + 100)\n",
        "            context = text[start:end].strip()\n",
        "            return True, context\n",
        "\n",
        "    return False, \"\"\n",
        "\n",
        "# ================= WEB CRAWLING FUNCTIONS =================\n",
        "\n",
        "def extract_links_and_content(url):\n",
        "    \"\"\"Extract links and content from a URL\"\"\"\n",
        "    try:\n",
        "        headers = {\"User-Agent\": random.choice(USER_AGENTS)}\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # Extract links\n",
        "        links = []\n",
        "        for a in soup.find_all(\"a\", href=True):\n",
        "            links.append(a['href'])\n",
        "\n",
        "        # Extract text content\n",
        "        content = soup.get_text(\" \", strip=True)\n",
        "\n",
        "        return links, content\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting from {url}: {e}\")\n",
        "        return [], \"\"\n",
        "\n",
        "def crawl_site_comprehensive(base_url, max_pages=15, careers_focused=False):\n",
        "    \"\"\"\n",
        "    Comprehensive website crawling for both general data and careers info\n",
        "    \"\"\"\n",
        "    visited = set()\n",
        "    queue = deque([base_url])\n",
        "    emails = set()\n",
        "    hr_emails = set()\n",
        "    social_links = set()\n",
        "    job_content = []\n",
        "    all_discovered_links = set()\n",
        "\n",
        "    debug_info = {\n",
        "        \"pages_fetched\": 0,\n",
        "        \"social_found\": 0,\n",
        "        \"emails_found\": 0,\n",
        "        \"job_content_found\": 0,\n",
        "        \"selenium_used\": False\n",
        "    }\n",
        "\n",
        "    domain = urlparse(base_url).netloc.replace(\"www.\", \"\")\n",
        "    pages_crawled = 0\n",
        "\n",
        "    # Determine max pages based on focus\n",
        "    max_pages_to_crawl = CAREERS_MAX_PAGES if careers_focused else max_pages\n",
        "\n",
        "    while queue and pages_crawled < max_pages_to_crawl:\n",
        "        url = queue.popleft()\n",
        "        if url in visited:\n",
        "            continue\n",
        "        visited.add(url)\n",
        "\n",
        "        try:\n",
        "            headers = {\"User-Agent\": random.choice(USER_AGENTS)}\n",
        "            resp = requests.get(url, headers=headers, timeout=10)\n",
        "            resp.raise_for_status()\n",
        "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "            text = soup.get_text(\" \", strip=True)\n",
        "\n",
        "            debug_info[\"pages_fetched\"] += 1\n",
        "\n",
        "            # Extract all emails\n",
        "            page_emails = extract_emails(text, soup)\n",
        "            emails.update(page_emails)\n",
        "\n",
        "            # Extract HR-specific emails\n",
        "            page_hr_emails = extract_hr_emails(text)\n",
        "            hr_emails.update(page_hr_emails)\n",
        "\n",
        "            # Extract social media links\n",
        "            page_social = extract_social_links(soup, url)\n",
        "            social_links.update(page_social)\n",
        "\n",
        "            # Detect job content\n",
        "            has_job, job_context = detect_job_content(text)\n",
        "            if has_job:\n",
        "                job_content.append({\n",
        "                    \"url\": url,\n",
        "                    \"context\": job_context\n",
        "                })\n",
        "                debug_info[\"job_content_found\"] += 1\n",
        "\n",
        "            # Discover more links (prioritize careers/jobs pages)\n",
        "            for a in soup.find_all(\"a\", href=True):\n",
        "                new_url = urljoin(url, a['href'])\n",
        "                parsed_url = urlparse(new_url)\n",
        "\n",
        "                if domain in parsed_url.netloc:\n",
        "                    all_discovered_links.add(new_url)\n",
        "\n",
        "                    # Prioritize careers-related pages\n",
        "                    link_text = a.get_text(strip=True).lower()\n",
        "                    href_lower = a['href'].lower()\n",
        "\n",
        "                    if any(keyword in link_text or keyword in href_lower\n",
        "                           for keyword in ['career', 'job', 'hiring', 'employment']):\n",
        "                        # Add careers pages to front of queue\n",
        "                        if new_url not in visited:\n",
        "                            queue.appendleft(new_url)\n",
        "                    elif new_url not in visited:\n",
        "                        queue.append(new_url)\n",
        "\n",
        "            # Add delay to be polite\n",
        "            time.sleep(random.uniform(0.5, 1.5))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error crawling {url}: {e}\")\n",
        "\n",
        "        pages_crawled += 1\n",
        "\n",
        "    # Use Selenium fallback if no emails or job content found\n",
        "    if not emails or (careers_focused and not job_content):\n",
        "        try:\n",
        "            debug_info[\"selenium_used\"] = True\n",
        "            driver.get(base_url)\n",
        "            time.sleep(3)  # Wait for JS to load\n",
        "            html = driver.page_source\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "            text = soup.get_text(\" \", strip=True)\n",
        "\n",
        "            if not emails:\n",
        "                selenium_emails = extract_emails(text, soup)\n",
        "                emails.update(selenium_emails)\n",
        "                selenium_hr_emails = extract_hr_emails(text)\n",
        "                hr_emails.update(selenium_hr_emails)\n",
        "\n",
        "            if careers_focused and not job_content:\n",
        "                has_job, job_context = detect_job_content(text)\n",
        "                if has_job:\n",
        "                    job_content.append({\n",
        "                        \"url\": base_url,\n",
        "                        \"context\": job_context\n",
        "                    })\n",
        "\n",
        "            if not social_links:\n",
        "                selenium_social = extract_social_links(soup, base_url)\n",
        "                social_links.update(selenium_social)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Selenium fallback failed for {base_url}: {e}\")\n",
        "\n",
        "    # Update debug info\n",
        "    debug_info[\"emails_found\"] = len(emails)\n",
        "    debug_info[\"social_found\"] = len(social_links)\n",
        "\n",
        "    print(f\"DEBUG for {base_url}: {debug_info}\")\n",
        "\n",
        "    return {\n",
        "        \"emails\": list(emails),\n",
        "        \"hr_emails\": list(hr_emails),\n",
        "        \"social_links\": list(social_links),\n",
        "        \"job_content\": job_content,\n",
        "        \"crawl_status\": \"Success\" if debug_info[\"pages_fetched\"] > 0 else \"Failed\"\n",
        "    }\n",
        "\n",
        "# ================= BUSINESS PROCESSING =================\n",
        "\n",
        "def process_business_comprehensive(place_id, name):\n",
        "    \"\"\"Process a business with comprehensive data extraction\"\"\"\n",
        "    print(f\"Processing business: {name}\")\n",
        "\n",
        "    # Get Google Places details\n",
        "    details = get_place_details(place_id)\n",
        "    if not details:\n",
        "        return None\n",
        "\n",
        "    website = details.get(\"website\", \"\")\n",
        "    address = details.get(\"formatted_address\", \"\")\n",
        "    phone = details.get(\"formatted_phone_number\", \"\")\n",
        "\n",
        "    # Initialize result structure\n",
        "    result = {\n",
        "        \"place_id\": place_id,\n",
        "        \"name\": name,\n",
        "        \"address\": address,\n",
        "        \"website\": website,\n",
        "        \"phone\": phone,\n",
        "        \"emails\": [],\n",
        "        \"hr_emails\": [],\n",
        "        \"social_links\": [],\n",
        "        \"has_job_posting\": \"No\",\n",
        "        \"job_urls\": [],\n",
        "        \"job_contexts\": [],\n",
        "        \"crawl_status\": \"Not crawled\"\n",
        "    }\n",
        "\n",
        "    # Crawl website if available\n",
        "    if website:\n",
        "        try:\n",
        "            crawl_results = crawl_site_comprehensive(website, MAX_PAGES_PER_SITE, careers_focused=True)\n",
        "\n",
        "            result[\"emails\"] = crawl_results[\"emails\"]\n",
        "            result[\"hr_emails\"] = crawl_results[\"hr_emails\"]\n",
        "            result[\"social_links\"] = crawl_results[\"social_links\"]\n",
        "            result[\"crawl_status\"] = crawl_results[\"crawl_status\"]\n",
        "\n",
        "            # Process job content\n",
        "            if crawl_results[\"job_content\"]:\n",
        "                result[\"has_job_posting\"] = \"Yes\"\n",
        "                result[\"job_urls\"] = [item[\"url\"] for item in crawl_results[\"job_content\"]]\n",
        "                result[\"job_contexts\"] = [item[\"context\"] for item in crawl_results[\"job_content\"]]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to crawl {website}: {e}\")\n",
        "            result[\"crawl_status\"] = f\"Error: {str(e)}\"\n",
        "\n",
        "    # Check Google-provided social links if no website\n",
        "    if not website:\n",
        "        google_social = get_social_from_google(details)\n",
        "        result[\"social_links\"].extend(google_social)\n",
        "\n",
        "    return result\n",
        "\n",
        "# ================= MAIN FUNCTION =================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    print(\"🚀 Starting comprehensive restaurant & careers scraper\")\n",
        "\n",
        "    lock = threading.Lock()\n",
        "    checkpoint_file = \"comprehensive_restaurant_data.csv\"\n",
        "\n",
        "    # Load existing data\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        existing_df = pd.read_csv(checkpoint_file)\n",
        "        print(f\"📄 Loaded {len(existing_df)} existing records\")\n",
        "    else:\n",
        "        existing_df = pd.DataFrame(columns=[\n",
        "            \"place_id\", \"name\", \"address\", \"website\", \"phone\", \"emails\", \"hr_emails\",\n",
        "            \"social_links\", \"has_job_posting\", \"job_urls\", \"job_contexts\", \"crawl_status\"\n",
        "        ])\n",
        "\n",
        "    existing_data = {row['place_id']: row for _, row in existing_df.iterrows()}\n",
        "\n",
        "    # Process each location and search tag\n",
        "    for location in SEARCH_LOCATIONS:\n",
        "        for tag in SEARCH_TAGS:\n",
        "            print(f\"\\n🔍 Searching for {tag} in {location}\")\n",
        "            place_ids, names = search_places(tag, location)\n",
        "            print(f\"📍 Found {len(place_ids)} businesses\")\n",
        "\n",
        "            # Process businesses with threading\n",
        "            with ThreadPoolExecutor(max_workers=THREAD_POOL_WORKERS) as executor:\n",
        "                futures = {\n",
        "                    executor.submit(process_business_comprehensive, pid, name): pid\n",
        "                    for pid, name in zip(place_ids, names)\n",
        "                    if pid not in existing_data\n",
        "                }\n",
        "\n",
        "                for future in as_completed(futures):\n",
        "                    result = future.result()\n",
        "                    if result:\n",
        "                        with lock:\n",
        "                            # Convert lists to strings for CSV storage\n",
        "                            csv_result = result.copy()\n",
        "                            csv_result[\"emails\"] = \"; \".join(result[\"emails\"]) if result[\"emails\"] else \"None found\"\n",
        "                            csv_result[\"hr_emails\"] = \"; \".join(result[\"hr_emails\"]) if result[\"hr_emails\"] else \"None found\"\n",
        "                            csv_result[\"social_links\"] = \"; \".join(result[\"social_links\"]) if result[\"social_links\"] else \"None found\"\n",
        "                            csv_result[\"job_urls\"] = \"; \".join(result[\"job_urls\"]) if result[\"job_urls\"] else \"None found\"\n",
        "                            csv_result[\"job_contexts\"] = \"; \".join(result[\"job_contexts\"]) if result[\"job_contexts\"] else \"None found\"\n",
        "\n",
        "                            # Add to DataFrame and save\n",
        "                            existing_df = pd.concat([existing_df, pd.DataFrame([csv_result])], ignore_index=True)\n",
        "                            existing_df.to_csv(checkpoint_file, index=False)\n",
        "                            print(f\"✅ Saved: {result['name']}\")\n",
        "\n",
        "    # Generate final metrics report\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"📊 FINAL METRICS REPORT\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    total_businesses = len(existing_df)\n",
        "    with_websites = len(existing_df[existing_df['website'] != ''])\n",
        "    with_emails = len(existing_df[existing_df['emails'] != 'None found'])\n",
        "    with_hr_emails = len(existing_df[existing_df['hr_emails'] != 'None found'])\n",
        "    with_social = len(existing_df[existing_df['social_links'] != 'None found'])\n",
        "    with_jobs = len(existing_df[existing_df['has_job_posting'] == 'Yes'])\n",
        "\n",
        "    print(f\"📈 Total businesses processed: {total_businesses}\")\n",
        "    print(f\"🌐 Businesses with websites: {with_websites} ({with_websites/total_businesses*100:.1f}%)\")\n",
        "    print(f\"📧 Businesses with emails: {with_emails} ({with_emails/total_businesses*100:.1f}%)\")\n",
        "    print(f\"👔 Businesses with HR emails: {with_hr_emails} ({with_hr_emails/total_businesses*100:.1f}%)\")\n",
        "    print(f\"📱 Businesses with social media: {with_social} ({with_social/total_businesses*100:.1f}%)\")\n",
        "    print(f\"💼 Businesses with job postings: {with_jobs} ({with_jobs/total_businesses*100:.1f}%)\")\n",
        "\n",
        "    print(\"\\n🎉 Scraping completed successfully!\")\n",
        "    print(f\"📄 Data saved to: {checkpoint_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        main()\n",
        "    finally:\n",
        "        # Ensure Selenium driver is properly closed\n",
        "        try:\n",
        "            driver.quit()\n",
        "            print(\"🔧 Selenium driver closed\")\n",
        "        except:\n",
        "            pass"
      ]
    }
  ]
}
