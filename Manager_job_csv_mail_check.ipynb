{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Email & URL Validation Tool\n",
        "\n",
        "## Overview\n",
        "\n",
        "This tool validates email addresses and website URLs in business contact datasets to improve data quality and deliverability. It outputs a cleaned dataset containing only verified, working contacts.\n",
        "\n",
        "## Features\n",
        "\n",
        "### Email Validation\n",
        "\n",
        "* Validates email format and domain structure\n",
        "* Verifies DNS MX records to ensure email deliverability\n",
        "\n",
        "### URL Validation\n",
        "\n",
        "* Checks URL syntax and structure\n",
        "* Confirms HTTP status (200–399)\n",
        "* Verifies website accessibility\n",
        "\n",
        "### Data Processing\n",
        "\n",
        "* Uses multi-threading (10 workers) for efficiency\n",
        "* Handles timeouts, connection failures, and malformed data\n",
        "* Retains original dataset structure while removing invalid entries\n",
        "\n",
        "## Input Requirements\n",
        "\n",
        "* Input file: CSV\n",
        "* Required columns: `channel_email`, `channel_website`, `organization_name`\n",
        "* Supports missing or inconsistent data\n",
        "\n",
        "## Output\n",
        "\n",
        "* `cleaned_manager_jobs.csv`: Filtered dataset with only valid contacts\n",
        "* `validation_results.csv`: Detailed results for each email and URL\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nZQZaSat9m7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dnspython\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2D8xXvB2Cxf",
        "outputId": "e3de1a9a-ea84-4b35-93ae-e8e8f8698b97"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dnspython in /usr/local/lib/python3.11/dist-packages (2.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install validators\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJwVxzUB2oCd",
        "outputId": "03f0f654-2c7c-4fdc-c4a6-119cbd2c057d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: validators in /usr/local/lib/python3.11/dist-packages (0.35.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_uBcSQeR2oGm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import httpx\n",
        "import re\n",
        "import dns.resolver\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import validators\n",
        "import time\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "class EmailURLValidator:\n",
        "    def __init__(self, max_workers: int = 10, timeout: int = 15):\n",
        "        self.max_workers = max_workers\n",
        "        self.timeout = timeout\n",
        "        self.headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
        "\n",
        "    def validate_email_format(self, email: str) -> bool:\n",
        "        \"\"\"Basic email format validation\"\"\"\n",
        "        email_regex = r'^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$'\n",
        "        return bool(re.match(email_regex, email.strip()))\n",
        "\n",
        "    def check_mx_record(self, domain: str) -> bool:\n",
        "        \"\"\"Check if domain has MX record\"\"\"\n",
        "        try:\n",
        "            dns.resolver.resolve(domain, 'MX')\n",
        "            return True\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    def validate_email(self, email) -> Dict:\n",
        "        \"\"\"Comprehensive email validation\"\"\"\n",
        "        if pd.isna(email) or not str(email).strip():\n",
        "            return {'status': 'empty', 'error': 'Empty email'}\n",
        "\n",
        "        email = str(email).strip()\n",
        "\n",
        "        if not self.validate_email_format(email):\n",
        "            return {'status': 'invalid', 'error': 'Invalid format'}\n",
        "\n",
        "        try:\n",
        "            domain = email.split('@')[1].lower()\n",
        "            if self.check_mx_record(domain):\n",
        "                return {'status': 'valid', 'error': ''}\n",
        "            else:\n",
        "                return {'status': 'no_mx', 'error': 'No MX record'}\n",
        "        except:\n",
        "            return {'status': 'invalid', 'error': 'Domain error'}\n",
        "\n",
        "    def check_url_status(self, url) -> Dict:\n",
        "        \"\"\"Check URL HTTP status\"\"\"\n",
        "        if pd.isna(url) or not str(url).strip():\n",
        "            return {'accessible': False, 'error': 'Empty URL'}\n",
        "\n",
        "        url = str(url).strip()\n",
        "\n",
        "        if not validators.url(url):\n",
        "            return {'accessible': False, 'error': 'Invalid URL format'}\n",
        "\n",
        "        try:\n",
        "            with httpx.Client(timeout=self.timeout, follow_redirects=True, headers=self.headers) as client:\n",
        "                response = client.get(url)\n",
        "                if response.status_code < 400:\n",
        "                    return {'accessible': True, 'error': ''}\n",
        "                else:\n",
        "                    return {'accessible': False, 'error': f'HTTP {response.status_code}'}\n",
        "        except:\n",
        "            return {'accessible': False, 'error': 'Connection failed'}\n",
        "\n",
        "    def process_single_row(self, row_index, row_data):\n",
        "        \"\"\"Process a single row\"\"\"\n",
        "        result = {\n",
        "            'row_index': row_index,\n",
        "            'organization_name': str(row_data.get('organization_name', '')),\n",
        "            'channel_email': str(row_data.get('channel_email', '')),\n",
        "            'channel_website': str(row_data.get('channel_website', '')),\n",
        "            'email_valid': False,\n",
        "            'url_accessible': False,\n",
        "            'email_error': '',\n",
        "            'url_error': ''\n",
        "        }\n",
        "\n",
        "        # Test email\n",
        "        if result['channel_email'] and result['channel_email'] != 'nan':\n",
        "            email_result = self.validate_email(result['channel_email'])\n",
        "            result['email_valid'] = email_result['status'] == 'valid'\n",
        "            result['email_error'] = email_result['error']\n",
        "\n",
        "        # Test URL\n",
        "        if result['channel_website'] and result['channel_website'] != 'nan':\n",
        "            url_result = self.check_url_status(result['channel_website'])\n",
        "            result['url_accessible'] = url_result['accessible']\n",
        "            result['url_error'] = url_result['error']\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "def main():\n",
        "    INPUT_CSV = \"manager_jobs_rows.csv\"\n",
        "    CLEANED_CSV = \"cleaned_manager_jobs.csv\"\n",
        "    RESULTS_CSV = \"validation_results.csv\"\n",
        "\n",
        "    print(\"🚀 Starting Email & URL Validation\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Load dataset\n",
        "    try:\n",
        "        df = pd.read_csv(INPUT_CSV)\n",
        "        print(f\"✅ Loaded dataset with {len(df)} rows\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error reading CSV: {e}\")\n",
        "        return\n",
        "\n",
        "    # Initialize validator\n",
        "    validator = EmailURLValidator(max_workers=10, timeout=10)\n",
        "\n",
        "    # Process all rows\n",
        "    print(\"🔍 Processing rows...\")\n",
        "    results = []\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        # Submit all rows for processing\n",
        "        futures = []\n",
        "        for idx, row in df.iterrows():\n",
        "            future = executor.submit(validator.process_single_row, idx, row)\n",
        "            futures.append(future)\n",
        "\n",
        "        # Collect results\n",
        "        for i, future in enumerate(as_completed(futures)):\n",
        "            try:\n",
        "                result = future.result()\n",
        "                results.append(result)\n",
        "\n",
        "                if (i + 1) % 100 == 0:\n",
        "                    print(f\"Processed {i + 1}/{len(futures)} rows...\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing row: {e}\")\n",
        "\n",
        "    print(f\"✅ Processed {len(results)} rows\")\n",
        "\n",
        "    # Create results DataFrame\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_csv(RESULTS_CSV, index=False)\n",
        "    print(f\"📊 Results saved to: {RESULTS_CSV}\")\n",
        "\n",
        "    # Create cleaned dataset\n",
        "    print(\"🧹 Creating cleaned dataset...\")\n",
        "\n",
        "    # Strict approach: remove rows if email is invalid OR URL is broken\n",
        "    cleaned_rows = []\n",
        "    removed_count = 0\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        # Find validation result for this row\n",
        "        result = next((r for r in results if r['row_index'] == idx), None)\n",
        "\n",
        "        if result:\n",
        "            remove_row = False\n",
        "\n",
        "            # Remove if email exists but is invalid\n",
        "            if result['channel_email'] and result['channel_email'] != 'nan' and not result['email_valid']:\n",
        "                remove_row = True\n",
        "\n",
        "            # Remove if URL exists but is not accessible\n",
        "            if result['channel_website'] and result['channel_website'] != 'nan' and not result['url_accessible']:\n",
        "                remove_row = True\n",
        "\n",
        "            if not remove_row:\n",
        "                cleaned_rows.append(row)\n",
        "            else:\n",
        "                removed_count += 1\n",
        "        else:\n",
        "            # If no validation was performed, keep the row\n",
        "            cleaned_rows.append(row)\n",
        "\n",
        "    # Save cleaned dataset\n",
        "    cleaned_df = pd.DataFrame(cleaned_rows)\n",
        "    cleaned_df.to_csv(CLEANED_CSV, index=False)\n",
        "\n",
        "    # Print summary\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"📈 CLEANING SUMMARY:\")\n",
        "    print(f\"Original dataset: {len(df)} rows\")\n",
        "    print(f\"Cleaned dataset: {len(cleaned_df)} rows\")\n",
        "    print(f\"Removed: {removed_count} rows\")\n",
        "\n",
        "    # Email summary\n",
        "    valid_emails = sum(1 for r in results if r['email_valid'])\n",
        "    total_emails = sum(1 for r in results if r['channel_email'] and r['channel_email'] != 'nan')\n",
        "    print(f\"\\n📧 EMAIL SUMMARY:\")\n",
        "    print(f\"Total emails tested: {total_emails}\")\n",
        "    print(f\"Valid emails: {valid_emails}\")\n",
        "    print(f\"Invalid emails: {total_emails - valid_emails}\")\n",
        "\n",
        "    # URL summary\n",
        "    accessible_urls = sum(1 for r in results if r['url_accessible'])\n",
        "    total_urls = sum(1 for r in results if r['channel_website'] and r['channel_website'] != 'nan')\n",
        "    print(f\"\\n🌐 URL SUMMARY:\")\n",
        "    print(f\"Total URLs tested: {total_urls}\")\n",
        "    print(f\"Accessible URLs: {accessible_urls}\")\n",
        "    print(f\"Inaccessible URLs: {total_urls - accessible_urls}\")\n",
        "\n",
        "    print(f\"\\n🎉 Cleaned dataset saved to: {CLEANED_CSV}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qL9ZsPO2Ct5",
        "outputId": "51b4c631-4359-4210-a17d-e9e41156e11d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting Email & URL Validation\n",
            "==================================================\n",
            "✅ Loaded dataset with 10170 rows\n",
            "🔍 Processing rows...\n",
            "Processed 100/10170 rows...\n",
            "Processed 200/10170 rows...\n",
            "Processed 300/10170 rows...\n",
            "Processed 400/10170 rows...\n",
            "Processed 500/10170 rows...\n",
            "Processed 600/10170 rows...\n",
            "Processed 700/10170 rows...\n",
            "Processed 800/10170 rows...\n",
            "Processed 900/10170 rows...\n",
            "Processed 1000/10170 rows...\n",
            "Processed 1100/10170 rows...\n",
            "Processed 1200/10170 rows...\n",
            "Processed 1300/10170 rows...\n",
            "Processed 1400/10170 rows...\n",
            "Processed 1500/10170 rows...\n",
            "Processed 1600/10170 rows...\n",
            "Processed 1700/10170 rows...\n",
            "Processed 1800/10170 rows...\n",
            "Processed 1900/10170 rows...\n",
            "Processed 2000/10170 rows...\n",
            "Processed 2100/10170 rows...\n",
            "Processed 2200/10170 rows...\n",
            "Processed 2300/10170 rows...\n",
            "Processed 2400/10170 rows...\n",
            "Processed 2500/10170 rows...\n",
            "Processed 2600/10170 rows...\n",
            "Processed 2700/10170 rows...\n",
            "Processed 2800/10170 rows...\n",
            "Processed 2900/10170 rows...\n",
            "Processed 3000/10170 rows...\n",
            "Processed 3100/10170 rows...\n",
            "Processed 3200/10170 rows...\n",
            "Processed 3300/10170 rows...\n",
            "Processed 3400/10170 rows...\n",
            "Processed 3500/10170 rows...\n",
            "Processed 3600/10170 rows...\n",
            "Processed 3700/10170 rows...\n",
            "Processed 3800/10170 rows...\n",
            "Processed 3900/10170 rows...\n",
            "Processed 4000/10170 rows...\n",
            "Processed 4100/10170 rows...\n",
            "Processed 4200/10170 rows...\n",
            "Processed 4300/10170 rows...\n",
            "Processed 4400/10170 rows...\n",
            "Processed 4500/10170 rows...\n",
            "Processed 4600/10170 rows...\n",
            "Processed 4700/10170 rows...\n",
            "Processed 4800/10170 rows...\n",
            "Processed 4900/10170 rows...\n",
            "Processed 5000/10170 rows...\n",
            "Processed 5100/10170 rows...\n",
            "Processed 5200/10170 rows...\n",
            "Processed 5300/10170 rows...\n",
            "Processed 5400/10170 rows...\n",
            "Processed 5500/10170 rows...\n",
            "Processed 5600/10170 rows...\n",
            "Processed 5700/10170 rows...\n",
            "Processed 5800/10170 rows...\n",
            "Processed 5900/10170 rows...\n",
            "Processed 6000/10170 rows...\n",
            "Processed 6100/10170 rows...\n",
            "Processed 6200/10170 rows...\n",
            "Processed 6300/10170 rows...\n",
            "Processed 6400/10170 rows...\n",
            "Processed 6500/10170 rows...\n",
            "Processed 6600/10170 rows...\n",
            "Processed 6700/10170 rows...\n",
            "Processed 6800/10170 rows...\n",
            "Processed 6900/10170 rows...\n",
            "Processed 7000/10170 rows...\n",
            "Processed 7100/10170 rows...\n",
            "Processed 7200/10170 rows...\n",
            "Processed 7300/10170 rows...\n",
            "Processed 7400/10170 rows...\n",
            "Processed 7500/10170 rows...\n",
            "Processed 7600/10170 rows...\n",
            "Processed 7700/10170 rows...\n",
            "Processed 7800/10170 rows...\n",
            "Processed 7900/10170 rows...\n",
            "Processed 8000/10170 rows...\n",
            "Processed 8100/10170 rows...\n",
            "Processed 8200/10170 rows...\n",
            "Processed 8300/10170 rows...\n",
            "Processed 8400/10170 rows...\n",
            "Processed 8500/10170 rows...\n",
            "Processed 8600/10170 rows...\n",
            "Processed 8700/10170 rows...\n",
            "Processed 8800/10170 rows...\n",
            "Processed 8900/10170 rows...\n",
            "Processed 9000/10170 rows...\n",
            "Processed 9100/10170 rows...\n",
            "Processed 9200/10170 rows...\n",
            "Processed 9300/10170 rows...\n",
            "Processed 9400/10170 rows...\n",
            "Processed 9500/10170 rows...\n",
            "Processed 9600/10170 rows...\n",
            "Processed 9700/10170 rows...\n",
            "Processed 9800/10170 rows...\n",
            "Processed 9900/10170 rows...\n",
            "Processed 10000/10170 rows...\n",
            "Processed 10100/10170 rows...\n",
            "✅ Processed 10170 rows\n",
            "📊 Results saved to: validation_results.csv\n",
            "🧹 Creating cleaned dataset...\n",
            "==================================================\n",
            "📈 CLEANING SUMMARY:\n",
            "Original dataset: 10170 rows\n",
            "Cleaned dataset: 9290 rows\n",
            "Removed: 880 rows\n",
            "\n",
            "📧 EMAIL SUMMARY:\n",
            "Total emails tested: 9036\n",
            "Valid emails: 8817\n",
            "Invalid emails: 219\n",
            "\n",
            "🌐 URL SUMMARY:\n",
            "Total URLs tested: 5622\n",
            "Accessible URLs: 4949\n",
            "Inaccessible URLs: 673\n",
            "\n",
            "🎉 Cleaned dataset saved to: cleaned_manager_jobs.csv\n"
          ]
        }
      ]
    }
  ]
}